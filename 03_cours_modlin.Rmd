# ANOVA

L’objectif de cette section est : 

  - d’introduire l’ANOVA comme **modèle statistique** (formulation, paramètres, adéquation aux données)
  - d’interroger le modèles grace au **tests statistiques** (Fisher et Tuckey)

---
  
Dans le cadre de l’analyse de la variance (**ANOVA**) on cherche une relation entre la variable observée $Y$ quantitative et la variable explicative $X$ qualitative, appellée aussi facteur. 

$$Y \sim X$$

Comme pour la regression linéaire, l’ANOVA est à la fois :

  * un **modèle** (linéaire) fondé sur la décomposition de la variance 
  * un **test** statistique permettant de comparer les moyennes d’une variable aléatoire *indépendante*, *gaussienne* et *homogène en variance*.

Ex : le poids moyen de différents groupes d’individus.

L’analyse de la variance est un outil statistique très largement utilisé.


## Présentation du modèle


$Y$ est la variable aléatoire expliquée (modélisée) par la variables explicatives $X$. 

Le modèle s’écrit :

$$Y = \mu + I_X \beta + \epsilon$$

avec : 

  - $Y$ un vecteur de taille $n$ (le nombre d’observations) correspondant aux valeurs de la variable aléatoire 
  - $\mu$ la moyenne des observations.  
  - $I_X \beta$ un vecteur correspondant à l’effet du groupe au quel appartient chaque observation
  - $\epsilon$ un vecteur de taille $n$ correspondant aux valeurs des résidus. 

On note : 
  
  - $\beta$ un vecteur de taille $p$ (le nombre de niveaux du facteur $X$) correspondant à la moyenne des observations pour chaque niveau du facteur moins la moyenne générale.
  - $I_X$ la *matrice d’incidence* (tableau disjonctif ?) de taille $n$ x $p$, ne comportant que des 0 et des 1 et permettant d’associer chaque observation à son niveau de facteur $X$.   



```
B = (b_grp1, b_grp2)

I = 
     grp1 grp2
A       1   0
B       1   0
C       1   0
D       0   1
E       0   1
F       0   1

I * B = 
              b_grp1
              b_grp2
        1   0         =   b_grp1
        1   0             b_grp1
        1   0             b_grp1
        0   1             b_grp2
        0   1             b_grp2
        0   1             b_grp2

Y_A = mu +  b_grp1 + e_A
Y_B = mu +  b_grp1 + e_B
Y_C = mu +  b_grp1 + e_C
Y_D = mu +  b_grp2 + e_D
Y_E = mu +  b_grp2 + e_E
Y_F = mu +  b_grp2 + e_F
```


**Exemple `multcomp::recovery`**


```{r echo=TRUE, results="verbatim"}
sub_recovery = multcomp::recovery[multcomp::recovery$blanket%in%c("b1", "b2"),]
sub_recovery$blanket = as.factor(as.character(sub_recovery$blanket))
sub_recovery
```


```{r echo=TRUE, results="verbatim"}
options(contrasts=c("contr.sum", "contr.poly"))
d = sub_recovery
m = lm(minutes~blanket, d)
summary(m)
```

```{r}
layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
p = plot(d$blanket, d$minutes, main="minutes~blanket", xlab="blanket", ylab="minutes", border="grey")
set.seed(1)
blk = jitter(as.numeric(d$blanket))
points(blk, d$minutes)

y_i = sapply(levels(d$blanket), function(s) {
 mean(d[d$blanket==s,]$minutes)
})
plot(blk, d$minutes, main="minutes~blanket", las=2, type="p", xaxt="n", xlab="blanket", ylab="minutes")
suppressWarnings(arrows(blk, d$minutes, blk, rep(y_i, each=table(d$blanket)[1]), col=adjustcolor(4, alpha.f=0.5), length=0.05))
points(y_i, pch=5, col=2)
suppressWarnings(arrows(1:6, mean(y_i), 1:6, y_i, col=adjustcolor(2, alpha.f=0.9), length=0.05))
abline(h=mean(y_i), lty=2)

axis(1, 1:length(levels(d$blanket)), labels=levels(d$blanket))
legend("topright", c("Y_i", "beta_i", "epsilon_i", "mu"), pch=c(1,5,5,NA), col=c(1,2,4,1), lty=c(0,0,0,2), border=0)
```

```{r echo=TRUE, results="verbatim"}
d = sub_recovery
m = lm(minutes~blanket, d)
summary(m)
```


```{r}
layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
p = plot(d$blanket, d$minutes, main="minutes~blanket", xlab="blanket", ylab="minutes", border="grey")
set.seed(1)
blk = jitter(as.numeric(d$blanket))
points(blk, d$minutes)

y_i = sapply(levels(d$blanket), function(s) {
 mean(d[d$blanket==s,]$minutes)
})
plot(blk, d$minutes, main="minutes~blanket", las=2, type="p", xaxt="n", xlab="blanket", ylab="minutes")
suppressWarnings(arrows(blk, d$minutes, blk, rep(y_i, each=table(d$blanket)[1]), col=adjustcolor(4, alpha.f=0.5), length=0.05))
abline(h=m$coefficient[[1]], col=2, lty=2)
abline(h=m$coefficient[[1]]+m$coefficient[[2]], col=3, lty=2)
axis(1, 1:length(levels(d$blanket)), labels=levels(d$blanket))
legend("topright", c("m$coef[1]", "m$coef[1]+m$coef[2]"), col=c(2,3), lty=2, border=0)
```















```{r echo=TRUE, results="verbatim"}
d = sub_recovery
d$blanketnum = c(0,0,0,1,1,1)
m = lm(minutes~blanketnum, d)
summary(m)
```

```{r}
layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
p = plot(d$blanket, d$minutes, main="minutes~blanket", xlab="blanket", ylab="minutes", border="grey")
set.seed(1)
blk = jitter(as.numeric(d$blanket))
points(blk, d$minutes)

plot(d$blanketnum, d$minutes, main="minutes ~ blanketnum")
arrows(d$blanketnum, d$minutes, d$blanketnum, d$minutes-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)
abline(m, col=2)
legend("topright",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)
```



**Notes** : `lm` sur ce jeu de données revient à faire un `t.test` sur ces même données en considérant que la variance des deux groupe est identique.

```{r echo=TRUE, results="verbatim"}
d = sub_recovery
t.test(d$minutes[1:3], d$minutes[4:6], var.equal=TRUE)
```

**Notes** : Une alternative au test paramétrique est une test non paramétrique (sur les rangs), *e.g.* Mann-Whitney U test (*aka* Wilcoxon Rank Sum test) ou Kruskal-Wallis H Tests.

```{r echo=TRUE, results="verbatim"}
d = sub_recovery
wilcox.test(d$minutes[1:3], d$minutes[4:6], exact=FALSE)
kruskal.test(minutes~blanket, d)
```


**Exemple *InsectSprays* **

Les données *InsectSprays* décrivent 72 parcelles agricoles par le nombre d’insectes qu’elles contiennent (variable à expliquer, *count*) et le type d’insecticide auquel elles ont été exposées (variable explicative, *spray*).

On pourra tenter d’expliquer le nombre d’insectes (Y) en fonction du type de insecticide (X), ou s’intérroger sur l’effet du type de insecticide utilisé (X) sur le nombre d’insectes observé (Y).

Nous allons ici sous-échantilloner ce jeu de données pour réduire sa puissance statistique.

```{r results="verbatim", echo=TRUE}
data("InsectSprays")
InsectSprays_sub = InsectSprays[c(1:3, 13:15, 25:27, 37:39, 49:51, 61:63),]
d = InsectSprays_sub
head(d)
dim(d)
table(d$spray)
```




```{r}
layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
p = plot(d$spray, d$count, main="count~spray", xlab="spray", ylab="count", border="grey")
points(jitter(as.numeric(d$spray)), d$count)


y_i = sapply(levels(d$spray), function(s) {
 mean(d[d$spray==s,]$count)
})
sp = jitter(as.numeric(d$spray))
plot(sp, d$count, main="count~spray", las=2, type="p", xaxt="n", xlab="spray", ylab="count")
suppressWarnings(arrows(sp, d$count, sp, rep(y_i, each=table(d$spray)[1]), col=adjustcolor(4, alpha.f=0.5), length=0.05))
points(y_i, pch=5, col=2)
suppressWarnings(arrows(1:6, mean(y_i), 1:6, y_i, col=adjustcolor(2, alpha.f=0.9), length=0.05))
abline(h=mean(y_i), lty=2)

axis(1, 1:length(levels(d$spray)), labels=levels(d$spray))
legend("top", c("Y_i", "beta_i", "epsilon_i", "mu"), pch=c(1,5,5,NA), col=c(1,2,4,1), lty=c(0,0,0,2), border=0)
```






---

## Estimation des paramètres


Le modèle $Y = \mu + I_X \beta + \epsilon$ devient pour chaque observation $i \in \{1, ..., n\}$ :

$$Y_i = \mu + \beta_i + \epsilon_i$$

avec $\beta_i$ l’effet du groupe auquel appartient la *i*-ème observation.

Dans le cas d’un plan **équi-distribué** : 

  - l’effet d’un groupe se calcul comme la différence entre la moyenne des observations du groupe et la moyenne de toutes les observations.
  - la somme des effets des groupes est nulle.

Dans notre exemple $\beta_i$ prend ses valeurs dans $\{ \beta_A, \beta_B, \beta_C, \beta_D, \beta_E, \beta_F \}$, les moyennes des observations pour les individus des groupes A, B, C, D, E et F. Les groupes sont équilibrés donc $\beta_A + \beta_B + \beta_C + \beta_D + \beta_E + \beta_F = \mu$ 




























---

## Qualité du modèle

### Décomposition de la variance

**Notations**

Plaçons nous dans le cas d’un plan **équi-distribué**. Les observations sont notées $Y_{jk}$ avec :

- $j$ identifie les niveaux du facteur $X$ (dans notre exemple, le type d’insecticide, $j \in J = \{ A, B, C, D, E, F \}$).
- $k$ identifie la *k*-éme observation pour un niveau de facteur donné (dans notre exemple, pour chaque type d’insecticide, $k \in K = \{1,...,3\}$).

Les observations d’un niveau de facteur donné sont appelés des **réplicats**. D’une maniére générale, les réplicats correspondnent à la répétition de la mesure Y pour des conditions expérimentales similaires.

Les groupes sont de même taille, nous disons que l’expérience est équilibrée. 
Au delà du fait que cela simplifie la notation, cela confère de bonnes propriétés utilisées pour décomposer la variance.

La moyenne observée du groupe $j$ s’écrit : 
$$\overline Y_j = \frac{1}{|K|}\sum_{k \in K}Y_{jk}$$

La variance observée du groupe $j$ s’écrit : 
$$s_j^2(Y) = \frac{1}{|K|}\sum_{k \in K}(Y_{jk} -\overline Y_j)^2$$


*Remarque* : Cette dernière formule exprime la variance non corrigée. Très souvent, dans les ouvrages ou les logiciels, c’est la variance corrigée qui est utilisée : au lieu d’être divisée par $|K|$, la somme est divisée par $|K| - 1$.


**Propriétés fondamentales**

La décomposition est fondée sur deux propriétés des moyennes et des variances.

(1) La moyenne de toutes les observations est la moyenne des moyennes de chaque échantillon : 

$$\overline{Y} 
  = \frac{1}{n} \sum_{j \in J} \sum_{k\in K} Y_{jk}
  = \frac{1}{|J| . |K|} \sum_{j \in J} \sum_{k\in K} Y_{jk}
  = \frac{1}{|J|} \sum_{j \in J} \frac{1}{|K|} \sum_{k\in K} Y_{jk}
  = \frac{1}{|J|} \sum_{j \in J} \overline{Y_j} $$


(2) La variance de toutes les observations est la somme de la variance des moyennes de chaque groupe et des variances de chaque observation autour de la moyenne de son groupe :

$$s^2(Y) 
  = \frac{1}{n} \sum_{j \in J} \sum_{k \in K} (Y_{jk} - \overline{Y})^2 
  = \frac{1}{n} \sum_{j \in J} \sum_{k \in K} (Y_{jk} - \overline Y_j)^2 + (\overline Y_j - \overline{Y})^2 
  = \frac{1}{|J|} \sum_{j \in J} (\overline{Y_j} - \overline{Y})^2 + \frac{1}{|J|} \sum_{j \in J} s_j^2(Y) $$



---

### $R^2$

On conserve la proriété vu dans la regression linéaire :
 
$$ SC_{tot} = SC_{F} + SC_{res}$$ 

Avec $SC_{tot}$ la somme des carrés totale,  $SC_{F}$ la somme des carrés des facteurs et  $SC_{res}$ la somme des carrés des résidus.


Le coefficient de determination $R^2$ mesure du pourcentage de la variance expliquée par le modèle :

$$ R^2 = \frac{SC_{F}}{SC_{tot}}$$


```{r}
d = InsectSprays_sub

y_i = sapply(levels(d$spray), function(s) {
 mean(d[d$spray==s,]$count)
})
layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
sp = jitter(as.numeric(d$spray))
plot(sp, d$count, main="count~spray", las=2, type="p", xaxt="n", xlab="spray", ylab="count")
suppressWarnings(arrows(sp, d$count, sp, rep(y_i, each=table(d$spray)[1]), col=adjustcolor(4, alpha.f=0.5), length=0.05))
points(y_i, pch=5, col=2)
suppressWarnings(arrows(1:6, mean(y_i), 1:6, y_i, col=adjustcolor(2, alpha.f=0.9), length=0.05))
abline(h=mean(y_i), lty=2)

axis(1, 1:length(levels(d$spray)), labels=levels(d$spray))
legend("top", c("Y_ij", "SC_F", "SC_res"), pch=c(1,5,5), col=c(1,2,4))
```






**Travaux pratiques**

  - Reprendre le travail effectué sur le jeu de données *CO2*.
  - Considérer la concentration en CO2 comme un facteur
  - Evaluer la part de variance expliquée par ce nouveau modèle (R^2)
  - Commenter


```{r echo=TRUE, results="verbatim"}
data(CO2)
d = CO2[CO2$Type=="Quebec", ]
d$conc_log = log10(d$conc)
d$conc_fact = as.factor(d$conc)
print(d$conc_fact)
m1 = lm(uptake~conc, d)
m2 = lm(uptake~conc_log, d)
# m3 = lm(uptake~...
layout(matrix(1:3, 1), respect=TRUE)
plot(d$conc, d$uptake, main=paste0("uptake~conc R^2=", signif(summary(m1)$r.squared, 2)*100, "%"))
abline(m1, col=2)
plot(d$conc_log, d$uptake, main=paste0("uptake~conc_log R^2=", signif(summary(m2)$r.squared, 2)*100, "%"))
abline(m2, col=2)
plot(d$conc_fact, d$uptake, main=paste0("uptake~conc_fact R^2=", "??", "%"))
```


[correction_co2_anova.R](./correction_co2_anova.R)  

---




























## Tests statistiques


### Test de   Fisher

Le test de l’ANOVA à 1 facteur (*one way ANOVA*), s’applique sous certaines conditions : 

- Indépendance des observations
- Homoscedasticité entre les groupes
- Normalité des résidus,

l’ANOVA est un test de comparaison des moyennes des facteurs : 


$$\left  \lbrace
\begin{array}{l}
H_0,\mu_1=\mu_2=...=\mu_I \\
H_1, \text{les moyennes } \mu_i \text{ ne sont pas toutes égales.}
\end{array}
\right.$$




```{r}
d = InsectSprays_sub

y_i = sapply(levels(d$spray), function(s) {
 mean(d[d$spray==s,]$count)
})
layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
sp = jitter(as.numeric(d$spray))
plot(sp, d$count, main="count~spray", las=2, type="p", xaxt="n", xlab="spray", ylab="count")
suppressWarnings(arrows(sp, d$count, sp, rep(y_i, each=table(d$spray)[1]), col=adjustcolor(4, alpha.f=0.5), length=0.05))
points(y_i, pch=5, col=2)
suppressWarnings(arrows(1:6, mean(y_i), 1:6, y_i, col=adjustcolor(2, alpha.f=0.9), length=0.05))
abline(h=mean(y_i), lty=2)

axis(1, 1:length(levels(d$spray)), labels=levels(d$spray))
legend("top", c("Y_ij", "SC_F", "SC_res"), pch=c(1,5,5), col=c(1,2,4))
```


Pour comparer ces quantités, R. A. Fisher, après les avoir
"corrigées" par leurs degrés de liberté (ddl), a considéré leur rapport.

Nous appelons *carré moyen associé au facteur* le terme 

$$CM_F = \frac{SC_F}{|J|-1}$$

et *carré moyen résiduel* le terme :

$$CM_{res} = \frac{SC_{res}}{n-|J|}$$

Le carré moyen résiduel est un estimateur sans biais de la variance des erreurs  $\sigma^2$.
C’est pourquoi il est souvent également appelé variance résiduelle et presque systématiquement noté $S_{res}^2$ lorsqu’il sert à estimer la variance des erreurs.
Sa valeur observée sur l’échantillon est ainsi notée $CM_{res}$ ou $s_{res}^2$ .

Définissons la statistique de test :

$$F_{obs} = \frac{CM_F}{CM_{res}}$$

Si les conditions d’aplications sont satisfaites et sous $H_0$ alors $F_{obs}$ est une réalisation d’une variable aléatoire $F$ qui suit une loi de Fisher à $|J|-1$ degrés de liberté au numérateur et $n-|J|$ degrés de liberté au dénominateur. Cette loi est notée  $\mathcal{F}_{|J|-1,n-|J|}$.

Ces informations sont résumées dans ce que le tableau de l’analyse de la variance : 

| Source de variation   | SC        | ddl  |  CM  | $F_{obs}$
| :-------------------- | :-------: | :--: | :--: | ---:
| Due au facteur        | $SC_{F}$  | $|J|-1$  | $CM_{F}  =\frac{SC_{F}}{|J|-1}$ | $\frac{CM_{F}}{CM_{res}}$
| Résiduelle            | $SC_{res}$| $n-|J|$  | $CM_{res}=\frac{SC_{res}}{n-|J|}$ 
| Totale                | $SC_{tot}$| $n-1$ 


```{r echo=TRUE, results="verbatim"}
d = InsectSprays_sub
m = lm(count~spray,d)
shapiro.test(m$residuals)
bartlett.test(count~spray,d)
anova(m)
summary(m)
```

```{r}
layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
p = plot(d$spray, d$count, main="count~spray", xlab="spray", ylab="count", border="grey")
points(jitter(as.numeric(d$spray)), d$count)



x=seq(0,15, length.out=200)
plot(x, df(x, anova(m)[1,1], anova(m)[2,1]), type="l", xlab="F", ylab="", main=paste0("Distibution de Fisher,  ", anova(m)[1,1]," et ", anova(m)[2,1]," ddl."))
abline(v=anova(m)[1,4], lty=2, col=4)
abline(v=qf(0.95, anova(m)[1,1], anova(m)[2,1]), lty=2, col=2)
x = seq(15,qf(0.95, anova(m)[1,1], anova(m)[2,1]), length.out=100)
polygon(c(15, x, qf(0.95, anova(m)[1,1], anova(m)[2,1])), c(0, df(x, anova(m)[1,1], anova(m)[2,1]), 0), col=adjustcolor(2, alpha.f=.5), border=2, lwd=3)
x = seq(15,anova(m)[1,4], length.out=100)
polygon(c(15, x, anova(m)[1,4]), c(0, df(x, anova(m)[1,1], anova(m)[2,1]), 0), col=adjustcolor(4, alpha.f=.5), border=4, lwd=3)
legend("topright",c("F_obs", "c_alpha", "p-value", "5%"), col=c(4,2, 4, 2), lty=c(2,2,1,1), lwd=c(1,1,3,3), cex=.8)
```










## Pour aller plus loin













### Test de Tuckey

Lorsque que nous rejettons $H_0$, nous pouvons chercher à analyser les différences entre les groupes. 
Nous procédons alors à des tests qui vont répondre aux questions suivantes : 

  - D’où vient la différence ? 
  - Quelles moyennes sont différentes ?
  

Ces tests qui vont répondre à cette question sont les tests de comparaisons multiples, des adaptations du test de Student.


En pratique, pour répondre à ces questions on utilise le test de **Tuckey**.
  
```{r echo=TRUE, results="verbatim"}
layout(matrix(1:2,1), respect=TRUE)
d = InsectSprays_sub
m = aov(count~spray,d)
plot(TukeyHSD(m, conf.level=0.95), las=2)

layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
d = InsectSprays
m = aov(count~spray,d)
plot(TukeyHSD(m, conf.level=0.95), las=2)
p = plot(d$spray, d$count, main="count~spray", xlab="spray", ylab="count", border="grey")
points(jitter(as.numeric(d$spray)), d$count)
```

On peut aussi réaliser plusieurs Test de Student et appliqué une correction

La plus connues est la correction de **Bonferroni**. Cela consiste à diviser le seuil par le nombre de comparaisons. 
Bonferroni a montré que cette procédure garantit un taux d’erreur global plus faible que le seuil initial (conservatif).  


```{r echo=TRUE, results="verbatim"}
d = InsectSprays_sub
pairwise.t.test(d$count, d$spray, p.adjust.method="none") 
pairwise.t.test(d$count, d$spray, p.adjust.method="bon") 
```


Une autre méthode utilisée quand le nombre de tests multiples à réalisé est très grand est la procédure de contrôle du taux de fausses découverte (FDR) de **Benjamini-Hochberg** [Y. Benjamini, Y. Hochberg. *Controlling the false discovery rate : a practical and powerful approach to multiple testing*. Journal of the Royal Statistical Society, Series B (Methodological), 1995, p289-300.].



### Paired t.test

Le test T apparié peut-être vu comme une ANOVA à deux facteurs : 


```{r echo=TRUE, results="verbatim"}
d = sleep
boxplot(extra~group, d)
t.test(sleep$extra[1:10], sleep$extra[11:20], var.equal=TRUE)
m = lm(extra~group, d)
summary(m)

t.test(sleep$extra[1:10], sleep$extra[11:20], var.equal=TRUE, paired=TRUE)
matplot(t(cbind(sleep$extra[1:10], sleep$extra[11:20])))
matlines(t(cbind(sleep$extra[1:10], sleep$extra[11:20])))
m = lm(extra~group+ID, d)
summary(m)
```



### Tableau disjonctif

*Remarque* : on peut aussi écrire ce test à l’aide d’un tableau disjonctif : 

```{r echo=TRUE, results="verbatim"}
d = InsectSprays_sub
d$A = 0
d$B = 0
d$C = 0
d$D = 0
d$E = 0
d$F = 0
for (i in 1:nrow(d)) {
  d[i,as.character(d[i,"spray"])] = 1
}
d
m0 = lm(count~A+B+C+D+E+F,d)
summary(m0)
m00 = lm(count~A+B+C+D+E,d)
summary(m00)
anova(m00)
m = lm(count~spray,d)
anova(m)
```















### Terme d'interaction

```{r, eval=TRUE, echo=TRUE, results="verbatim"}
options(contrasts=c("contr.treatment", "contr.poly"))
sleepstudy = lme4::sleepstudy
sleepstudy = sleepstudy[sleepstudy$Days>1,]
d = sleepstudy
head(d)
dim(d)
table(d$Subject, d$Days)
layout(1, respect=TRUE)
plot(Reaction~Days, d, pch=16, col=d$Subject)

m0 = lm(Reaction~Days, d)
m1 = lm(Reaction~Days+Subject, d)
m2 = lm(Reaction~Days*Subject, d)

layout(1, respect=TRUE)

m0 = lm(Reaction~Days, d)
summary(m0)
plot(Reaction~Days, d, pch=16, col=d$Subject)
abline(m0)

m1 = lm(Reaction~Days+Subject, d)
summary(m1)
plot(Reaction~Days, d, pch=16, col=d$Subject)
abline(a=m1$coef[1]             , b=m1$coef[2], col=1)
n = length(levels(d$Subject))
for (i in 2:n) {
  abline(a=m1$coef[1] + m1$coef[i+1], b=m1$coef[2], col=i)  
}

m2 = lm(Reaction~Days*Subject, d)
summary(m2)
plot(Reaction~Days, d, pch=16, col=d$Subject)
abline(a=m2$coef[1]             , b=m2$coef[2]              , col=1)
n = length(levels(d$Subject))
for (i in 2:n) {
  abline(a=m2$coef[1] + m2$coef[i+1], b=m2$coef[2] + m2$coef[n+i] , col=i)
}


```

**Souris, Regime et exercice**

```{r}
d = rbind(
  c(1,    1,   15) ,
  c(1,    1,   10) ,
  c(0,    1,   25) ,
  c(0,    1,   20) ,
  c(1,    0,   20) ,
  c(1,    0,   15) ,
  c(0,    0,   30) ,
  c(0,    0,   25)
)

layout(1)
colnames(d) = c("R", "E" , "P")
d = data.frame(d)
boxplot(P~R+E, d)
m = lm(P~R+E, d)
summary(m)
m = lm(P~R*E, d)
summary(m)

d = rbind(
  c(1,    1,   10) ,
  c(1,    1,   5) ,
  c(0,    1,   25) ,
  c(0,    1,   20) ,
  c(1,    0,   20) ,
  c(1,    0,   15) ,
  c(0,    0,   30) ,
  c(0,    0,   25)
)


layout(1)
colnames(d) = c("R", "E" , "P")
d = data.frame(d)
boxplot(P~R+E, d)
m = lm(P~R+E, d)
summary(m)
m = lm(P~R*E, d)
summary(m)

```



























































### Exercices



Etudier les jeux de données suivants : 


1. Ecriture analytique du modèle ;
2. Formulation des hypothèses $\mathcal{H}_0$ et $\mathcal{H}_1$ ;
3. Statistique de test utilisée et **conditions d’application** ; 
4. Valeur de la statistique de test ; 
5. P-valeur associée ; 
6. Conclusion du test.
7. Tests de comparaisons multiples à l’aide de la méthode de votre choix.



**faraway::coagulation**

```{r echo=TRUE, results="verbatim"}
d = faraway::coagulation
head(d)
dim(d)
table(d$diet)

# Graphicals
layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
plot(d$diet, d$coag, main="coag~diet", xlab="diet", ylab="coag", border="grey")
points(jitter(as.numeric(d$diet)), d$coag)
```


[correction_coagulation.R](./correction_coagulation.R)








**ISwR::lung**

On s’intéresse à l’effet de la méthode sur la mesure du volume.

```{r, eval=TRUE, echo=TRUE, results="verbatim"}
# Note sur les contrasts
# https://www.dummies.com/programming/r/how-to-set-the-contrasts-for-your-data-with-r/
# X = levels(rats$poison)
# contr.treatment(X)
# contr.sum(X)
options(contrasts=c("contr.sum", "contr.poly")) # indique que, dans le cas équilibré, les estimations des effets alpha de chaque groupe doivent être de somme nulle
# options(contrasts=c("contr.treatment", "contr.poly"))

d = ISwR::lung
head(d)
dim(d)
table(d$method, d$subject)
layout(matrix(1:2,1), respect=TRUE)
plot(volume~method+subject, d)

```

[correction_anova_lung.R](./correction_anova_lung.R)



**auto.noise**

On s’intéresse à l’effet du type de pot sur le bruit.

```{r, eval=TRUE, echo=TRUE, results="verbatim"}
library(lsmeans)
d = auto.noise
head(d)
dim(d)
table(d$size, d$type)
layout(matrix(1:2,1), respect=TRUE)
plot(noise~size+type, d)

```


[correction_anova_auto.noise.R](./correction_anova_auto.noise.R)






**faraway::rats**

On s’intéresse à l’efficacité du treatment pour chaque type de poison.


```{r, eval=TRUE, echo=TRUE, results="verbatim"}
d = faraway::rats
head(d)
dim(d)
table(d$poison, d$treat)
layout(matrix(1:2,1), respect=TRUE)
plot(time~poison+treat, d)
```



[correction_anova_rats.R](./correction_anova_rats.R)























**Autres jeux de données** 





```{r echo=TRUE, results="verbatim"}
d = multcomp::recovery
head(d)
dim(d)
table(d$blanket)
layout(1, respect=TRUE)
plot(minutes~blanket, d)

d = multcomp::cholesterol
head(d)
dim(d)
table(d$trt)
layout(1, respect=TRUE)
plot(response~trt, d)

d = MASS::anorexia
head(d)
dim(d)
table(d$Treat)
d$delta = d$Postwt - d$Prewt 
layout(matrix(1:2,1), respect=TRUE)
plot(d$Postwt, d$Prewt)
plot(delta~Treat, d)
```










```{r, eval=TRUE, echo=TRUE, results="verbatim"}
Penicillin = lme4::Penicillin
head(Penicillin)
dim(Penicillin)
table(Penicillin$plate, Penicillin$sample)
```

```{r, eval=TRUE, echo=TRUE, results="verbatim"}
cake = lme4::cake
head(cake)
dim(cake)
table(cake$temperature, cake$recipe)
```











```{r, eval=FALSE}
boxplot(diameter~plate+sample,Penicillin, las=2)
m = lm(diameter~plate+sample,Penicillin)
shapiro.test(m$residuals)
anova(m)
summary(m)
```

```{r, eval=FALSE}
m = lm (yield~variety+fungicide, varfung)
anova(m)
m = lm (yield~fungicide+variety, varfung)
anova(m)

d = state_us
m = lm(Life.Exp ~ Murder + HS.Grad, d)
anova(m)
m = lm(Life.Exp ~ HS.Grad + Murder, d)
anova(m)






Y~A+B



# equirépété
df = data.frame(
  A=c(0, 0, 1, 1, 2, 2, 0, 1, 2),
  B=c(0, 1, 0, 1, 0, 1, 2, 2, 2)
)
table(df$A, df$B)
plot(df$A, df$B)
df

df$Y = 6*df$A + 3*df$B + rnorm(nrow(df))
df 

m = lm(Y~A+B, df)
anova(m)
m = lm(Y~B+A, df)
anova(m)


# factors
df$A = as.factor(df$A)
df$B = as.factor(df$B)


m = lm(Y~A+B, df)
anova(m)
m = lm(Y~B+A, df)
anova(m)



# pas equirépété (correlation entre A et B)
df = data.frame(
  A=c(0, 0, 1, 1, 2, 1, 2),
  B=c(0, 1, 0, 1, 1, 2, 2)
)
table(df$A, df$B)
plot(df$A, df$B, col=2)
df

df$Y = 6*df$A + 3*df$B + rnorm(nrow(df))
df 

m = lm(Y~A+B, df)
anova(m)
m = lm(Y~B+A, df)
anova(m)












# pas equilibré equirépété (correlation entre A et B)
df = data.frame(
  A=c(0, 1, 2, 0, 2),
  B=c(0, 1, 0, 2, 2)
)
table(df$A, df$B)
plot(df$A, df$B, col=2, pch=16, cex=3)
df

df$Y = 6*df$A + 3*df$B + rnorm(nrow(df))
df 

m = lm(Y~A+B, df)
anova(m)
m = lm(Y~B+A, df)
anova(m)





replicat
df = rbind(df, df, df, df, df, df, df, df)
set.seed(1)
df$Y = 6*df$A+rnorm(nrow(df))+3*df$B*rnorm(nrow(df))

# factors
df$A = as.factor(df$A)
df$B = as.factor(df$B)


m = lm(Y~A+B, df)
anova(m)
m = lm(Y~B+A, df)
anova(m)
```












