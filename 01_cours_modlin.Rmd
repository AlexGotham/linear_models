# Régression linéaire simple

L’objectif de cette section est : 

  - d’introduire la régression linéaire comme **modèle statistique** (formulation, paramètres, adéquation aux données)
  - d’interroger le modèles grace à des **tests statistiques** (Student, Fisher)
  
---




On cherche une relation entre la variable observée $Y$ et la variable explicative $x$. 

$$ Y \sim x $$

Les motivations peuvent être :

  - modèle explicatif
  - modèle prédictif
  
Domaines d’application : chimie, astronomie, sismologie, économie, démographie, épidémiologie, géographie, écologie, géologie, physique, médecine...

Historiquement, le modèle linéaire a été développé par Ronald Aylmer Fisher, avec applications en génétique et en agronomie.

---

## Présentation du modèle

Considérons $x$ et $Y$ deux variables. $Y$ est la variable expliquée (modélisée) par la variable $x$. 

**Important**, dans le cadre des modèles linéaire Y est *quantitatives* (Ex : age, poids, taille...).

Dans le cas de la regression linéaire simple, $x$ est également quantitatives et la relation entre $Y$ et $x$ est une fonction **affine** (ou **linéaire**) de $x$ (Cf. programme du brevet des collèges).

\begin{eqnarray}
              \text{f : } \mathbb{R} & \rightarrow & \mathbb{R} \\
                                 x & \rightarrow & f(x)  =  ax + b \\ 
\end{eqnarray}

**Attention**, la fonction *abline* de R utilise la formule $bx+a$ ! Dans ce cours nous utliserons la notation $f(x)  = \beta_0 + \beta_1 x$



```{r  echo=FALSE, results="hide"}
data_nutri = read.table("data/data_nutri.csv", header=TRUE, sep=",", row.names = 1)
set.seed(1)
data_nutri_20 = data_nutri[sample(1:nrow(data_nutri), 20), ]
d = data_nutri_20

layout(matrix(1:2, 1), respect=TRUE)

plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille")
m = lm(poids~taille, d)
m$coefficients
legend("topleft", c("observations", "regression line"), pch=c(1, NA), col=1:2, lty=0:1)
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille", xlim=c(0,max(d$taille)), ylim=c(m$coefficients[[1]]-20, max(d$poids)))
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
# points(0,m$coefficients[[1]], pch=16)
abline(h=m$coefficients[[1]], v=0, lty=2)
axis(2, at=m$coefficients[[1]], "beta_0", las=2)


segments(50, predict(m, data.frame(taille=50)), 100, predict(m, data.frame(taille=50)), lty=2)
segments(100, predict(m, data.frame(taille=50)), 100, predict(m, data.frame(taille=100)), lty=2)
text(75, predict(m, data.frame(taille=50)), "50", pos=1)
text(100, predict(m, data.frame(taille=75)), "50*beta_1", pos=4)

```



Les $n$ observations vont permettre :

  - de construire le modèle (définir $\beta_0$ et $\beta_1$),
  - de quantifier l’adéquation du modèle aux données.


Remarques : 

  - On utilise un modèle linéaire Gaussien pour des observations pouvant être modélisées par une loi normales. Pour d’autres distributions (Poisson, Bernoulli...), on utilisera un modèle linéaire généralisé (GLM).
  - La régression linéaire se caractérise par des variables explicatives **quantitatives** (*e.g.*, $poids \sim taille$). L’ANOVA se caractérise par des variables explicatives  **qualitatives** (*e.g.*, $poids \sim sexe$).

---

**Exemple d’une relation déterministe**

La température $x$ en degrés Celsius explique entièrement la température  $y$ en degrés Farenheit par la relation : $y=32 + 9/5 x$.

\begin{eqnarray}
              \text{f : } \mathbb{R} & \rightarrow & \mathbb{R} \\
                                 x & \rightarrow & f(x)  = \beta_0 + \beta_1 x = 32 + 9/5 x \\ 
                                 0 & \rightarrow & f(0)  = 32 \\ 
                                 10 & \rightarrow & f(10)  = 50 \\ 
\end{eqnarray}



```{r  echo=FALSE, results="hide"}
layout(1, respect=TRUE)
beta_0 = 32
beta_1 = 9/5
d_f = -10:20
plot(d_f, beta_0 + beta_1 * d_f, main="Température", xlab="deg. C", ylab="deg. F", col=0)
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
abline(v=c(0,10), h=c(32,50), lty=2)

```



---

**Exemple d’une relation stochastique**


Dans le cas d’une variable aléatoire $Y$, il faut chercher la droite qui ajuste le mieux les observations et on ajoute un terme qui portera la partira non déterministe des observations.

$$Y   = \beta_0 + \beta_1x + \epsilon $$

$$Y_i   = \beta_0 + \beta_1x_i + \epsilon_i \ \ \ \ \  \forall i \in {1, ..., n}$$


où $\beta_0$ et $\beta_1$ sont des réels fixes, mais inconnus, et $\epsilon_i$ une valeur caractérisant le comportement individuel de l’observation $i$ (résidus).

Modèle : $$\mathbb{E} (Y|x) = \widehat Y = f(x) = \beta_0 + \beta_1 x$$

Par exemple $x$ est la taille, $Y$ le poids, à une même taille, plusieurs poids peuvent correspondrent. Les données ne sont plus alignées, mais s’ajustent autour de la droite de régression laissant apparaitre les résidus.

```{r}
layout(1, respect=TRUE)

plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille")
m = lm(poids~taille, d)
m$coefficients
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)
legend("topleft",c("E(Y)", "epsilon"), col=c(2,4), lty=1, cex=.8)
```



---

## Estimation des paramètres

**Question** : comment estimer les paramètres de la droite de regression ?

**Réponse** : Par la méthode des moindres carrés.

---

**Droite de regression, estimation et résidus**



Pour un individu $i$, on a :

$$Y_i=\beta_0+ \beta_1 x_i + \epsilon_i$$
 
$$Y_i=\widehat{\beta}_0+ \widehat{\beta}_1x_i + e_i$$

$$\widehat Y_i=\widehat{\beta}_0+ \widehat{\beta}_1x_i$$

$$Y_i = \widehat Y_i + e_i$$

- $x_i$ est la valeur de la varible explicative , $Y_i$ est la valeur observée de la variable aléatoire
- $\beta_0$ et $\beta_1$ sont les paramètre du modèle, ils sont inobservable (il faudrait avoir accès a l’exhaustivité de la population de référence)
- $\widehat{\beta}_0$ et $\widehat{\beta}_1$ sont des estimnateur de ces parametres (estimés à partir d’un échantillon de la population de référence)
- la composante aléatoire de $Y_i$ est $\epsilon_i$ (inobservable) estimé par $e_i$ (les **résidus du modèle**.)
- la valeur de $Y_i$ prédite par le modèle est $\widehat Y_i$.

La plupart des méthodes d’estimation cherchent à définir une droite qui minimise une fonction de résidus, la plus connue : **la méthode des moindres carrés ordinaires** (MCO).



```{r echo=FALSE}
layout(1, respect=TRUE)
plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille", col=adjustcolor(1, alpha.f=0.5))
m = lm(poids~taille, d)
# abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
pred = predict(m, data.frame(taille=d$taille))
i = which(d$poids - pred == max(d$poids - pred))
points(d$taille[i], d$poids[i], pch=16, col=2)
abline(v=d$taille[i], h=c(d$poids[i], d$poids[i]-m$residuals[i]), lty=2, col=2)
axis(3, at=d$taille[i], "xi", las=2, col=2)
axis(4, at=c(d$poids[i], d$poids[i]-m$residuals[i]), c("Yi", expression(hat("Yi"))), las=2, col=2)
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=1) # /!\ y = b.x + a
arrows(d$taille[i], d$poids[i]-m$residuals[i], d$taille[i], d$poids[i], col=4, length=0.1, lwd=2)
text(d$taille[i], d$poids[i]-m$residuals[i]/2, "e_i", col=4, las=2, pos=2, cex=2)
```

---

**Intutition des moindres carrés**


Pour l’instant, la droite de régression est inconnue.
Le problème est d’estimer $\beta_0$ et $\beta_1$ à partir d’un échantillon de données.

On essaie de déterminer la **droite de regression** qui approche le mieux les données : 

$$\widehat Y_{\beta_0, \beta_1}(x) = \widehat{\beta}_0 + \widehat{\beta}_1x$$

On définit l’erreur quadratique moyenne (EQM) comme la moyenne du carré des résidus $EQM_{\beta_0, \beta_1} = \frac{1}{n}\sum^n_{i=1} e_{i,\beta_0, \beta_1}^2$ avec :

$$e_{i,\beta_0, \beta_1} = Y_i - \widehat Y_{\beta_0, \beta_1}(x_i)$$


Remarque : en première instance on peut estimer $\beta_0$ par la moyenne des $Y$ observés, *i.e.* $\overline{Y} = \frac{1}{n}\sum^n_{i=1}Y_i$. On obtient ainsi un premier modèle avec $\beta_0 = \overline{Y}$ et $\beta_1 = 0$. Ce modèle correspond au modèle dit *nul*, *i.e*, le modèle pour lequel $Y$ ne depend pas de $x$ ($\beta_1 = 0$), $\widehat Y_{nul}(x)=\overline{Y}$


```{r echo=TRUE}
d = data_nutri_20
layout(1, respect=TRUE)
beta_0 = mean(d$poids)
beta_1 = 0
res =  d$poids - (beta_0 + beta_1*d$taille)
eqm = sum(res^2) / length(res)
plot(d$taille, d$poids, main=paste0("poids~taille, EQM=", round(eqm)), ylab="poids", xlab="taille")
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
arrows(d$taille, d$poids, d$taille, d$poids-res, length=0.1, col=adjustcolor(4, alpha.f=0.5))
```

```{r echo=TRUE}
layout(matrix(1:2, 1), respect=TRUE)
beta_0 = -100
beta_1 = 1
res =  d$poids - (beta_0 + beta_1*d$taille)
eqm = sum(res^2) / length(res)
plot(d$taille, d$poids, main=paste0("poids=", signif(beta_0, 3), "+", signif(beta_1, 3), " taille, EQM=", round(eqm)), ylab="poids", xlab="taille")
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
arrows(d$taille, d$poids, d$taille, d$poids-res, length=0.1, col=adjustcolor(4, alpha.f=0.5))

beta_0 = -16
beta_1 = 0.5
res =  d$poids - (beta_0 + beta_1*d$taille)
eqm = sum(res^2) / length(res)
plot(d$taille, d$poids, main=paste0("poids=", signif(beta_0, 3), "+", signif(beta_1, 3), " taille, EQM=", round(eqm)), ylab="poids", xlab="taille")
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
arrows(d$taille[res!=0], d$poids[res!=0], d$taille[res!=0], (d$poids-res)[res!=0], length=0.1, col=adjustcolor(4, alpha.f=0.5))
```




```{r echo=TRUE}
layout(1, respect=TRUE)
results = list()
for(beta_0 in 0:-260) {
  for(beta_1 in seq(0,2, length.out=21)) {
    res =  d$poids - (beta_0 + beta_1*d$taille)
    eqm = sum(res^2) / length(res)
    results[[length(results)+1]] = c(beta_0=beta_0, beta_1=beta_1, eqm=eqm)
  }
}
results = as.data.frame(do.call(rbind, results))
# plot(results$beta_0, results$beta_1, pch=16, col=colorRampPalette(c("cyan", "black", "red"))(100)[as.numeric(cut(results$eqm, breaks=quantile(results$eqm, probs=(0:100)/100), include.lowest=TRUE))])
r = results[results$eqm==min(results$eqm),]
beta_0 = r[["beta_0"]]
beta_1 = r[["beta_1"]]
res =  d$poids - (beta_0 + beta_1*d$taille)
eqm = sum(res^2) / length(res)
plot(d$taille, d$poids, main=paste0("poids=", signif(beta_0, 3), "+", signif(beta_1, 3), " taille, EQM=", round(eqm)), ylab="poids", xlab="taille")
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
arrows(d$taille[res!=0], d$poids[res!=0], d$taille[res!=0], (d$poids-res)[res!=0], length=0.1, col=adjustcolor(4, alpha.f=0.5))
```



---

**Méthode  et démonstration**

L’objectifest de définir des estimateurs qui minimisent la somme des carrés des résidus (ou **erreur quadratique moyenne** EQM).

Les estimateurs sont donc les coordonnées du minimum de la fonction à deux variables :

$$EQM(\beta_0, \beta_1) = \frac{1}{n}\sum^{n}_{i=1} e^2_i = \sum^{n}_{i=1} (Y_i - \widehat Y_i)^2 = \sum^{n}_{i=1} (Y_i - \widehat\beta_0 -  \widehat\beta_1x_i)^2$$

Cette fonction est appelée la **fonction objectif**.


Les estimateurs correspondent aux valeurs annulant les dérivées partielles de cette fonction :

$$\frac{\delta EQM(\beta_0, \beta_1)}{\delta \beta_0} = -2 \sum (Y_i - \beta_0 - \beta_1x_i)$$
$$\frac{\delta EQM(\beta_0, \beta_1)}{\delta \beta_1} = -2 \sum x_i (Y_i - \beta_0 - \beta_1x_i)$$

Les estimateurs $(\widehat\beta_0^{MCO},\widehat\beta_1^{MCO})$ sont les solutions du système d’équation :

\begin{equation}
    \begin{cases}
    -2 \sum (Y_i - \beta_0 - \beta_1x_i) = 0  \\
    -2 \sum x_i (Y_i - \beta_0 - \beta_1x_i) = 0
    \end{cases}\,.
\end{equation}


Soient les équations suivantes : 

$$(1) \sum{Y}_i = n\widehat\beta_0 + \widehat\beta_1 \sum{x_i} $$

$$(2) \sum{x_iY_i} = \widehat\beta_0 \sum{x_i} + \widehat\beta_1 \sum{x^2_i}$$
On utilise les notations usuelles des moyennes empiriques $$\overline{x}_n = \frac{1}{n}\sum{x_i} ,\ \ \ \ \ \ \overline{Y}_n = \frac{1}{n}\sum{Y_i}$$

D’après (1): $$\widehat\beta_0 = \overline{Y}_n - \widehat\beta_1 \overline{x}_n$$

D’après (2) :

\begin{eqnarray}
\widehat\beta_1 \sum{x^2_i} & = & \sum{x_iY_i} - \widehat\beta_0 \overline{x}_n \\
                            & = & \sum{x_iY_i} - n\overline{x}_n\overline{Y}_n + \widehat\beta_1 (\overline{x}_n)^2
\end{eqnarray}

Ainsi $$\widehat\beta_1 = \frac{\sum{x_iY_i} -  n\overline{x}_n\overline{Y}_n}{\sum{x_i^2} -  n(\overline{x}_n)^2}$$

Et comme nous avons :

\begin{eqnarray}
\sum{(x_i-\overline{x}_n)(Y_i-\overline{Y}_n)} & = & \sum{x_iY_i} -  n\overline{x}_n\overline{Y}_n \\
                   \sum{(x_i-\overline{x}_n)^2}   & = & \sum{x_i^2} -  n(\overline{x}_n)^2
\end{eqnarray}

Nous obtenons : $$\widehat\beta_1 = \frac{\sum{(x_i-\overline{x}_n)(Y_i-\overline{Y}_n)}}{\sum{(x_i-\overline{x}_n)^2}}=\frac{cov(x,Y)}{var(x)}$$

Dans la pratique, nous calculons $\widehat\beta_1$ puis $\widehat\beta_0$.

Nous obtenons ainsi une estimation de la droite de régression, appelée la droite des moindres carrés ordinaires :

$$\widehat Y(x) = \widehat\beta_0 + \widehat\beta_1x$$


$\widehat\beta_0$ correspond à l’ordonnée à l’origine. Ce paramètre n’est pas toujours interprétable (il dépend de la signifaction de $X$ et du fait que $X$ soit centré ou non).

$\widehat\beta_1$ correspond à la pente.

Attention, la technique des MCO crée des estimateurs sensibles aux valeurs atypiques.

---

**Mise en oeuvre sour R**

```{r echo=TRUE, results="verbatim"}
layout(1, respect=TRUE)
plot(d$taille, d$poids, main="poids~taille")
## Model
# Y~X
# E(Y) = b.X
# E(Y) = b_0 + b_1.X
# Y_i = b_0 + b_1.X_i + e_i
m = lm(poids~taille, d)
m$coefficients
abline(a=m$coef[[1]], b=m$coef[[2]], col=2) # /!\ y = b.x + a
# residuals
# head(m$residuals)
arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)
```

---

## Qualité du modèle

### Décomposition de la variance


La variation de $Y$ vient du fait de sa dépendance à la variable explicative $X$. 
C’est la **variation expliquée par le modèle**.

Dans l’exemple $poids \sim taille$, nous avons remarqué que lorsque nous mesurons $Y$ avec une même valeur de $X$, nous observons une certaine variation sur $Y$. 
C’est la **variation inexpliquée par le modèle**.

On dit que la *variation totale de Y* se décompose en *Variation expliquée par le modèle* et *variation inexpliquée par le modèle*

Soit : 


\begin{eqnarray}
Y_i - \overline{Y}_n &=& Y_i - \widehat Y_i + \widehat Y_i - \overline{Y}_n \\
                    &=& (Y_i - \widehat Y_i) + (\widehat Y_i - \overline{Y}_n)
\end{eqnarray}

avec $\widehat Y_i - \overline{Y}_n$ la différence expliquée par le modèle et $Y_i - \widehat Y_i$ la différence inexpliquée (ou résiduelle).

Si l’on considèrere la somme des carrés de cette différence on obtient : 

\begin{eqnarray}
\sum(Y_i - \overline{Y}_n)^2 &=& \sum((Y_i - \widehat Y_i) + (\widehat Y_i - \overline{Y}_n))^2 \\
&=& \sum(Y_i - \widehat Y_i)^2 + \sum(\widehat Y_i - \overline{Y}_n)^2 + 2 \sum(Y_i - \widehat Y_i) (\widehat Y_i - \overline{Y}_n) \\
&=& \sum(\widehat Y_i - \overline{Y}_n)^2 + \sum(Y_i - \widehat Y_i)^2
\end{eqnarray}

Soit : 

$$ SC_{tot} = SC_{reg} + SC_{res}$$ 

Avec $SC_{tot}$ la somme des carrés totale,  $SC_{reg}$ la somme des carrés due à la régression et  $SC_{res}$ la somme des carrés des résidus.

La variance totale se décompose en variance expliquée et variance résiduelle.

Notons que $2 \sum(Y_i - \widehat Y_i) (\widehat Y_i - \overline{Y}_n)=0$ est une propriété des moindres carrés [[Théorème 1.2 +  demo](https://www.unine.ch/files/live/sites/statistics/files/shared/documents/cours_modeles_regression.pdf)].


### $R^2$

Le **coefficient de determination $R^2$** mesure le pourcentage de la variation totale expliquée par le modèle.

$$ R^2 = \frac{SC_{reg}}{SC_{tot}}$$

- $R^2$ est compris entre 0 et 1.
- $R^2 = 1$ : cas où les points sont parfaitement alignées
(comme c’est le cas pour un modèle déterministe).
- $R^2 = 0$ : cas où la variation de $Y$ n’est pas due à la variation de $X$.
- Plus $R^2$  est proche de 1, plus les points sont prochesde la droite de régression.



```{r}
layout(matrix(1:2, 1), respect=TRUE)
m = lm(d$poids~d$taille)
plot(d$taille, d$poids, main=paste0("poids~taille R^2=", signif(summary(m)$r.squared, 2)*100, "%"))
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)

m = lm(d$poids~d$age)
plot(d$age, d$poids, main=paste0("poids~age R^2=", signif(summary(m)$r.squared, 2)*100, "%"))
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
arrows(d$age, d$poids, d$age, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)
```




### Travaux pratiques

**Jeu de données `anscombe`**

```{r echo=TRUE, results="verbatim"}
data(anscombe)
print(anscombe)
layout(matrix(1:4,2), respect=TRUE)
plot(anscombe$x1, anscombe$y1)
plot(anscombe$x2, anscombe$y2)
plot(anscombe$x3, anscombe$y3)
plot(anscombe$x4, anscombe$y4)
```

- Calculer les modèles de regression linéaire `m1 : y1~x1`, `m2 : y2~x2`, `m3 : y3~x3`, `m4 : y4~x4`.
- Tracer les nuages de points et les droites de regression correspondantes.
- Calculer, comparer et commenter les coéficiants de chacun des modéles.
- Calculer, comparer et commenter le pourcentage de variance expliquée de chaque modèle.

[correction_anscombe.R](./correction_anscombe.R)



**Jeu de données `CO2`**

```{r echo=TRUE, results="verbatim"}
data(CO2)
d = CO2[CO2$Type=="Quebec", ]
head(d)
layout(matrix(1:2, 1), respect=TRUE)
plot(d$conc, d$uptake, main="uptake~conc")
plot(log10(d$conc), d$uptake, main="uptake~log10(conc)")
```

On s’intéresse uniquement aux plantes dont le `Type` est `Quebec`.
 
- Calculer les modèles de regression linéaire `m1: uptake~conc` et `m2: uptake~log10(conc)`
- Tracer les nuages de points et les droites de regression correspondantes.
- Calculer, comparer et commenter le pourcentage de variance expliquée de chaque modèle.

[correction_co2_linear_model.R](./correction_co2_linear_model.R)




---


































## Tests statistiques

Il est important de noter que la construction du modèle de régression linéaire et l’estimation des paramètres par MCO **ne fait appel à aucune hypothèses sur la distribution des résidus**. Les hypothèses concerne l’application des tests sur les modèles.

En effet, si le modéle existe indépendamment des tests, l’inverse n’est vrai.

L’objectif d’un test statitsique est de :

  1. valider (ou non) une hypothèses en interrogeant le modèle ;
  2. quantifier le risque associé à cette décision.
  
Souvent, on cherche à invalider l’hypothèse dîte *nulle* ($\mathcal{H}_0$) et donc valider l’hypothèse dîte *alternative* ($\mathcal{H}_1$) au risque de première espèce $\alpha$ de 5%.

Cette procédure est une **démonstration par l’absurde** manipulant des **probabilités** : *Si $\mathcal{H}_0$ est vraie, alors les observations sont improbables, ainsi il est plus propable que $\mathcal{H}_1$ soit vraie*.



---

### Test de Student


Le test de Student dans le cadre de la regression linéaire simple teste le coéfficient de la droite régression.

Il pose la question de l’effet de la variables $x$ sur $Y$.


**Ecriture analytique du modèle**

$$Y = \beta_0 + \beta_1 x + \epsilon$$


**Hypothèse nulle et hypothèse alternative**

On se pose la question de l’effet de la variable $x$ sur la variable $Y$, *i.e.* :

$$\left  \lbrace
\begin{array}{l}
\mathcal{H}_0 : {\beta_1 = 0} \\
\mathcal{H}_1 : {\beta_1 \neq 0} 
\end{array}
\right.$$

**Conditions d’application du test**

- Les observations sont indépendantes
- La loi des erreurs est une loi normale
- La variance des erreurs est constante

**Statistique du test** 

Sous $\mathcal{H}_0$, 

$$t_{\widehat\beta_1,n-2} = \frac{\widehat\beta_1}{s_{\widehat\beta_1}} \sim \mathcal{T}_{n-2}$$

avec la variance résiduelle estimée :

$$ \widehat\sigma^2_n = \frac{1}{n-2}\sum^n_{i=1}  (Y_i - \widehat\beta_0 - \widehat\beta_1x_i)^2 = \frac{1}{n-2}\sum^n_{i=1} \widehat\epsilon_i^2 $$

et l’erreur standard sur $\beta_1$ estimée :

$$ s^2_{\widehat\beta_1} = \frac{\widehat\sigma^2_n}{ns^2_x} = \frac{\widehat\sigma^2_n}{\sum^n_{i=1} (x_i - \overline x)^2} $$



**Risque de première espèce $\alpha$ et p-valeur**

Le *risque de première espèce* ou risque $\alpha$ est le risque de rejeter l’hypothèse nulle $\mathcal{H}_0$ alors que celle-ci est vraie.

On prend généralement $\alpha=0.05$.

La *p-valeur* est la probabilité d’observer la valeur de la statistique de test sous $\mathcal{H}_0$.

Notons que le risque de seconde espèce ou risque $\beta$ est le risque de conserver $\mathcal{H}_0$ alors que celle-ci est fausse. 
Attention, le calcul du risque $\beta$ est plus compliqué. Souvent en minimisant $\alpha$ on augmente $\beta$. 
Pour s’assurer que $\beta$ reste faible il faut généralement augmenter $n$. On prle de *puissance* du test 


**Décision et conclusion du test**

La valeur critique du test, notée $c_\alpha$ est lue dans une table de la loi de Student (ou avec le logiciel R). 
C’est la valeur de la statistique de test pour laquelle $\mathcal{H}_0$ devient improbable compte tenu du risque $\alpha$ que l’on s’est fixé. 

Ainsi : 

- si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $t_{\widehat\beta_1,n-2}$ est supérieure ou égale à $c_\alpha$, alors le test est significatif. Vous rejetez  $\mathcal{H}_0$ et vous décidez que $\mathcal{H}_1$ est vraie avec un risque d’erreur de première espèce $\alpha$.

- si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $t_{\widehat\beta_1,n-2}$ est strictement inférieure à $c_\alpha$, alors le test n’est pas significatif. Vous conservez $\mathcal{H}_0$ avec un risque d’erreur de deuxième espèce $\beta$ à évaluer. 




**Mais alors pourquoi ?** 

En raison des conditions d’application du test.
En effet, si l’hypothèse nulle $\mathcal{H}_0$ est vérifiée, *i.e*. $\beta_1=0$, 
alors la variable aléatoire $t_{\widehat\beta_1,n-2} = \frac{\widehat\beta_1}{s_{\widehat\beta_1}}$, *i.e*, l’effet de la variable $x$ estimé à partir des observations *"réduit"* de la *"variabilité"* observée sur cet estimateur, ne s’écarte pas trop de 0.



**Exemple**

On restreint le jeu de données à 20 observations prises aléatoirement parmi les observations du jeu de données `data_nutri`.
Etudions le modèle $poids \sim taille$, au risque $\alpha = 0.05$.


```{r}
d = data_nutri_20
m = lm(poids~taille, d)
```

```{r}
layout(matrix(1:2, 1), respect=TRUE)
plot( d$taille, d$poids, main="poids ~ taille")
arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)

abline(m, col=2)
x=seq(-7,7, length.out=200)
plot(x, dt(x, m$df.residual), type="l", xlab="t", ylab="", main=paste0("Distibution de Student, ", m$df.residual," ddl."))
abline(v=c(-1,1) * summary(m)$coefficients[2,3], lty=2, col=4)
abline(v=c(-1,1) * qt(0.025, m$df.residual), lty=2, col=2)
x = seq(-7,qt(0.025, m$df.residual), length.out=100)
polygon(c(-7, x, qt(0.025, m$df.residual)), c(0, dt(x, m$df.residual), 0), col=adjustcolor(2, alpha.f=.5), border=2, lwd=3)
x = seq(7,qt(0.975, m$df.residual), length.out=100)
polygon(c(7, x, qt(0.975, m$df.residual)), c(0, dt(x, m$df.residual), 0), col=adjustcolor(2, alpha.f=.5), border=2, lwd=3)
x = seq(-7,-summary(m)$coefficients[2,3], length.out=100)
polygon(c(-7, x, -summary(m)$coefficients[2,3]), c(0, dt(x, m$df.residual), 0), col=adjustcolor(4, alpha.f=.5), border=4, lwd=3)
x = seq(7,summary(m)$coefficients[2,3], length.out=100)
polygon(c(7, x, summary(m)$coefficients[2,3]), c(0, dt(x, m$df.residual), 0), col=adjustcolor(4, alpha.f=.5), border=4, lwd=3)
legend("topleft",c("t_beta1", "c_alpha", "p-value", "5%"), col=c(4,2, 4, 2), lty=c(2,2,1,1), lwd=c(1,1,3,3), cex=.8)

```



1. Ecriture analytique du modèle

$$poids = \beta_0 + \beta_1 taille + \epsilon$$

2. Formulation des hypothèses $\mathcal{H}_0$ et $\mathcal{H}_1$

$$\mathcal{H}_0 : {\beta_1 = 0} \text{ et } \mathcal{H}_1 : {\beta_1 \neq 0}$$

3. Statistique de test utilisée et conditions d’application

Test de Student, les residus sont gaussiens.

4. Valeur de la statistique de test

`r signif(summary(m)$coefficients[2,3],3)`

5. P-valeur associée

`r signif(summary(m)$coefficients[2,4],3)`

6. Conclusion du test

La p-valeur est < 0.05, alors le test est significatif. Nous rejetons donc $\mathcal{H}_0$ et acceptons $\mathcal{H}_1$ au risque d’erreur de première espèce $\alpha = 0.05$.




```{r echo=TRUE, results="verbatim"}
m = lm(poids~taille, d)
# Shapiro test on residuals
shapiro.test(m$residuals)
# Student test
summary(m)
m$coefficient[[2]]
n = length(m$residuals)
xi = m$model$taille
s_beta1 = sqrt( sum(m$residuals^2)/(n-2) / sum((xi-mean(xi))^2))
s_beta1
stat_t =  m$coefficient[[2]]/s_beta1
stat_t
pval = pt(-stat_t, n-2) + 1-pt(stat_t, n-2)
pval
```



**TP `MASS::cats`**

On restreint le jeu de données à 10 observations prises aléatoirement parmi les observations du jeu de données `MASS::cats`.

```{r echo=TRUE}
set.seed(1)
d = d_mass_cats_10 = MASS::cats[sample(1:nrow(MASS::cats),10),]
m = lm(Hwt~Bwt, d)
```

Etudier le modèle le modèle $Hwt \sim Bwt$

1. Ecriture analytique du modèle
2. Formulation des hypothèses $\mathcal{H}_0$ et $\mathcal{H}_1$
3. Statistique de test utilisée et conditions d’application
4. Valeur de la statistique de test
5. P-valeur associée
6. Conclusion du test
7. Augmenter le nombre d’observations prisent aléatoirement (20,30), quel est l’impact sur l’analyse ?


```{r}
layout(1, respect=TRUE)
plot( d$Bwt, d$Hwt, xlab="Body weight", ylab="Heart weight", main="Heart weight ~ Body weight")
arrows(d$Bwt, d$Hwt, d$Bwt, d$Hwt-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)
abline(m, col=2)
```

[correction_cats_student.R](./correction_cats_student.R)

[correction_tailleage_student.R](./correction_tailleage_student.R)


















### Test de Fisher

Le test de Fisher repose sur la décomposition de la variance.

Il pose la question de la part de variance expliquée par la variables $x$.


**Ecriture analytique du modèle**

$$Y = \beta_0 + \beta_1 x + \epsilon$$


**Hypothèse nulle et hypothèse alternative**

On se pose la question de la quantité de variable expliquée par la variable $x$ :

$$\left  \lbrace
\begin{array}{l}
\mathcal{H}_0 : {\beta_1 = 0} \\
\mathcal{H}_1 : {\beta_1 \neq 0} 
\end{array}
\right.$$


**Conditions d’application du test**

- Les observations sont indépendantes
- La loi des erreurs est une loi normale
- La variance des erreurs est constante

**Statistique du test** 

La statistique de test $\mathcal{F}$ est obtenue à partir du tableau de l’analyse de la variance : 

| Source de variation   | sc        | ddl  |  cm  | $F_{obs}$
| :-------------------- | :-------: | :--: | :--: | ---:
| Régression            | $sc_{reg} = \sum^n_{i=1}(\widehat{y}_i - \overline{y})^2$    | p-1  | $\frac{sc_{reg}}{p-1}$ | $\frac{cm_{reg}}{cm_{res}}$
| Résiduelle            | $sc_{res} = \sum^n_{i=1}(y_i - \widehat{y})^2$    | n-p  | $\frac{sc_{res}}{n-p}$ 
| Totale            | $sc_{tot} = \sum^n_{i=1}(y_i - \overline{y})^2$    | n-1  



Sous $\mathcal{H}_0$, 


$$F_{obs} = \frac{CM_{reg}} {CM_{res}} \sim \mathcal{F}_{p-1, n-p}$$


**Risque de première espèce $\alpha$ et p-valeur**

Le *risque de première espèce* ou risque $\alpha$ est le risque de rejeter l’hypothèse nulle $\mathcal{H}_0$ alors que celle-ci est vraie.

La *p-valeur* est la probabilité d’observer la valeur de la statistique de test sous $\mathcal{H}_0$ en ayant préalablement vérifié les conditions d’application du test.

**Décision et conclusion du test**

La valeur critique du test, notée $c_\alpha$ est lue dans une table de la loi de Fisher (ou avec le logiciel R). 
C’est la valeur de la statistique de test pour laquelle $\mathcal{H}_0$ devient improbable compte tenu du risuqe $alpha$ que l’on s’est fixé. 

Ainsi : 

- si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $F_{obs}$ est supérieure ou égale à $c_\alpha$, alors le test est significatif. Vous rejetez  $\mathcal{H}_0$ et vous décidez que $\mathcal{H}_1$ est vraie avec un risque d’erreur de première espèce $\alpha$.

- si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $F_{obs}$ est strictement inférieure à $c_\alpha$, alors le test n’est pas significatif. Vous conservez $\mathcal{H}_0$ avec un risque d’erreur de deuxième espèce $\beta$ à évaluer. 




**Mais alors pourquoi ?** 

En raison des conditions d’application du test.

En effet, sous l’hypothèse nulle $\mathcal{H}_0$ :  $\beta_1=0$, donc $SC_{reg}$ (l’écart entre la droite de régression et la moyenne globales des Y) doit être petite (proche de 0) et $SC_{res}$ procher $SC_{tot}$.
Ainsi, la statistique de test $F_{obs} = \frac{CM_{reg}} {CM_{res}}$ ne doit pas trop s’écarter de 0.






**Exemple**

On restreint le jeu de données à 20 observations prises aléatoirement parmi les observations du jeu de données `data_nutri`.
Etudions le modèle $poids \sim taille$, au risque $\alpha = 0.05$.


```{r}
d = data_nutri_20
m = lm(poids~taille, d)
```

```{r}
layout(matrix(1:2, 1), respect=TRUE)
plot( d$taille, d$poids, main="poids ~ taille")
arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)

abline(m, col=2)
x=seq(0,15, length.out=200)
plot(x, df(x, anova(m)[1,1], anova(m)[2,1]), type="l", xlab="t", ylab="", main=paste0("Distibution de Fisher,  ", anova(m)[1,1]," et ", anova(m)[2,1]," ddl."))
abline(v=anova(m)[1,4], lty=2, col=4)
abline(v=qf(0.95, anova(m)[1,1], anova(m)[2,1]), lty=2, col=2)
x = seq(15,qf(0.95, anova(m)[1,1], anova(m)[2,1]), length.out=100)
polygon(c(15, x, qf(0.95, anova(m)[1,1], anova(m)[2,1])), c(0, df(x, anova(m)[1,1], anova(m)[2,1]), 0), col=adjustcolor(2, alpha.f=.5), border=2, lwd=3)
x = seq(15,anova(m)[1,4], length.out=100)
polygon(c(15, x, anova(m)[1,4]), c(0, df(x, anova(m)[1,1], anova(m)[2,1]), 0), col=adjustcolor(4, alpha.f=.5), border=4, lwd=3)
legend("topleft",c("F_obs", "c_alpha", "p-value", "5%"), col=c(4,2, 4, 2), lty=c(2,2,1,1), lwd=c(1,1,3,3), cex=.8)
```



1. Ecriture analytique du modèle

$$poids = \beta_0 + \beta_1 taille + \epsilon$$

2. Formulation des hypothèses $\mathcal{H}_0$ et $\mathcal{H}_1$

$$\mathcal{H}_0 : {\beta_1 = 0} \text{ et } \mathcal{H}_1 : {\beta_1 \neq 0}$$

3. Statistique de test utilisée et conditions d’application

Test de Fisher, les résidus sont gaussiens.

4. Valeur de la statistique de test

`r signif(anova(m)[1,4],3)`

5. P-valeur associée

`r signif(anova(m)[1,5],3)`

6. Conclusion du test

La p-valeur est < 0.05, alors le test est significatif. Nous rejetons donc $\mathcal{H}_0$ et acceptons $\mathcal{H}_1$ au risque d’erreur de première espèce $\alpha = 0.05$.




```{r echo=TRUE, results="verbatim"}
d = data_nutri_20
m = lm(poids~taille, d)
# Shapiro test on residuals
shapiro.test(m$residuals)
# Fisher test
anova(m)
# Sum Sq
y_hat = d$taille * m$coefficient[2] + m$coefficient[1]
y_bar = mean(d$poids)
sum((y_hat - y_bar)^2)
sum(m$res^2)
#Mean Sq 
sum((y_hat - y_bar)^2)
sum(m$res^2) / m$df.residual
#F value   
sum((y_hat - y_bar)^2) / (sum(m$res^2) / m$df.residual)
#Pr(>F)
1-pf(sum((y_hat - y_bar)^2) / (sum(m$res^2) / m$df.residual), 1, m$df.residual)
```




**TP `MASS::cats`**

On restreint le jeu de données à 10 observations prises aléatoirement parmi les observations du jeu de données `MASS::cats`.

```{r echo=TRUE}
set.seed(1)
d = d_mass_cats_10 = MASS::cats[sample(1:nrow(MASS::cats),10),]
m = lm(Hwt~Bwt, d)
```

Etudier le modèle le modèle $Hwt \sim Bwt$

1. Ecriture analytique du modèle
2. Formulation des hypothèses $\mathcal{H}_0$ et $\mathcal{H}_1$
3. Statistique de test utilisée et conditions d’application
4. Valeur de la statistique de test
5. P-valeur associée
6. Conclusion du test
7. Augmenter le nombre d’observations prisent aléatoirement (20,30), quel est l’impact sur l’analyse ?


```{r}
layout(1, respect=TRUE)
plot( d$Bwt, d$Hwt, xlab="Body weight", ylab="Heart weight", main="Heart weight ~ Body weight")
arrows(d$Bwt, d$Hwt, d$Bwt, d$Hwt-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)
abline(m, col=2)
```

[correction_cats_fisher.R](./correction_cats_fisher.R)

[correction_tailleage_student.R](./correction_tailleage_student.R)









## Pour aller plus loin










### Intervalle de confiance

On peut construire les intervalles de confiance suivants

$$IC_{1-\alpha}(\widehat\beta_1) = \Big]\widehat\beta_1 - t_{n-2;1-\alpha/2}*s_{\beta_1}; \widehat\beta_1 + t_{n-2;1-\alpha/2}*s_{\beta_1}\Big[$$

$$IC_{1-\alpha}(\widehat\beta_0) = \Big]\widehat\beta_0 - t_{n-2;1-\alpha/2}*s_{\beta_0}; \widehat\beta_0 + t_{n-2;1-\alpha/2}*s_{\beta_0}\Big[$$


```{r echo=TRUE, results="verbatim"}
confint(m)
```

Ainsi que l’intervalle de prédiction d’une valeur moyenne de $Y$, sachant que $x = x_0$. 

L’estimation ponctuelle pour cette valeur de $x_0$ est alors égale a $\widehat Y_0 = \widehat\beta_0 + \widehat\beta_1x_0$

$$IP_{1-\alpha}(Y) = \Big]\widehat Y_0 - t_{n-2;1-\alpha/2}\sqrt{\widehat\sigma^2_n\Big(\frac{1}{n}+\frac{(x_0-\overline{x}_n)^2}{(n-1)s^2_x}\Big)}; \widehat Y_0 + t_{n-2;1-\alpha/2}*\sqrt{\widehat\sigma^2_n\Big(\frac{1}{n}+\frac{(x_0-\overline{x}_n)^2}{(n-1)s^2_x}\Big)}\Big[$$


```{r echo=TRUE, results="verbatim"}
d = d_mass_cats_10
m = lm(Hwt~Bwt, d)
layout(1, respect=TRUE)
plot(
  d$Bwt, d$Hwt, 
  xlab="Body weight", ylab="Heart weight", 
  main="Heart weight ~ Body weight"
)
abline(m, col=2)

x = seq(min(m$model$Bwt), max(m$model$Bwt), length.out=100)
x = seq(2.5, 3, length.out=100)
pred =  predict(m, interval="predict", level=0.95)
head(pred)
points(m$model$Bwt, pred[,2], col=2, pch=3)
points(m$model$Bwt, pred[,3], col=2, pch=3)
pred =  predict(m, interval="predict", level=0.99)
points(m$model$Bwt, pred[,2], col=3, pch=3)
points(m$model$Bwt, pred[,3], col=3, pch=3)
pred =  predict(m, interval="predict", level=0.90)
points(m$model$Bwt, pred[,2], col=4, pch=3)
points(m$model$Bwt, pred[,3], col=4, pch=3)
legend("topleft",c("95% ci", "99% ci", "90% ci"), col=2:4, pch=3)

```

**Travaux pratiques**


- Tracer l’intervalle de prédiction du modèle obtenu sur la totalité des données `cats`.
- Comparer la l’intervalle de prédiction obtenu avec ce modèle avec celui obtenu sur le modèle du cours (30 observations).

```{r eval=FALSE}
layout(1, respect=TRUE)
m2 = lm(MASS::cats$Hwt~MASS::cats$Bwt)
pred =  predict(m2, interval="predict")
plot(
  MASS::cats$Bwt, MASS::cats$Hwt, 
  xlab="Body weight", ylab="Heart weight", 
  main="Heart weight ~ Body weight"
)
abline(m2, col=2)
head(pred)
points(m2$model$Bwt, pred[,2], col=4, pch=3)
points(m2$model$Bwt, pred[,3], col=4, pch=3)
pred =  predict(m2, interval="predict")

```






















### Tester les conditions d’application des tests

- Les observations sont indépendantes
- La variance des erreurs est constante $\sigma^2$
- La loi des erreurs est une loi normale $\mathcal{N}(0, \sigma^2)$


Pour tester la normalité des résidus : 

- Test de Shapiro-Wilk
- Test de Kolmogorov-Smirnov (comparaison de distributions)
- QQ plot 
- Test de Anderson-Darling
- Test de Jarque-Bera (package `tseries`, grands échantillons)

Pour tester l’homoscedasticité : 

- Test de Goldfeld-Quandt (quantiles,`lmtest::gqtest`, $H_0$ : Homoscedasticity ; $H_1$ : Heteroscedasticity).
- Test de Hartley (rapport des variances)
- Test de Bartlett (ANOVA)


**Shapiro-Wilk**

H_0 : les données sont normales

H_1 : les données ne sont pas normales


**Kolmogorov-Smirnov**

H_0 : les deux distributions sont identiques

H_1 : les deux distributions ne sont pas identiques



```{r echo=TRUE, results="verbatim"}
shapiro.test(rnorm(100))
shapiro.test(runif(100))
shapiro.test(m$residuals)

ks.test(rnorm(100), rnorm(100))
ks.test(rnorm(100), runif(100))
ks.test(rnorm(100), m$residuals)

layout(matrix(1:6,2), respect=TRUE)
set.seed(1)
plot(sort(rnorm(10)  ), scale(sort(runif(10)  )))
abline(0, 1, col=2)
plot(sort(rnorm(10)  ), sort(rnorm(10)  ))
abline(0, 1, col=2)
plot(sort(rnorm(100) ), scale(sort(runif(100) )))
abline(0, 1, col=2)
plot(sort(rnorm(100) ), sort(rnorm(100) ))
abline(0, 1, col=2)
plot(sort(rnorm(1000)), scale(sort(runif(1000))))
abline(0, 1, col=2)
plot(sort(rnorm(1000)), sort(rnorm(1000)))
abline(0, 1, col=2)


layout(matrix(1:3,1), respect=TRUE)
qqnorm(scale(rnorm(100)))
abline(0, 1, col=2)
qqnorm(scale(runif(100)))
abline(0, 1, col=2)
qqnorm(scale(m$residuals))
abline(0, 1, col=2)
```

































### Test empirique

Le test empirique repose sur la construction par échantillonage aléatoire (randomisation) d’une distribution empirique sous $\mathcal{H}_0$.

Il pose la question de la siginificativité de la pente de la droite de régression, *compte tenu des données*.


**Ecriture analytique du modèle**

$$Y = \beta_0 + \beta_1 x + \epsilon$$


**Hypothèse nulle et hypothèse alternative**

On pose la question de la siginificativité de la pente de la droite de régression, compte tenu des données.

$$\left  \lbrace
\begin{array}{l}
\mathcal{H}_0 : {\beta_1 = 0} \\
\mathcal{H}_1 : {\beta_1 \neq 0} 
\end{array}
\right.$$


**Principe** 

Compte tenu des données, le modèle nous retourne une valeur pour la pente $\beta_{1,obs}$.

Sous $\mathcal{H}_0$, $Y$ est indépendant des valeurs prises par $x$. 
Si on réattribut les valeurs de $x$ à nos observations de façon aléatoire, la valeur de $\beta_{1,rnd}$ obtenue pour le modèle corespeondant à ces nouvelles observations ne doit statistiquement pas plus s’écarter de $0$ ($\mathcal{H}_0$).

Si l’on réitére cette procédure $n_{rnd}$ fois, et que l’on dénombre $n_{sup}$ les cas où $\beta_{1,rnd}$ s’écarte plus de $0$ que $\beta_{1,obs}$ ($|\beta_{1,rnd}| > |\beta_{1,obs}|$), la *p-valeur empirique* $p=\frac{n_{sup}}{n_{rnd}}$ quantifie la probabilité d’observer par hasard $|\beta_1| > |\beta_{1,obs}| > 0$, i.e, la probabilité d’observer par hasard $\mathcal{H}_1$.

Notez que la p-valeur empirique ne peut pas être plus petite que $\frac{1}{n_{rnd}}$ et que sa précisions dépendra de $n_{rnd}$.

```{r echo=TRUE, results="verbatim"}
d = data_nutri_20
m = lm(poids~taille, d)
beta_obs = m$coefficients[[2]]

beta_rnds = sapply(1:1000, function(i){ # <=> for (i in 1:1000) ...
  d = data_nutri_20
  set.seed(i)
  d$poids = sample(d$poids)
  m = lm(poids~taille, d)
  beta_rnd = m$coefficients[[2]]
  return(beta_rnd)
})

layout(1, respect=TRUE)
hist(beta_rnds, freq=FALSE)
lines(density(beta_rnds), col="grey")
abline(v=beta_obs, col=2)
legend("topleft",c("null distributions", "beta_obs"), col=c("grey","red"), lty=1)

pval = sum(abs(beta_rnds) > abs(beta_obs)) / length(beta_rnds)
pval
```




















































### Exercices


**TP `ISwR::cystfibr`**


```{r echo=TRUE}
d = ISwR::cystfibr
m = lm(pemax~weight, d)
```

Etudier le modèle le modèle $pemax \sim weight$

1. Ecriture analytique du modèle
2. Formulation des hypothèses $\mathcal{H}_0$ et $\mathcal{H}_1$
3. Statistique de test utilisée et conditions d’application
4. Valeur de la statistique de test
5. P-valeur associée
6. Conclusion du test
7. Commenter le pourcentage de variance expliquée ($R^2$)


```{r}
layout(1, respect=TRUE)
plot( d$weight, d$pemax, xlab="weight (kg)", ylab="maximum expiratory pressure", main="pemax ~ weight")
abline(m, col=2)
```


**TP `data(birth)`**


```{r echo=TRUE}
library(catdata)
data(birth)
d = birth
head(d)
m = lm(Weight~Height, d)
```

Etudier le modèle le modèle $Weight \sim Height$

1. Ecriture analytique du modèle
2. Formulation des hypothèses $\mathcal{H}_0$ et $\mathcal{H}_1$
3. Statistique de test utilisée et conditions d’application
4. Valeur de la statistique de test
5. P-valeur associée
6. Conclusion du test
7. Commenter le pourcentage de variance expliquée ($R^2$)


```{r}
layout(1, respect=TRUE)
plot( d$Height, d$Weight, main="Weight ~ Height")
abline(m, col=2)
```


**TP `datasets::mtcars`**


```{r echo=TRUE}
d = datasets::mtcars
m = lm(mpg~wt, d)
```

Etudier le modèle le modèle $mpg \sim wt$

1. Ecriture analytique du modèle
2. Formulation des hypothèses $\mathcal{H}_0$ et $\mathcal{H}_1$
3. Statistique de test utilisée et conditions d’application
4. Valeur de la statistique de test
5. P-valeur associée
6. Conclusion du test
7. Commenter le pourcentage de variance expliquée ($R^2$)


```{r}
layout(1, respect=TRUE)
plot( d$wt, d$mpg, main="mpg~wt")
abline(m, col=2)
```




**TP `datasets::trees`**


```{r echo=TRUE}
d = datasets::trees
m = lm(Volume~Girth, d)
```

Etudier le modèle le modèle $Volume \sim Girth$

1. Ecriture analytique du modèle
2. Formulation des hypothèses $\mathcal{H}_0$ et $\mathcal{H}_1$
3. Statistique de test utilisée et conditions d’application
4. Valeur de la statistique de test
5. P-valeur associée
6. Conclusion du test
7. Commenter le pourcentage de variance expliquée ($R^2$)


```{r}
layout(1, respect=TRUE)
plot( d$Girth, d$Volume, main="Volume~Girth")
abline(m, col=2)
```


