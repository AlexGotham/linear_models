# Régression linéaire simple

On cherche une relation entre la variable observée $Y$ et la variable explicative $x$. 

$$ Y \sim x $$

Les objectifs peuvent être multiples:

  * approche exploratoire

  * recherche de corrélation significative

  * modèles de prédiction

Domaines d’application : chimie, astronomie, sismologie, économie, démographie, épidémiologie, géographie, écologie, géologie, physique, médecine...

Historiquement, le modèle linéaire a été développé par Fisher, avec applications en génétique et en agronomie.

---

## Présentation du modèle

Considérons $x$ et $Y$ deux variables. Y est expliquée (modélisée) par  la variable explicative $x$. 

**IMPORTANT**, dans le cadre des modèles linéaire Y est quantitatives (Ex : taille, poids, age...).

Dans le cas de la regression linéaire simple, $x$ est également quantitatives et la relation entre $Y$ et $x$ est une fonction **affine** (ou **linéaire**) de x (Cf. cours de maths de 3ème au collège).

\begin{eqnarray}
              \text{f: } \mathbb{R} & \rightarrow & \mathbb{R} \\
                                 x & \rightarrow & f(x)  =  ax + b \\ 
\end{eqnarray}

**ATTENTION**, dans R la fonction *abline* utilise la formule $bx+a$ ! Dans ce cours nous utliserons la notation $f(x)  = \beta_0 + \beta_1 x$



```{r  echo=FALSE, results="hide"}
layout(matrix(1:2, 1), respect=TRUE)

plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille")
m = lm(poids~taille, d)
m$coefficients
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille", xlim=c(0,max(d$taille)), ylim=c(m$coefficients[[1]]-20, max(d$poids)))
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
# points(0,m$coefficients[[1]], pch=16)
abline(h=m$coefficients[[1]], v=0, lty=2)
axis(2, at=m$coefficients[[1]], "beta_0", las=2)

segments(50, predict(m, data.frame(taille=50)), 100, predict(m, data.frame(taille=50)), lty=2)
segments(100, predict(m, data.frame(taille=50)), 100, predict(m, data.frame(taille=100)), lty=2)
text(75, predict(m, data.frame(taille=50)), "50", pos=1)
text(100, predict(m, data.frame(taille=75)), "50*beta_1", pos=4)

```



Les $n$ observations vont permettre de vérifier si la droite candidate est adéquate.

$Y$ et $x$ ne jouent pas un rôle identique : 

  - $Y$ est expliqué par $x$
  - $x$ est une variable *indépendante* (ou *explicative*) et $Y$ est une variable *dépendante* (ou *expliquée*)


Remarques : 

  - On utilise un modèle linéaire Gaussien pour des observations pouvant être modélisées par une loi normales. Pour d’autres distributions (Poisson, Bernoulli...), on utilisera un modèle linéaire généralisé.
  - La régression linéaire se caractérise par des variables explicatives **continues** ou quantitatives (Ex : poids~taille). L’ANOVA se caractérise par des variables explicatives **discrètes**, catégorielle, ou qualitatives (Ex : poids~sexe).

---

### Exemple d’une relation déterministe

La température $x$ en degrés Celsius explique entièrement la température  $y$ en degrés Farenheit par la relation : $y=32 + 9/5 x$.

\begin{eqnarray}
              \text{f: } \mathbb{R} & \rightarrow & \mathbb{R} \\
                                 x & \rightarrow & f(x)  = \beta_0 + \beta_1 x = 32 + 9/5 x \\ 
                                 0 & \rightarrow & f(0)  = 32 \\ 
                                 10 & \rightarrow & f(10)  = 50 \\ 
\end{eqnarray}



```{r  echo=FALSE, results="hide"}
layout(matrix(1:2, 1), respect=TRUE)
beta_0 = 32
beta_1 = 9/5
d_f = -10:20
plot(d_f, beta_0 + beta_1 * d_f, main="Température", xlab="deg. C", ylab="deg. F", col=0)
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
abline(v=c(0,10), h=c(32,50), lty=2)

```



---

### Exemple d’une relation stochastique


Si ce cas déterministe n’est pas vérifié, il faut chercher la droite qui ajuste le mieux l’échantillon : **modèle linéaire non déterministe**.

$$Y   = \beta_0 + \beta_1x + \epsilon $$

où $\beta_0$ et $\beta_1$ sont des réels fixes, mais inconnus, et $\epsilon$ une variable représentant le comportement individuel (résidus).

Modèle : $$E(Y) = \widehat Y = f(x) = \beta_0 + \beta_1 x$$

Par exemple $x$ est la taille, $Y$ le poids, à une même taille, plusieurs poids peuvent correspondrent. Les données ne sont plus alignées.

```{r}
layout(matrix(1:2, 1), respect=TRUE)

plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille")
m = lm(poids~taille, d)
m$coefficients
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
suppressWarnings(arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("E(Y)", "epsilon"), col=c(2,4), lty=1, cex=.8)
```



---

## Estimation des paramètres (méthode des moindres carrés)



**Exemple** : représentation des données *poids~tailles* dans le jeu de données `data_nutri`.

**Question** : comment définir la droite de regression ?


---

### Droite de regression, estimation et résidus

En analyse de régression linéaire, pour un individu $i$ :

- $x_i$ est la valeur de la varible explicative
- $Y_i$ est la valeur observée de la variable aléatoire
- la composante aléatoire de $Y_i$ est le $\epsilon_i$ correspondant
- la valeur de $Y_i$ estimée par le modèle est $\widehat Y_i$.

Ainsi : 

$$Y_i = \widehat Y_i + \epsilon_i$$


```{r echo=FALSE}
layout(matrix(1:2, 1), respect=TRUE)
plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille", col=adjustcolor(1, alpha.f=0.5))
m = lm(poids~taille, d)
# abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
pred = predict(m, data.frame(taille=d$taille))
i = which(d$poids - pred == max(d$poids - pred))
points(d$taille[i], d$poids[i], pch=16, col=2)
abline(v=d$taille[i], h=c(d$poids[i], d$poids[i]-m$residuals[i]), lty=2, col=2)
axis(1, at=d$taille[i], "xi", las=2, col=2)
axis(4, at=c(d$poids[i], d$poids[i]-m$residuals[i]), c("Yi", expression(bar("Yi"))), las=2, col=2)
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=1) # /!\ y = b.x + a
suppressWarnings(arrows(d$taille[i], d$poids[i]-m$residuals[i], d$taille[i], d$poids[i], col=4, length=0.1, lwd=2))
text(d$taille[i], d$poids[i]-m$residuals[i]/2, "eps_i", col=4, las=2, pos=2, cex=2)

```

Lorsque $x = x_i$, alors $\widehat Y(x_i) = \widehat Y_i$ , c’est-à-dire :

$$ \widehat Y_i=\widehat{\beta}_0+ \widehat{\beta}_1x_i $$
$\widehat Y_i$ est appelée la valeur estimée par le modèle.

Les quantités inobservables :
$$\epsilon_i = Y_i - \beta_0 - \beta_1x_i$$ 
sont estimées par les quantités observables :
$$ e_i= Y_i-\widehat Y_i$$ 

- Les quantités $e_i$ sont appelées les **résidus du modèle**.
- La plupart des méthodes d’estimation : estimer la droite de régression par une droite qui minimise une fonction de résidus.
- La plus connue : **la méthode des moindres carrés ordinaires (MCO)**.

---

### Intutition des moindres carrés 


Pour l’instant, la droite de régression est inconnue.
Tout le problème est d’estimer $\beta_0$ et $\beta_1$ à partir d’un échantillon de données.

On essaie de déterminer la **droite de regression** qui approche le mieux les données: $$\widehat Y(x) = \widehat{\beta}_0 + \widehat{\beta}_1x$$

Avec $\widehat Y(x)$ un estimateur de la moyenne théorique $\mu_Y(x)$ ($\widehat Y(x)$ correspond à la moyenne de $Y$ mésurée sur tous les individus pour lesquels $x$ prend une valeur donnée). 

$\mu_Y(x)$ n’est ni observable, ni calculable (il faudrait alors recenser **tous** les individus de la population).

*Remarque* : la moyenne empirique de $Y$, définie par $\overline{Y}_n(x) = \frac{1}{n}\sum^n_{i=1}Y_i(x)$ est un autre estimateur de $\mu_Y(x)$ . Si le modèle est bon, $\widehat Y(x)$ est plus précis que $\overline{Y}_n(x)$



```{r results="verbatim", results="verbatim", echo=TRUE}
d = read.table("data/data_nutri.csv", header=TRUE, sep=",", row.names = 1)
head(d)
dim(d)
layout(matrix(1:2, 1), respect=TRUE)
plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille")
```

```{r echo=TRUE}
layout(matrix(1:2, 1), respect=TRUE)
beta_0 = mean(d$poids)
beta_1 = 0
res =  d$poids - (beta_0 + beta_1*d$taille)
eqm = sum(res^2)
plot(d$taille, d$poids, main=paste0("poids~taille, EQM=", round(eqm)), ylab="poids", xlab="taille")
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
suppressWarnings(arrows(d$taille, d$poids, d$taille, d$poids-res, length=0.1, col=adjustcolor(4, alpha.f=0.5)))

layout(matrix(1:2, 1), respect=TRUE)
beta_0 = -100
beta_1 = 1
res =  d$poids - (beta_0 + beta_1*d$taille)
eqm = sum(res^2)
plot(d$taille, d$poids, main=paste0("poids~taille, EQM=", round(eqm)), ylab="poids", xlab="taille")
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
suppressWarnings(arrows(d$taille, d$poids, d$taille, d$poids-res, length=0.1, col=adjustcolor(4, alpha.f=0.5)))

beta_0 = -260
beta_1 = 2
res =  d$poids - (beta_0 + beta_1*d$taille)
eqm = sum(res^2)
plot(d$taille, d$poids, main=paste0("poids~taille, EQM=", round(eqm)), ylab="poids", xlab="taille")
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
suppressWarnings(arrows(d$taille, d$poids, d$taille, d$poids-res, length=0.1, col=adjustcolor(4, alpha.f=0.5)))

```




```{r echo=TRUE}
layout(matrix(1:2, 1), respect=TRUE)
results = list()
for(beta_0 in 0:-260) {
  for(beta_1 in seq(0,2, length.out=70)) {
    res =  d$poids - (beta_0 + beta_1*d$taille)
    eqm = sum(res^2)
    results[[length(results)+1]] = c(beta_0=beta_0, beta_1=beta_1, eqm=eqm)
  }
}
results = as.data.frame(do.call(rbind, results))
head(results)
min(results$eqm)
r = results[results$eqm==min(results$eqm),]
beta_0 = r[["beta_0"]]
beta_1 = r[["beta_1"]]
res =  d$poids - (beta_0 + beta_1*d$taille)
eqm = sum(res^2)
plot(d$taille, d$poids, main=paste0("poids~taille, EQM=", round(eqm)), ylab="poids", xlab="taille")
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
suppressWarnings(arrows(d$taille, d$poids, d$taille, d$poids-res, length=0.1, col=adjustcolor(4, alpha.f=0.5)))

```



---

### Méthode  et démonstration

**Objectif** : Définir des estimateurs qui minimisent la somme des carrés des résidus (ou **erreur quadratique moyenne** EQM).

Les estimateurs sont donc les coordonnées du minimum de la fonction à deux variables :

$$EQM(\beta_0, \beta_1) = \sum^{n}_{i=1} e^2_i = \sum^{n}_{i=1} (Y_i - \widehat Y_i)^2 = \sum^{n}_{i=1} (Y_i - \widehat\beta_0 -  \widehat\beta_1x_i)^2$$

Cette fonction est appelée la **fonction objectif**.


Les estimateurs correspondent aux valeurs annulant les dérivées partielles de cette fonction :

$$\frac{\delta EQM(\beta_0, \beta_1)}{\delta \beta_0} = -2 \sum (Y_i - \beta_0 - \beta_1x_i)$$
$$\frac{\delta EQM(\beta_0, \beta_1)}{\delta \beta_1} = -2 \sum x_i (Y_i - \beta_0 - \beta_1x_i)$$

Les estimateurs $(\widehat\beta_0^{MCO},\widehat\beta_1^{MCO})$ sont les solutions du système d’équation:
$$ \Bigg\{
\begin{align}
-2 \sum (Y_i - \beta_0 - \beta_1x_i) = 0  \\ 
-2 \sum x_i (Y_i - \beta_0 - \beta_1x_i) = 0
\end{align}
$$


Soient les equations suivantes: 

$$(1) \sum{Y}_i = n\widehat\beta_0 + \widehat\beta_1 \sum{x_i} $$

$$(2) \sum{x_iY_i} = \widehat\beta_0 \sum{x_i} + \widehat\beta_1 \sum{x^2_i}$$
On utilise les notations usuelles des moyennes empiriques $$\overline{x}_n = \frac{1}{n}\sum{x_i} ,\ \ \ \ \ \ \overline{Y}_n = \frac{1}{n}\sum{Y_i}$$

D’après (1): $$\widehat\beta_0 = \overline{Y}_n - \widehat\beta_1 \overline{x}_n$$

D’après (2):

\begin{eqnarray}
\widehat\beta_1 \sum{x^2_i} & = & \sum{x_iY_i} - \widehat\beta_0 \overline{x}_n \\
                            & = & \sum{x_iY_i} - n\overline{x}_n\overline{Y}_n + \widehat\beta_1 (\overline{x}_n)^2
\end{eqnarray}

Ainsi $$\widehat\beta_1 = \frac{\sum{x_iY_i} -  n\overline{x}_n\overline{Y}_n}{\sum{x_i^2} -  n(\overline{x}_n)^2}$$

Et comme nous avons :

\begin{eqnarray}
\sum{(x_i-\overline{x}_n)(Y_i-\overline{Y}_n)} & = & \sum{x_iY_i} -  n\overline{x}_n\overline{Y}_n \\
                   \sum{(x_i-\overline{x}_n)}   & = & \sum{x_i^2} -  n(\overline{x}_n)^2
\end{eqnarray}

Nous obtenons: $$\widehat\beta_1 = \frac{\sum{(x_i-\overline{x}_n)(Y_i-\overline{Y}_n)}}{\sum{(x_i-\overline{x}_n)}}$$

Dans la pratique, nous calculons $\widehat\beta_1$ puis $\widehat\beta_0$.

Nous obtenons ainsi une estimation de la droite de régression, appelée la droite des moindres carrés ordinaires:

$$\widehat Y(x) = \widehat\beta_0 + \widehat\beta_1x$$

---

### Exemple

$\widehat\beta_0$ correspond à l’ordonnée à l’origine. Ce paramètre n’est pas toujours interprétable (il dépend de la signifaction de $X$ et du fait que $X$ soit centré ou non).

$\widehat\beta_1$ correspond à la pente.

Attention, la technique des MCO crée des estimateurs sensibles aux valeurs atypiques.

```{r echo=TRUE, results="verbatim"}
layout(1, respect=TRUE)
plot(d$taille, d$poids, main="poids~taille")
## Model
# Y~X
# E(Y) = b.X
# E(Y) = b_0 + b_1.X
# Y_i = b_0 + b_1.X_i + e_i
m = lm(d$poids~d$taille)
m$coefficients
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
# residuals
m$residuals
suppressWarnings(arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)
```

---

## Décomposition de la variance

### Variation expliquée et inexpliquée

La variation de $Y$ vient du fait de sa dépendance à la variable explicative $X$. C’est la **variation expliquée par le modèle**.

Dans l’exemple « taille-poids », nous avons remarqué que lorsque nous mesurons $Y$ avec une même valeur de $X$, nous observons une certaine variation sur $Y$. C’est la **variation inexpliquée par le modèle**.

Variation totale de Y = Variation expliquée par le modèle + Variation inexpliquée par le modèle

Soit $$(Y_i - \overline{Y}_n) = (\widehat Y_i - \overline{Y}_n) + (Y_i - \widehat Y_i)$$ avec $(\widehat Y_i - \overline{Y}_n)$ la différence expliquée par le modèle et $(Y_i - \widehat Y_i)$ la différence inexpliquée (ou résidu).

---

### Interêt des moindres carrés

La méthode des moindres carrés conserve la décomposition (en variance expliquée et résiduelle) en considérenat la somme des carrés de ces différences: 

$$ \sum(Y_i - \overline{Y}_n)^2 = \sum(\widehat Y_i - \overline{Y}_n)^2 + \sum(Y_i - \widehat Y_i)^2 $$
Soit $$ SC_{tot} = SC_{reg} + SC_{res}$$ 

Avec $SC_{tot}$ la somme des carrés totale,  $SC_{reg}$ la somme des carrés due à la régression et  $SC_{res}$ la somme des carrés des résidus.


---

### Coefficient de determination $R^2$



La mesure du pourcentage de la variation totale expliquée par le modèle se fait par le **coefficient de détermination**.

$$ R^2 = \frac{SC_{reg}}{SC_{tot}}$$

- $R^2$ est compris entre 0 et 1.
- $R^2 = 1$ : cas où les données sont parfaitement alignées
(comme c’est le cas pour un modèle déterministe).
- $R^2 = 0$ :cas où la variation de $Y$ n’est pas due à la variation de $X$. Les données ne sont pas du tout alignées.
- Plus $R^2$  est proche de 1, plus les données sont alignées sur la droite de régression.



```{r}
layout(matrix(1:2, 1), respect=TRUE)
m = lm(d$poids~d$taille)
plot(d$taille, d$poids, main=paste0("poids~taille R^2=", signif(summary(m)$r.squared, 2)*100, "%"))
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
suppressWarnings(arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)

m = lm(d$poids~d$age)
plot(d$age, d$poids, main=paste0("poids~age R^2=", signif(summary(m)$r.squared, 2)*100, "%"))
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
suppressWarnings(arrows(d$age, d$poids, d$age, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)
```



---




### Travaux pratiques

#### Jeu de données `anscombe`

```{r echo=TRUE, results="verbatim"}
data(anscombe)
print(anscombe)

layout(matrix(1:4,2), respect=TRUE)
plot(anscombe$x1, anscombe$y1)
plot(anscombe$x2, anscombe$y2)
plot(anscombe$x3, anscombe$y3)
plot(anscombe$x4, anscombe$y4)
```

- Calculer les modèles de regression linéaire `m1: y1~x1`, `m2: y2~x2`, `m3: y3~x3`, `m4: y4~x4`.
- Tracer les nuages de points et les droites de regression correspondantes.
- Calculer, comparer et commenter les coéficiants de chacun des modéles.
- Calculer, comparer et commenter le pourcentage de variance expliquée de chaque modèle.

```{r eval=FALSE, echo=FALSE, results="hide"}
# create models
m1 = lm(anscombe$y1~anscombe$x1)
m2 = lm(anscombe$y2~anscombe$x2)
m3 = lm(anscombe$y3~anscombe$x3)
m4 = lm(anscombe$y4~anscombe$x4)

# model details
summary(m1)
summary(m2)
summary(m3)
summary(m4)

# plot
layout(matrix(1:4,2), respect=TRUE)
plot(anscombe$x1,anscombe$y1, xlim = c(3, 19), ylim = c(3, 13), main=paste0("m1 R^2=", signif(summary(m1)$r.squared, 2)*100, "%")) ; abline(m1)
plot(anscombe$x2,anscombe$y2, xlim = c(3, 19), ylim = c(3, 13), main=paste0("m2 R^2=", signif(summary(m2)$r.squared, 2)*100, "%")) ; abline(m2)
plot(anscombe$x3,anscombe$y3, xlim = c(3, 19), ylim = c(3, 13), main=paste0("m3 R^2=", signif(summary(m3)$r.squared, 2)*100, "%")) ; abline(m3)
plot(anscombe$x4,anscombe$y4, xlim = c(3, 19), ylim = c(3, 13), main=paste0("m4 R^2=", signif(summary(m4)$r.squared, 2)*100, "%")) ; abline(m4)

# abline(a=m1$coefficients[[1]],b=m1$coefficients[[2]], lty=2, lwd=3, col=2)
```



---



#### Jeu de données `CO2`

```{r echo=TRUE, results="verbatim"}
data(CO2)
head(CO2)
layout(matrix(1:2, 1), respect=TRUE)
plot(
  CO2[CO2$Type=="Quebec", ]$conc, 
  CO2[CO2$Type=="Quebec", ]$uptake, 
  xlab="food", ylab="growth", main="growth~food"
)
plot(
  log(CO2[CO2$Type=="Quebec", ]$conc), 
  CO2[CO2$Type=="Quebec", ]$uptake, 
  xlab="log(food)", ylab="growth", main="growth~log(food)"
)
```

```{r eval=FALSE}
m1 = lm(CO2[CO2$Type=="Quebec", ]$uptake~CO2[CO2$Type=="Quebec", ]$conc)
m2 = lm(CO2[CO2$Type=="Quebec", ]$uptake~log(CO2[CO2$Type=="Quebec", ]$conc))
layout(matrix(1:2, 1), respect=TRUE)
plot(
  CO2[CO2$Type=="Quebec", ]$conc, 
  CO2[CO2$Type=="Quebec", ]$uptake, 
  xlab="food", ylab="growth", main=paste0("growth~food ")
)
abline(m1, col=2)
plot(
  log(CO2[CO2$Type=="Quebec", ]$conc), 
  CO2[CO2$Type=="Quebec", ]$uptake, 
  xlab="log(food)", ylab="growth", main=paste0("growth~log(food) ")
)
abline(m2, col=2)
summary(m1)
summary(m2)

```

On s’intéresse uniquement aux plantes dont le `Type` est `Quebec`.
 
- Calculer les modèles de regression linéaire `m1: growth~food` et `m2: growth~log(food)`
- Tracer les nuages de points et les droites de regression correspondantes.
- Calculer, comparer et commenter le pourcentage de variance expliquée de chaque modèle.
- Comparer les coéficiants de chacun des modéles.

```{r eval=FALSE, echo=FALSE, results="hide"}
```

---







---

## Test statistique et IC/IP

Il est important de noter que la **construction du modèle de régression** et l’estimation des paramètres par MCO **ne fait pas appel aux hypothèses de distribution**.

Les hypothèses de distribution sont essentielles lorsqu’il s’agit de construire des tests et des intervalles de confiance et de prédiction.


---

### Test de Student


**Hypothèse nulle et hypothèse alternative**

On se pose la question de l’effet de la variable $x$ :

$\mathcal{H}_0 : {\beta_1 = 0}$ contre $\mathcal{H}_1 : {\beta_1 \neq 0}$


**Conditions d’applications du test**

- Les observations sont indépendantes
- La variance des erreurs est constante $\sigma^2$
- La loi des erreurs est une loi normale $\mathcal{N}(0, \sigma^2)$


Pour tester la normalité des résidus : 

- Test de Shapiro-Wilk
- Test de Anderson-Darling
- Test de Jarque-Bera (package `tseries`, grands échantillons)
- Test de Kolmogorov-Smirnov (comparaison de distributions)
- QQ plot 

Pour tester l’homoscedasticité : 

- Test de Goldfeld-Quandt 
- Test de Hartley (??)
- Test de Bartlett (ANOVA)


```{r echo=TRUE, results="verbatim"}
set.seed(1)
idx = sample(1:nrow(MASS::cats),30)
m = lm(Hwt~Bwt, MASS::cats[idx,])
layout(1, respect=TRUE)
plot(
  MASS::cats[idx,]$Bwt, MASS::cats[idx,]$Hwt, 
  xlab="Body weight", ylab="Heart weight", 
  main="Heart weight ~ Body weight"
)
abline(m, col=2)

shapiro.test(rnorm(100))
shapiro.test(runif(100))
shapiro.test(m$residuals)

ks.test(rnorm(100), rnorm(100))
ks.test(rnorm(100), runif(100))
ks.test(rnorm(100), m$residuals)

layout(matrix(1:6,2), respect=TRUE)
set.seed(1)
plot(sort(rnorm(10)  ), scale(sort(runif(10)  )))
abline(0, 1, col=2)
plot(sort(rnorm(10)  ), sort(rnorm(10)  ))
abline(0, 1, col=2)
plot(sort(rnorm(100) ), scale(sort(runif(100) )))
abline(0, 1, col=2)
plot(sort(rnorm(100) ), sort(rnorm(100) ))
abline(0, 1, col=2)
plot(sort(rnorm(1000)), scale(sort(runif(1000))))
abline(0, 1, col=2)
plot(sort(rnorm(1000)), sort(rnorm(1000)))
abline(0, 1, col=2)


layout(matrix(1:3,1), respect=TRUE)
qqnorm(scale(rnorm(100)))
abline(0, 1, col=2)
qqnorm(scale(runif(100)))
abline(0, 1, col=2)
qqnorm(scale(m$residuals))
abline(0, 1, col=2)
```

**Risque de première espèce $\alpha$**

Le risque de première espèce ou risque $\alpha$ est le risque de rejeter l’hypothèse nulle $\mathcal{H}_0$ alors que celle-ci est vraie.

Généralement on fixe $\alpha=0.05$.

Note : Le risque de seconde espèce ou risque $\beta$ est le risque de conserver $\mathcal{H}_0$ alors que celle-ci est fausse. Attention, le calcul du risque $\beta$ est plus compliqué. Souvent en minimisant $\alpha$ on augmente $\beta$. Pour s’assurer que $\beta$ reste faible il faut généralement augmenter $n$.

**Statistique du test** 

Si l’hypothèse nulle $\mathcal{H}_0$ est vérifiée, alors la variable aléatoire $\mathcal{T}_{\widehat\beta_1,n-2} = \frac{\widehat\beta_1}{s_{\widehat\beta_1}}$ suit la loi de Student $\mathcal{T}(n-2)$

**Décision et conclusion du test**: La valeur critique du test, notée $c_\alpha$ est lue dans une table de la loi de Student.

- Si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $\mathcal{T}_{\widehat\beta_1,n-2}(obs)$ est supérieure ou égale à $c_\alpha$, alors le test est significatif. Vous rejetez  $\mathcal{H}_0$ et vous décidez que $\mathcal{H}_1$ est vraie avec un risque d’erreur de première espèce $\alpha$.

- Si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $\mathcal{T}_{\widehat\beta_1,n-2}(obs)$ est strictement inférieure à $c_\alpha$, alors le test n’est pas significatif. Vous conservez $\mathcal{H}_0$ avec un risque d’erreur de deuxième espèce $\beta$ à évaluer. 


Calcul de la statistique de test :


La variance des résidus est estimée par :

$$ \widehat\sigma^2_n = \frac{1}{n-2}\sum^n_{i=1}  (Y_i - \widehat\beta_0 - \widehat\beta_1x_i)^2 = \frac{1}{n-2}\sum^n_{i=1} \widehat\epsilon_i^2 $$

L’erreur standard sur $\beta_1$ est estimée par :

$$ s^2_{\widehat\beta_1} = \frac{\widehat\sigma^2_n}{ns^2_x} = \frac{\widehat\sigma^2_n}{\sum^n_{i=1} (x_i - \overline x)^2} $$



```{r echo=TRUE, results="verbatim"}
# Student test
summary(m)
n = length(m$residuals)
xi = m$model$Bwt
s_beta1 = sqrt( sum(m$residuals^2)/(n-2) / sum((xi-mean(xi))^2))
s_beta1
stat_t =  m$coefficient[2]/s_beta1
stat_t
pval = pt(-stat_t, n-2) + 1-pt(stat_t, n-2)
pval
```




```{r eval=FALSE}
# Note (vérifier les notations):
#
#
# - $\widehat\sigma^2_n$ : la variance résiduelle
# - $s^2_Y$ : la variance de l’échantillon des $Y_i$
# - $s^2_x$ : la variance de l’échantillon des $x_i$
# - $s^2_{\widehat\beta_1}$ : La variance de l’estimateur $\widehat\beta_1$
# - $s^2_{\widehat\beta_0}$ : La variance de l’estimateur $\widehat\beta_0$
#
#
# $$\widehat\sigma^2_n = \frac{n}{n-2}s^2_Y(1-R^2)$$
#
# Avec $s^2_Y = \frac{1}{n}(\sum^n_{i=1}Y_i^2)-(\overline{Y}_n)^2$ la variance de l’échantillon des $Y_i$ et $\overline{Y}_n$ la moyenne des $Y_i$.
#
# $$ s^2_{\widehat\beta_0} = \widehat\sigma^2_n\Big(\frac{1}{n}+\frac{\overline{x}_n}{ns^2_x}\Big)$$

s_beta0 = sqrt( sum(m$residuals^2)/(n-2) * ( 1/n +  mean(xi) / sum((xi-mean(xi))^2) ))
s_beta0

sum(m$residuals^2)/(n-2) * ( 1/n +  mean(xi) / sum((xi-mean(xi))^2) )

stat_t =  m$coefficient[1]/s_beta0
stat_t

s_beta1 = sqrt( 1/(n-2) * sum(m$residuals^2) / (sum((xi-mean(xi))^2)))
stat_t =  m$coefficient[2]/s_beta1
stat_t


# Fisher test
anova(m)
n = length(m$residuals)
p = m$rank
res_sum_sq = sum(m$residuals^2)
tot_sum_sq = sum((MASS::cats[idx,]$Hwt - mean(MASS::cats[idx,]$Hwt))^2)
fac_sum_sq = tot_sum_sq - res_sum_sq
res_mean_sq = res_sum_sq / (n-p)
fac_mean_sq = fac_sum_sq
stat_f = fac_mean_sq / res_mean_sq
stat_f
pval = 1-pf(stat_f, p-1, n-p)
pval
```


**Travaux pratiques**

- Construire le modèle de régréssion linéaire sur la totalité des données `cats`.
- Tracer la droite de régression correspondante.
- Tester l’hypothèse $\mathcal{H}_0 : {\beta_1 = 0}$ contre $\mathcal{H}_1 : {\beta_1 \neq 0}$ sur ce modèle.
- Comparer la p-valeur obtenue avec ce modèle avec celle obtenue sur le modèle du cours (30 observations).
  
```{r echo=TRUE, results="verbatim"}
layout(1, respect=TRUE)
plot(
  MASS::cats$Bwt, MASS::cats$Hwt, 
  xlab="Body weight", ylab="Heart weight", 
  main="Heart weight ~ Body weight"
)
```

```{r eval=FALSE}
m2 = lm(Hwt~Bwt, MASS::cats)
abline(m, col="grey")
abline(m2, col=2)
shapiro.test(m2$residuals)
summary(m2)
``` 

---

### Intervalle de confiance

On peut construire les intervalles de confiance suivants

$$IC_{1-\alpha}(\widehat\beta_1) = \Big]\widehat\beta_1 - t_{n-2;1-\alpha/2}*s_{\beta_1}; \widehat\beta_1 + t_{n-2;1-\alpha/2}*s_{\beta_1}\Big[$$

$$IC_{1-\alpha}(\widehat\beta_0) = \Big]\widehat\beta_0 - t_{n-2;1-\alpha/2}*s_{\beta_0}; \widehat\beta_0 + t_{n-2;1-\alpha/2}*s_{\beta_0}\Big[$$


```{r echo=TRUE, results="verbatim"}
confint(m)
```

Ainsi que l’intervalle de prédiction d’une valeur moyenne de $Y$, sachant que $x = x_0$. 

L’estimation ponctuelle pour cette valeur de $x_0$ est alors égale a $\widehat Y_0 = \widehat\beta_0 + \widehat\beta_1x_0$

$$IP_{1-\alpha}(Y) = \Big]\widehat Y_0 - t_{n-2;1-\alpha/2}\sqrt{\widehat\sigma^2_n\Big(\frac{1}{n}+\frac{(x_0-\overline{x}_n)^2}{(n-1)s^2_x}\Big)}; \widehat Y_0 + t_{n-2;1-\alpha/2}*\sqrt{\widehat\sigma^2_n\Big(\frac{1}{n}+\frac{(x_0-\overline{x}_n)^2}{(n-1)s^2_x}\Big)}\Big[$$


```{r echo=TRUE, results="verbatim"}
layout(1, respect=TRUE)
plot(
  MASS::cats[idx,]$Bwt, MASS::cats[idx,]$Hwt, 
  xlab="Body weight", ylab="Heart weight", 
  main="Heart weight ~ Body weight"
)
abline(m, col=2)

x = seq(min(m$model$Bwt), max(m$model$Bwt), length.out=100)
x = seq(2.5, 3, length.out=100)
pred =  predict(m, interval="predict", level=0.95)
head(pred)
points(m$model$Bwt, pred[,2], col=2, pch=3)
points(m$model$Bwt, pred[,3], col=2, pch=3)
pred =  predict(m, interval="predict", level=0.99)
points(m$model$Bwt, pred[,2], col=3, pch=3)
points(m$model$Bwt, pred[,3], col=3, pch=3)
pred =  predict(m, interval="predict", level=0.90)
points(m$model$Bwt, pred[,2], col=4, pch=3)
points(m$model$Bwt, pred[,3], col=4, pch=3)
legend("topleft",c("95% ci", "99% ci", "90% ci"), col=2:4, pch=3)

```

**Travaux pratiques**


- Tracer l’intervalle de prédiction du modèle obtenu sur la totalité des données `cats`.
- Comparer la l’intervalle de prédiction obtenu avec ce modèle avec celui obtenu sur le modèle du cours (30 observations).

```{r eval=FALSE}
layout(1, respect=TRUE)
m2 = lm(MASS::cats$Hwt~MASS::cats$Bwt)
pred =  predict(m2, interval="predict")
plot(
  MASS::cats$Bwt, MASS::cats$Hwt, 
  xlab="Body weight", ylab="Heart weight", 
  main="Heart weight ~ Body weight"
)
abline(m2, col=2)
head(pred)
points(m2$model$Bwt, pred[,2], col=4, pch=3)
points(m2$model$Bwt, pred[,3], col=4, pch=3)
pred =  predict(m2, interval="predict")

```

---





















































































