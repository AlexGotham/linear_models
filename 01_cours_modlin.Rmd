# Régression linéaire simple

On cherche une relation entre la variable observée $Y$ et la variable explicative $x$. 

$$ Y \sim x $$

Les objectifs peuvent être multiples :

  * modèle explicatif

  * recherche d’une corrélation significative

  * modèles de prédiction

Domaines d’application : chimie, astronomie, sismologie, économie, démographie, épidémiologie, géographie, écologie, géologie, physique, médecine...

Historiquement, le modèle linéaire a été développé par Fisher, avec applications en génétique et en agronomie.

---

## Présentation du modèle

Considérons $x$ et $Y$ deux variables. Y est expliquée (modélisée) par  la variable explicative $x$. 

**Important**, dans le cadre des modèles linéaire Y est quantitatives (Ex : taille, poids, age...).

Dans le cas de la regression linéaire simple, $x$ est également quantitatives et la relation entre $Y$ et $x$ est une fonction **affine** (ou **linéaire**) de $x$ (Cf. programme du brevet des collèges).

\begin{eqnarray}
              \text{f : } \mathbb{R} & \rightarrow & \mathbb{R} \\
                                 x & \rightarrow & f(x)  =  ax + b \\ 
\end{eqnarray}

**Attention**, la fonction *abline* de R utilise la formule $bx+a$ ! Dans ce cours nous utliserons la notation $f(x)  = \beta_0 + \beta_1 x$



```{r  echo=FALSE, results="hide"}
data_nutri = read.table("data/data_nutri.csv", header=TRUE, sep=",", row.names = 1)
set.seed(1)
data_nutri_20 = data_nutri[sample(1:nrow(data_nutri), 20), ]
d = data_nutri_20

layout(matrix(1:2, 1), respect=TRUE)

plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille")
m = lm(poids~taille, d)
m$coefficients
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille", xlim=c(0,max(d$taille)), ylim=c(m$coefficients[[1]]-20, max(d$poids)))
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
# points(0,m$coefficients[[1]], pch=16)
abline(h=m$coefficients[[1]], v=0, lty=2)
axis(2, at=m$coefficients[[1]], "beta_0", las=2)

segments(50, predict(m, data.frame(taille=50)), 100, predict(m, data.frame(taille=50)), lty=2)
segments(100, predict(m, data.frame(taille=50)), 100, predict(m, data.frame(taille=100)), lty=2)
text(75, predict(m, data.frame(taille=50)), "50", pos=1)
text(100, predict(m, data.frame(taille=75)), "50*beta_1", pos=4)

```



Les $n$ observations vont permettre :

  - de construire le modèle (définir $\beta_0$ et $\beta_1$),
  - de quantifier l’adéquation du modèle aux données.


Remarques : 

  - On utilise un modèle linéaire Gaussien pour des observations pouvant être modélisées par une loi normales. Pour d’autres distributions (Poisson, Bernoulli...), on utilisera un modèle linéaire généralisé (GLM).
  - La régression linéaire se caractérise par des variables explicatives **quantitatives** (*e.g.*, $poids~taille$). L’ANOVA se caractérise par des variables explicatives  **qualitatives** (*e.g.*, $poids~sexe$).

---

**Exemple d’une relation déterministe**

La température $x$ en degrés Celsius explique entièrement la température  $y$ en degrés Farenheit par la relation : $y=32 + 9/5 x$.

\begin{eqnarray}
              \text{f : } \mathbb{R} & \rightarrow & \mathbb{R} \\
                                 x & \rightarrow & f(x)  = \beta_0 + \beta_1 x = 32 + 9/5 x \\ 
                                 0 & \rightarrow & f(0)  = 32 \\ 
                                 10 & \rightarrow & f(10)  = 50 \\ 
\end{eqnarray}



```{r  echo=FALSE, results="hide"}
layout(1, respect=TRUE)
beta_0 = 32
beta_1 = 9/5
d_f = -10:20
plot(d_f, beta_0 + beta_1 * d_f, main="Température", xlab="deg. C", ylab="deg. F", col=0)
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
abline(v=c(0,10), h=c(32,50), lty=2)

```



---

**Exemple d’une relation stochastique**


Dans le cas d’une variable aléatoire $Y$, il faut chercher la droite qui ajuste le mieux les observations et on ajoute un terme qui portera la partira non déterministe des observations.

$$Y   = \beta_0 + \beta_1x + \epsilon $$

$$Y_i   = \beta_0 + \beta_1x_i + \epsilon_i \ \ \ \ \  \forall i \in {1, ..., n}$$


où $\beta_0$ et $\beta_1$ sont des réels fixes, mais inconnus, et $\epsilon_i$ une valeur caractérisant le comportement individuel de l’observation $i$ (résidus).

Modèle : $$\mathbb{E} (Y|x) = \widehat Y = f(x) = \beta_0 + \beta_1 x$$

Par exemple $x$ est la taille, $Y$ le poids, à une même taille, plusieurs poids peuvent correspondrent. Les données ne sont plus alignées, mais s’ajustent autour de la droite de régression laissant apparaitre les résidus.

```{r}
layout(1, respect=TRUE)

plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille")
m = lm(poids~taille, d)
m$coefficients
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
suppressWarnings(arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("E(Y)", "epsilon"), col=c(2,4), lty=1, cex=.8)
```



---

## Estimation des paramètres

**Question** : comment estimer les paramètres de la droite de regression ?

**Réponse** : Par la méthode des moindres carrés.

---

**Droite de regression, estimation et résidus**

En analyse de régression linéaire, pour un individu $i$ :

- $x_i$ est la valeur de la varible explicative
- $Y_i$ est la valeur observée de la variable aléatoire
- la composante aléatoire de $Y_i$ est le $\epsilon_i$ correspondant
- la valeur de $Y_i$ estimée par le modèle est $\widehat Y_i$.

Ainsi : 

$$Y_i = \widehat Y_i + \epsilon_i$$


```{r echo=FALSE}
layout(1, respect=TRUE)
plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille", col=adjustcolor(1, alpha.f=0.5))
m = lm(poids~taille, d)
# abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
pred = predict(m, data.frame(taille=d$taille))
i = which(d$poids - pred == max(d$poids - pred))
points(d$taille[i], d$poids[i], pch=16, col=2)
abline(v=d$taille[i], h=c(d$poids[i], d$poids[i]-m$residuals[i]), lty=2, col=2)
axis(3, at=d$taille[i], "xi", las=2, col=2)
axis(4, at=c(d$poids[i], d$poids[i]-m$residuals[i]), c("Yi", expression(hat("Yi"))), las=2, col=2)
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=1) # /!\ y = b.x + a
suppressWarnings(arrows(d$taille[i], d$poids[i]-m$residuals[i], d$taille[i], d$poids[i], col=4, length=0.1, lwd=2))
text(d$taille[i], d$poids[i]-m$residuals[i]/2, "eps_i", col=4, las=2, pos=2, cex=2)
```

Lorsque $x = x_i$, alors $\widehat Y(x_i) = \widehat Y_i$ , c’est-à-dire :

$$ \widehat Y_i=\widehat{\beta}_0+ \widehat{\beta}_1x_i $$
$\widehat Y_i$ est appelée la valeur estimée par le modèle.

Les quantités inobservables :
$$\epsilon_i = Y_i - \beta_0 - \beta_1x_i$$ 
sont estimées par les quantités observables :
$$ e_i= Y_i-\widehat Y_i$$ 

- Les quantités $e_i$ sont appelées les **résidus du modèle**.
- La plupart des méthodes d’estimation cherchent à définir une droite qui minimise une fonction de résidus.
- La plus connue : **la méthode des moindres carrés ordinaires** (MCO).

---

**Intutition des moindres carrés**


Pour l’instant, la droite de régression est inconnue.
Tout le problème est d’estimer $\beta_0$ et $\beta_1$ à partir d’un échantillon de données.

On essaie de déterminer la **droite de regression** qui approche le mieux les données : 

$$\mathbb{E} (Y|x)) =  \widehat Y(x) = \widehat{\beta}_0 + \widehat{\beta}_1x$$

Avec $\widehat Y(x)$ un estimateur de la moyenne théorique $\mu_Y(x)$ ($\widehat Y(x)$ correspond à la moyenne de $Y$ mésurée sur tous les individus pour lesquels $x$ prend une valeur donnée). 

$\mu_Y(x)$ n’est ni observable, ni calculable (il faudrait alors recenser **tous** les individus de la population).

Remarque : la moyenne empirique de $Y$, définie par $\overline{Y}_n(x) = \frac{1}{n}\sum^n_{i=1}Y_i(x)$ est un autre estimateur de $\mu_Y(x)$ . Si le modèle est bon, $\widehat Y(x)$ est plus précis que $\overline{Y}_n(x)$ qui lui correspondrait au modèle *nul*, *i.e*, $Y$ ne depend pas de $x$.



```{r}
d = data_nutri_20
head(d)
dim(d)
layout(matrix(1:2, 1), respect=TRUE)
plot(d$taille, d$poids, main="poids~taille", ylab="poids", xlab="taille")
```

```{r echo=TRUE}
layout(1, respect=TRUE)
beta_0 = mean(d$poids)
beta_1 = 0
res =  d$poids - (beta_0 + beta_1*d$taille)
eqm = sum(res^2) / length(res)
plot(d$taille, d$poids, main=paste0("poids~taille, EQM=", round(eqm)), ylab="poids", xlab="taille")
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
arrows(d$taille, d$poids, d$taille, d$poids-res, length=0.1, col=adjustcolor(4, alpha.f=0.5))
```

```{r echo=TRUE}
layout(matrix(1:2, 1), respect=TRUE)
beta_0 = -100
beta_1 = 1
res =  d$poids - (beta_0 + beta_1*d$taille)
eqm = sum(res^2) / length(res)
plot(d$taille, d$poids, main=paste0("poids=", signif(beta_0, 3), "+", signif(beta_1, 3), " taille, EQM=", round(eqm)), ylab="poids", xlab="taille")
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
arrows(d$taille, d$poids, d$taille, d$poids-res, length=0.1, col=adjustcolor(4, alpha.f=0.5))

beta_0 = -16
beta_1 = 0.5
res =  d$poids - (beta_0 + beta_1*d$taille)
eqm = sum(res^2) / length(res)
plot(d$taille, d$poids, main=paste0("poids=", signif(beta_0, 3), "+", signif(beta_1, 3), " taille, EQM=", round(eqm)), ylab="poids", xlab="taille")
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
arrows(d$taille, d$poids, d$taille, d$poids-res, length=0.1, col=adjustcolor(4, alpha.f=0.5))
```




```{r echo=TRUE}
layout(1, respect=TRUE)
results = list()
for(beta_0 in 0:-260) {
  for(beta_1 in seq(0,2, length.out=21)) {
    res =  d$poids - (beta_0 + beta_1*d$taille)
    eqm = sum(res^2) / length(res)
    results[[length(results)+1]] = c(beta_0=beta_0, beta_1=beta_1, eqm=eqm)
  }
}
results = as.data.frame(do.call(rbind, results))
# plot(results$beta_0, results$beta_1, pch=16, col=colorRampPalette(c("cyan", "black", "red"))(100)[as.numeric(cut(results$eqm, breaks=quantile(results$eqm, probs=(0:100)/100), include.lowest=TRUE))])
r = results[results$eqm==min(results$eqm),]
beta_0 = r[["beta_0"]]
beta_1 = r[["beta_1"]]
res =  d$poids - (beta_0 + beta_1*d$taille)
eqm = sum(res^2) / length(res)
plot(d$taille, d$poids, main=paste0("poids=", signif(beta_0, 3), "+", signif(beta_1, 3), " taille, EQM=", round(eqm)), ylab="poids", xlab="taille")
abline(a=beta_0, b=beta_1, col=2) # /!\ y = b.x + a
suppressWarnings(arrows(d$taille, d$poids, d$taille, d$poids-res, length=0.1, col=adjustcolor(4, alpha.f=0.5)))
```



---

**Méthode  et démonstration**

L’objectifest de définir des estimateurs qui minimisent la somme des carrés des résidus (ou **erreur quadratique moyenne** EQM).

Les estimateurs sont donc les coordonnées du minimum de la fonction à deux variables :

$$EQM(\beta_0, \beta_1) = \frac{1}{n}\sum^{n}_{i=1} e^2_i = \sum^{n}_{i=1} (Y_i - \widehat Y_i)^2 = \sum^{n}_{i=1} (Y_i - \widehat\beta_0 -  \widehat\beta_1x_i)^2$$

Cette fonction est appelée la **fonction objectif**.


Les estimateurs correspondent aux valeurs annulant les dérivées partielles de cette fonction :

$$\frac{\delta EQM(\beta_0, \beta_1)}{\delta \beta_0} = -2 \sum (Y_i - \beta_0 - \beta_1x_i)$$
$$\frac{\delta EQM(\beta_0, \beta_1)}{\delta \beta_1} = -2 \sum x_i (Y_i - \beta_0 - \beta_1x_i)$$

Les estimateurs $(\widehat\beta_0^{MCO},\widehat\beta_1^{MCO})$ sont les solutions du système d’équation :
$$ \Bigg\{
\begin{align}
-2 \sum (Y_i - \beta_0 - \beta_1x_i) = 0  \\ 
-2 \sum x_i (Y_i - \beta_0 - \beta_1x_i) = 0
\end{align}
$$


Soient les equations suivantes : 

$$(1) \sum{Y}_i = n\widehat\beta_0 + \widehat\beta_1 \sum{x_i} $$

$$(2) \sum{x_iY_i} = \widehat\beta_0 \sum{x_i} + \widehat\beta_1 \sum{x^2_i}$$
On utilise les notations usuelles des moyennes empiriques $$\overline{x}_n = \frac{1}{n}\sum{x_i} ,\ \ \ \ \ \ \overline{Y}_n = \frac{1}{n}\sum{Y_i}$$

D’après (1): $$\widehat\beta_0 = \overline{Y}_n - \widehat\beta_1 \overline{x}_n$$

D’après (2) :

\begin{eqnarray}
\widehat\beta_1 \sum{x^2_i} & = & \sum{x_iY_i} - \widehat\beta_0 \overline{x}_n \\
                            & = & \sum{x_iY_i} - n\overline{x}_n\overline{Y}_n + \widehat\beta_1 (\overline{x}_n)^2
\end{eqnarray}

Ainsi $$\widehat\beta_1 = \frac{\sum{x_iY_i} -  n\overline{x}_n\overline{Y}_n}{\sum{x_i^2} -  n(\overline{x}_n)^2}$$

Et comme nous avons :

\begin{eqnarray}
\sum{(x_i-\overline{x}_n)(Y_i-\overline{Y}_n)} & = & \sum{x_iY_i} -  n\overline{x}_n\overline{Y}_n \\
                   \sum{(x_i-\overline{x}_n)}   & = & \sum{x_i^2} -  n(\overline{x}_n)^2
\end{eqnarray}

Nous obtenons : $$\widehat\beta_1 = \frac{\sum{(x_i-\overline{x}_n)(Y_i-\overline{Y}_n)}}{\sum{(x_i-\overline{x}_n)}}$$

Dans la pratique, nous calculons $\widehat\beta_1$ puis $\widehat\beta_0$.

Nous obtenons ainsi une estimation de la droite de régression, appelée la droite des moindres carrés ordinaires :

$$\widehat Y(x) = \widehat\beta_0 + \widehat\beta_1x$$

---

**Exemple**

$\widehat\beta_0$ correspond à l’ordonnée à l’origine. Ce paramètre n’est pas toujours interprétable (il dépend de la signifaction de $X$ et du fait que $X$ soit centré ou non).

$\widehat\beta_1$ correspond à la pente.

Attention, la technique des MCO crée des estimateurs sensibles aux valeurs atypiques.

```{r echo=TRUE, results="verbatim"}
layout(1, respect=TRUE)
plot(d$taille, d$poids, main="poids~taille")
## Model
# Y~X
# E(Y) = b.X
# E(Y) = b_0 + b_1.X
# Y_i = b_0 + b_1.X_i + e_i
m = lm(d$poids~d$taille)
m$coefficients
abline(a=m$coef[[1]], b=m$coef[[2]], col=2) # /!\ y = b.x + a
# residuals
# head(m$residuals)
arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)
```

---

## Qualité du modèle

### Décomposition de la variance


La variation de $Y$ vient du fait de sa dépendance à la variable explicative $X$. 
C’est la **variation expliquée par le modèle**.

Dans l’exemple $poids \sim taille$, nous avons remarqué que lorsque nous mesurons $Y$ avec une même valeur de $X$, nous observons une certaine variation sur $Y$. 
C’est la **variation inexpliquée par le modèle**.

On dit que la *variation totale de Y* se décompose en *Variation expliquée par le modèle* et *variation inexpliquée par le modèle*

Soit : 


\begin{eqnarray}
Y_i - \overline{Y}_n &=& Y_i - \widehat Y_i + \widehat Y_i - \overline{Y}_n \\
                    &=& (Y_i - \widehat Y_i) + (\widehat Y_i - \overline{Y}_n)
\end{eqnarray}

avec $\widehat Y_i - \overline{Y}_n$ la différence expliquée par le modèle et $Y_i - \widehat Y_i$ la différence inexpliquée (ou résiduelle).

La méthode des moindres carrés, en considérant la somme des carrés de ces différences, nous assure la décomposition de la variance totale en variance expliquée et variance résiduelle : 


\begin{eqnarray}

\sum(Y_i - \overline{Y}_n)^2 &=& \sum((Y_i - \widehat Y_i) + (\widehat Y_i - \overline{Y}_n))^2 \\
&=& \sum(Y_i - \widehat Y_i)^2 + \sum(\widehat Y_i - \overline{Y}_n)^2 + 2 \sum(Y_i - \widehat Y_i) (\widehat Y_i - \overline{Y}_n) \\
&=& \sum(\widehat Y_i - \overline{Y}_n)^2 + \sum(Y_i - \widehat Y_i)^2

\end{eqnarray}


$$  $$

Soit : 

$$ SC_{tot} = SC_{reg} + SC_{res}$$ 

Avec $SC_{tot}$ la somme des carrés totale,  $SC_{reg}$ la somme des carrés due à la régression et  $SC_{res}$ la somme des carrés des résidus.

Notons que $2 \sum(Y_i - \widehat Y_i) (\widehat Y_i - \overline{Y}_n)=0$ en raison des moindres carrés [[demo](https://www.unine.ch/files/live/sites/statistics/files/shared/documents/cours_modeles_regression.pdf)].


### Coefficient de determination $R^2$



La mesure du pourcentage de la variation totale expliquée par le modèle se fait par le **coefficient de détermination**.

$$ R^2 = \frac{SC_{reg}}{SC_{tot}}$$

- $R^2$ est compris entre 0 et 1.
- $R^2 = 1$ : cas où les données sont parfaitement alignées
(comme c’est le cas pour un modèle déterministe).
- $R^2 = 0$ : cas où la variation de $Y$ n’est pas due à la variation de $X$. Les données ne sont pas du tout alignées.
- Plus $R^2$  est proche de 1, plus les données sont alignées sur la droite de régression.



```{r}
layout(matrix(1:2, 1), respect=TRUE)
m = lm(d$poids~d$taille)
plot(d$taille, d$poids, main=paste0("poids~taille R^2=", signif(summary(m)$r.squared, 2)*100, "%"))
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
suppressWarnings(arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)

m = lm(d$poids~d$age)
plot(d$age, d$poids, main=paste0("poids~age R^2=", signif(summary(m)$r.squared, 2)*100, "%"))
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
suppressWarnings(arrows(d$age, d$poids, d$age, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)
```




### Travaux pratiques


**Jeu de données `CO2`**

```{r echo=TRUE, results="verbatim"}
data(CO2)
head(CO2)
layout(matrix(1:2, 1), respect=TRUE)
plot(
  CO2[CO2$Type=="Quebec", ]$conc, 
  CO2[CO2$Type=="Quebec", ]$uptake, 
  xlab="food", ylab="growth", main="growth~food"
)
plot(
  log10(CO2[CO2$Type=="Quebec", ]$conc), 
  CO2[CO2$Type=="Quebec", ]$uptake, 
  xlab="log(food)", ylab="growth", main="growth~log(food)"
)
```

```{r eval=FALSE}
m1 = lm(CO2[CO2$Type=="Quebec", ]$uptake~CO2[CO2$Type=="Quebec", ]$conc)
m2 = lm(CO2[CO2$Type=="Quebec", ]$uptake~log(CO2[CO2$Type=="Quebec", ]$conc))
layout(matrix(1:2, 1), respect=TRUE)
plot(
  CO2[CO2$Type=="Quebec", ]$conc, 
  CO2[CO2$Type=="Quebec", ]$uptake, 
  xlab="food", ylab="growth", 
  main=paste0("growth~food R^2=", signif(summary(m1)$r.squared, 2)*100, "%")
)
abline(m1, col=2)
plot(
  log10(CO2[CO2$Type=="Quebec", ]$conc), 
  CO2[CO2$Type=="Quebec", ]$uptake, 
  xlab="log(food)", ylab="growth", 
  main=paste0("growth~log(food) R^2=", signif(summary(m2)$r.squared, 2)*100, "%")
)
abline(m2, col=2)
summary(m1)
summary(m2)
```

On s’intéresse uniquement aux plantes dont le `Type` est `Quebec`.
 
- Calculer les modèles de regression linéaire `m1: growth~food` et `m2: growth~log(food)`
- Tracer les nuages de points et les droites de regression correspondantes.
- Calculer, comparer et commenter le pourcentage de variance expliquée de chaque modèle.
- Comparer les coéficiants de chacun des modéles.




**Jeu de données `anscombe`**

```{r echo=TRUE, results="verbatim"}
data(anscombe)
print(anscombe)

layout(matrix(1:4,2), respect=TRUE)
plot(anscombe$x1, anscombe$y1)
plot(anscombe$x2, anscombe$y2)
plot(anscombe$x3, anscombe$y3)
plot(anscombe$x4, anscombe$y4)
```

- Calculer les modèles de regression linéaire `m1 : y1~x1`, `m2 : y2~x2`, `m3 : y3~x3`, `m4 : y4~x4`.
- Tracer les nuages de points et les droites de regression correspondantes.
- Calculer, comparer et commenter les coéficiants de chacun des modéles.
- Calculer, comparer et commenter le pourcentage de variance expliquée de chaque modèle.

```{r eval=FALSE, echo=FALSE, results="hide"}
# create models
m1 = lm(anscombe$y1~anscombe$x1)
m2 = lm(anscombe$y2~anscombe$x2)
m3 = lm(anscombe$y3~anscombe$x3)
m4 = lm(anscombe$y4~anscombe$x4)

# model details
summary(m1)
summary(m2)
summary(m3)
summary(m4)

# plot
layout(matrix(1:4,2), respect=TRUE)
plot(anscombe$x1,anscombe$y1, xlim = c(3, 19), ylim = c(3, 13), main=paste0("m1 R^2=", signif(summary(m1)$r.squared, 2)*100, "%, y=", signif(m1$coefficients[1], 3), "+", signif(m1$coefficients[2], 3), "x")) ; abline(m1, col=2)
plot(anscombe$x2,anscombe$y2, xlim = c(3, 19), ylim = c(3, 13), main=paste0("m2 R^2=", signif(summary(m2)$r.squared, 2)*100, "%, y=", signif(m2$coefficients[1], 3), "+", signif(m2$coefficients[2], 3), "x")) ; abline(m2, col=2)
plot(anscombe$x3,anscombe$y3, xlim = c(3, 19), ylim = c(3, 13), main=paste0("m3 R^2=", signif(summary(m3)$r.squared, 2)*100, "%, y=", signif(m3$coefficients[1], 3), "+", signif(m3$coefficients[2], 3), "x")) ; abline(m3, col=2)
plot(anscombe$x4,anscombe$y4, xlim = c(3, 19), ylim = c(3, 13), main=paste0("m4 R^2=", signif(summary(m4)$r.squared, 2)*100, "%, y=", signif(m4$coefficients[1], 3), "+", signif(m4$coefficients[2], 3), "x")) ; abline(m4, col=2)
```





---

## Tests statistiques

Il est important de noter que la **construction du modèle de régression** et l’estimation des paramètres par MCO **ne fait pas appel aux hypothèses de distribution**.

Les hypothèses de distribution sont essentielles lorsqu’il s’agit de construire des tests et des intervalles de confiance et de prédiction.

Pour rappel, l’objectif d’un test statitsique est de :

  1. valider (ou non) une hypothèses,
  2. quantifier le risque associé à cette décision.
  
Souvent, on cherche à invalider l’hypothèse dîte *nulle* ($\mathcal{H}_0$) et donc valider l’hypothèse dîte *alternative* ($\mathcal{H}_1$) au risque de première espèce $\alpha$ de 5%.


---

### Test de Student


Le test de Student dans le cadre de la regression linéaire simple teste le coéfficient de la droite ®égression.

Il pose la question de l’effet de la variables $x$.


**Ecriture analytique du modèle**

$$Y = \beta_0 + \beta_1 x + \epsilon$$


**Hypothèse nulle et hypothèse alternative**

On se pose la question de l’effet de la variable $x$ :

$\mathcal{H}_0 : {\beta_1 = 0}$ contre $\mathcal{H}_1 : {\beta_1 \neq 0}$



**Conditions d’application du test**

- Les observations sont indépendantes
- La loi des erreurs est une loi normale
- La variance des erreurs est constante

**Statistique du test** 

Sous $\mathcal{H}_0$, 

$$t_{\widehat\beta_1,n-2} = \frac{\widehat\beta_1}{s_{\widehat\beta_1}} \sim \mathcal{T}_{n-2}$$

avec la variance résiduelle estimée :

$$ \widehat\sigma^2_n = \frac{1}{n-2}\sum^n_{i=1}  (Y_i - \widehat\beta_0 - \widehat\beta_1x_i)^2 = \frac{1}{n-2}\sum^n_{i=1} \widehat\epsilon_i^2 $$

et l’erreur standard sur $\beta_1$ estimée :

$$ s^2_{\widehat\beta_1} = \frac{\widehat\sigma^2_n}{ns^2_x} = \frac{\widehat\sigma^2_n}{\sum^n_{i=1} (x_i - \overline x)^2} $$



**Risque de première espèce $\alpha$ et p-valeur**

Le *risque de première espèce* ou risque $\alpha$ est le risque de rejeter l’hypothèse nulle $\mathcal{H}_0$ alors que celle-ci est vraie.

On prend généralement $\alpha=0.05$.

Notons que le risque de seconde espèce ou risque $\beta$ est le risque de conserver $\mathcal{H}_0$ alors que celle-ci est fausse. 
Attention, le calcul du risque $\beta$ est plus compliqué. Souvent en minimisant $\alpha$ on augmente $\beta$. 
Pour s’assurer que $\beta$ reste faible il faut généralement augmenter $n$.

La *p-valeur* est la probabilité d’observer la valeur de la statistique de test sous $\mathcal{H}_0$ en ayant préalablement vérifié les conditions d’application du test.

**Décision et conclusion du test**

La valeur critique du test, notée $c_\alpha$ est lue dans une table de la loi de Student (ou avec le logiciel R). 
C’est la valeur de la statistique de test pour laquelle $\mathcal{H}_0$ devient improbable compte tenu du risuqe $alpha$ que l’on s’est fixé. 

Ainsi : 

- si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $t_{\widehat\beta_1,n-2}$ est supérieure ou égale à $c_\alpha$, alors le test est significatif. Vous rejetez  $\mathcal{H}_0$ et vous décidez que $\mathcal{H}_1$ est vraie avec un risque d’erreur de première espèce $\alpha$.

- si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $t_{\widehat\beta_1,n-2}$ est strictement inférieure à $c_\alpha$, alors le test n’est pas significatif. Vous conservez $\mathcal{H}_0$ avec un risque d’erreur de deuxième espèce $\beta$ à évaluer. 




**Mais alors pourquoi ?** 

Si l’hypothèse nulle $\mathcal{H}_0$ est vérifiée, *i.e*. $\beta_1=0$, 
alors la variable aléatoire $t_{\widehat\beta_1,n-2} = \frac{\widehat\beta_1}{s_{\widehat\beta_1}}$, *i.e*, l’effet de la variable $x$ estimé à partir des observations *"réduit"* de la *"variabilité"* observée sur cet estimateur, 
suit une la loi de Student à $n-2$ dégrés de liberté notée $\mathcal{T}_{n-2}$ **en raison des conditions d’application du test**.



**Exemple**

On restreint le jeu de données à 20 observations prises aléatoirement parmi les observations du jeu de données `data_nutri`.
Etudions le modèle $poids \sim taille$, au risque $\alpha = 0.05$.


```{r}
d = data_nutri_20
m = lm(poids~taille, d)
```

```{r}
layout(matrix(1:2, 1), respect=TRUE)
plot( d$taille, d$poids, main="poids ~ taille")
suppressWarnings(arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)

abline(m, col=2)
x=seq(-7,7, length.out=200)
plot(x, dt(x, m$df.residual), type="l", xlab="t", ylab="", main=paste0("Distibution de Student, ", m$df.residual," ddl."))
abline(v=c(-1,1) * summary(m)$coefficients[2,3], lty=2, col=4)
abline(v=c(-1,1) * qt(0.025, m$df.residual), lty=2, col=2)
x = seq(-7,qt(0.025, m$df.residual), length.out=100)
polygon(c(-7, x, qt(0.025, m$df.residual)), c(0, dt(x, m$df.residual), 0), col=adjustcolor(2, alpha.f=.5), border=2, lwd=3)
x = seq(7,qt(0.975, m$df.residual), length.out=100)
polygon(c(7, x, qt(0.975, m$df.residual)), c(0, dt(x, m$df.residual), 0), col=adjustcolor(2, alpha.f=.5), border=2, lwd=3)
x = seq(-7,-summary(m)$coefficients[2,3], length.out=100)
polygon(c(-7, x, -summary(m)$coefficients[2,3]), c(0, dt(x, m$df.residual), 0), col=adjustcolor(4, alpha.f=.5), border=4, lwd=3)
x = seq(7,summary(m)$coefficients[2,3], length.out=100)
polygon(c(7, x, summary(m)$coefficients[2,3]), c(0, dt(x, m$df.residual), 0), col=adjustcolor(4, alpha.f=.5), border=4, lwd=3)
legend("topleft",c("t_beta1", "c_alpha", "p-value", "5%"), col=c(4,2, 4, 2), lty=c(2,2,1,1), lwd=c(1,1,3,3), cex=.8)

```



1. Ecriture analytique du modèle

$$poids = \beta_0 + \beta_1 taille + \epsilon$$

2. Formulation des hypothèses $\mathcal{H}_0$ et $\mathcal{H}_1$

$$\mathcal{H}_0 : {\beta_1 = 0}$ \text{ et } $\mathcal{H}_1 : {\beta_1 \neq 0}$$

3. Statistique de test utilisée et conditions d’application

Test de Student

4. Valeur de la statistique de test

`r summary(m)$coefficients[2,3]`

5. P-valeur associée

`r summary(m)$coefficients[2,4]`

6. Conclusion du test

La p-valeur est < 0.05, alors le test est significatif. Nous rejetons donc $\mathcal{H}_0$ et acceptons $\mathcal{H}_1$ au risque d’erreur de première espèce $\alpha = 0.05$.




```{r echo=TRUE, results="verbatim"}
m = lm(poids~taille, d)
# Student test
summary(m)
n = length(m$residuals)
xi = m$model$taille
s_beta1 = sqrt( sum(m$residuals^2)/(n-2) / sum((xi-mean(xi))^2))
s_beta1
stat_t =  m$coefficient[2]/s_beta1
stat_t
pval = pt(-stat_t, n-2) + 1-pt(stat_t, n-2)
pval
```
```{r eval=FALSE}
x=seq(0,50, length.out=30)[-1]
plot(x, dp(x, p-1,n-p), type="l")
abline(v=stat_f, lty=2, col="grey")

```



**TP `MASS::cats`**

On restreint le jeu de données à 10 observations prises aléatoirement parmi les observations du jeu de données `MASS::cats`.

```{r echo=TRUE}
set.seed(1)
d = d_mass_cats_10 = MASS::cats[sample(1:nrow(MASS::cats),10),]
m = lm(Hwt~Bwt, d)
```

Etudier le modèle le modèle $Hwt \sim Bwt$

1. Ecriture analytique du modèle
2. Formulation des hypothèses $\mathcal{H}_0$ et $\mathcal{H}_1$
3. Statistique de test utilisée et conditions d’application
4. Valeur de la statistique de test
5. P-valeur associée
6. Conclusion du test
7. Augmenter le nombre d’observations prisent alétoirement (20,30), quel est l’impact sur l’analyse ?


```{r}
layout(1, respect=TRUE)
plot( d$Bwt, d$Hwt, xlab="Body weight", ylab="Heart weight", main="Heart weight ~ Body weight")
suppressWarnings(arrows(d$Bwt, d$Hwt, d$Bwt, d$Hwt-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)
abline(m, col=2)
```

```{r eval=FALSE}
layout(matrix(1:2, 1), respect=TRUE)
plot( d$Bwt, d$Hwt, xlab="Body weight", ylab="Heart weight", main="Heart weight ~ Body weight")
suppressWarnings(arrows(d$Bwt, d$Hwt, d$Bwt, d$Hwt-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)

abline(m, col=2)
x=seq(-7,7, length.out=200)
plot(x, dt(x, m$df.residual), type="l", xlab="t", ylab="", main=paste0("Distibution de Student, ", m$df.residual," ddl."))
abline(v=c(-1,1) * summary(m)$coefficients[2,3], lty=2, col=4)
abline(v=c(-1,1) * qt(0.025, m$df.residual), lty=2, col=2)
x = seq(-7,qt(0.025, m$df.residual), length.out=100)
polygon(c(-7, x, qt(0.025, m$df.residual)), c(0, dt(x, m$df.residual), 0), col=adjustcolor(2, alpha.f=.5), border=2, lwd=3)
x = seq(7,qt(0.975, m$df.residual), length.out=100)
polygon(c(7, x, qt(0.975, m$df.residual)), c(0, dt(x, m$df.residual), 0), col=adjustcolor(2, alpha.f=.5), border=2, lwd=3)
x = seq(-7,-summary(m)$coefficients[2,3], length.out=100)
polygon(c(-7, x, -summary(m)$coefficients[2,3]), c(0, dt(x, m$df.residual), 0), col=adjustcolor(4, alpha.f=.5), border=4, lwd=3)
x = seq(7,summary(m)$coefficients[2,3], length.out=100)
polygon(c(7, x, summary(m)$coefficients[2,3]), c(0, dt(x, m$df.residual), 0), col=adjustcolor(4, alpha.f=.5), border=4, lwd=3)
legend("topleft",c("t_beta1", "c_alpha", "p-value", "5%"), col=c(4,2, 4, 2), lty=c(2,2,1,1), lwd=c(1,1,3,3), cex=.8)

d = d_mass_cats_10
m = lm(Hwt~Bwt, d)
# Student test
summary(m)
n = length(m$residuals)
xi = m$model$Bwt
s_beta1 = sqrt( sum(m$residuals^2)/(n-2) / sum((xi-mean(xi))^2))
s_beta1
stat_t =  m$coefficient[2]/s_beta1
stat_t
pval = pt(-stat_t, n-2) + 1-pt(stat_t, n-2)
pval
```



















### Test de Fisher

Le test de Fisher repose sur cette décomposition de la variance.

Il pose la question de l’effet de la variables $x$.


**Ecriture analytique du modèle**

$$Y = \beta_0 + \beta_1 x + \epsilon$$


**Hypothèse nulle et hypothèse alternative**

On se pose la question de l’effet de la variable $x$ :

$\mathcal{H}_0 : {\beta_1 = 0}$ contre $\mathcal{H}_1 : {\beta_1 \neq 0}$



**Conditions d’application du test**

- Les observations sont indépendantes
- La loi des erreurs est une loi normale
- La variance des erreurs est constante

**Statistique du test** 

La statistique de test $\mathcal{F}$ est obtenue à partir du tableau de l’analyse de la variance : 

| Source de variation   | sc        | ddl  |  cm  | $F_{obs}$
| :-------------------- | :-------: | :--: | :--: | ---:
| Régression            | $sc_{reg} = \sum^n_{i=1}(\widehat{y}_i - \overline{y})^2$    | p-1  | $\frac{sc_{reg}}{p-1}$ | $\frac{cm_{reg}}{cm_{res}}$
| Résiduelle            | $sc_{res} = \sum^n_{i=1}(y_i - \widehat{y})^2$    | n-p  | $\frac{sc_{res}}{n-p}$ 
| Totale            | $sc_{tot} = \sum^n_{i=1}(y_i - \overline{y})^2$    | n-1  



Sous $\mathcal{H}_0$, 


$$F_{obs} = \frac{CM_{reg}} {CM_{res}} \sim \mathcal{F}_{p-1, n-p}$$


**Risque de première espèce $\alpha$ et p-valeur**

Le *risque de première espèce* ou risque $\alpha$ est le risque de rejeter l’hypothèse nulle $\mathcal{H}_0$ alors que celle-ci est vraie.

La *p-valeur* est la probabilité d’observer la valeur de la statistique de test sous $\mathcal{H}_0$ en ayant préalablement vérifié les conditions d’application du test.

**Décision et conclusion du test**

La valeur critique du test, notée $c_\alpha$ est lue dans une table de la loi de Fisher (ou avec le logiciel R). 
C’est la valeur de la statistique de test pour laquelle $\mathcal{H}_0$ devient improbable compte tenu du risuqe $alpha$ que l’on s’est fixé. 

Ainsi : 

- si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $F_{obs}$ est supérieure ou égale à $c_\alpha$, alors le test est significatif. Vous rejetez  $\mathcal{H}_0$ et vous décidez que $\mathcal{H}_1$ est vraie avec un risque d’erreur de première espèce $\alpha$.

- si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $F_{obs}$ est strictement inférieure à $c_\alpha$, alors le test n’est pas significatif. Vous conservez $\mathcal{H}_0$ avec un risque d’erreur de deuxième espèce $\beta$ à évaluer. 




**Mais alors pourquoi ?** 

Si l’hypothèse nulle $\mathcal{H}_0$ est vérifiée, *i.e*. $\beta_1=0$,  $CM_{reg}$, *i.e* la part de variabilité expliquée par le modèle doit être petite (proche de 0).
Ainsi, la variable aléatoire $F_{obs} = \frac{CM_{reg}} {CM_{res}}$, doit être petite également.

Par ailleurs, en raison des conditions d’application du test, $F_obs$ suit une la loi de Fisher à $p-1$ et $n-p$ dégrés de liberté notée $\mathcal{F}_{p-1,n-p}$.





**Exemple**

On restreint le jeu de données à 20 observations prises aléatoirement parmi les observations du jeu de données `data_nutri`.
Etudions le modèle $poids \sim taille$, au risque $\alpha = 0.05$.


```{r}
d = data_nutri_20
m = lm(poids~taille, d)
```

```{r}
layout(matrix(1:2, 1), respect=TRUE)
plot( d$taille, d$poids, main="poids ~ taille")
suppressWarnings(arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)

abline(m, col=2)
x=seq(0,15, length.out=200)
plot(x, df(x, anova(m)[1,1], anova(m)[2,1]), type="l", xlab="t", ylab="", main=paste0("Distibution de Fisher,  ", anova(m)[1,1]," et ", anova(m)[2,1]," ddl."))
abline(v=anova(m)[1,4], lty=2, col=4)
abline(v=qf(0.95, anova(m)[1,1], anova(m)[2,1]), lty=2, col=2)
x = seq(15,qf(0.95, anova(m)[1,1], anova(m)[2,1]), length.out=100)
polygon(c(15, x, qf(0.95, anova(m)[1,1], anova(m)[2,1])), c(0, df(x, anova(m)[1,1], anova(m)[2,1]), 0), col=adjustcolor(2, alpha.f=.5), border=2, lwd=3)
x = seq(15,anova(m)[1,4], length.out=100)
polygon(c(15, x, anova(m)[1,4]), c(0, df(x, anova(m)[1,1], anova(m)[2,1]), 0), col=adjustcolor(4, alpha.f=.5), border=4, lwd=3)
legend("topleft",c("F_obs", "c_alpha", "p-value", "5%"), col=c(4,2, 4, 2), lty=c(2,2,1,1), lwd=c(1,1,3,3), cex=.8)
```



1. Ecriture analytique du modèle

$$poids = \beta_0 + \beta_1 taille + \epsilon$$

2. Formulation des hypothèses $\mathcal{H}_0$ et $mathcal{H}_1$

$$\mathcal{H}_0 : {\beta_1 = 0}$ \text{ et } $\mathcal{H}_1 : {\beta_1 \neq 0}$$

3. Statistique de test utilisée et conditions d’application

Test de Fisher

4. Valeur de la statistique de test

`r anova(m)[1,4]`

5. P-valeur associée

`r anova(m)[1,5]`

6. Conclusion du test

La p-valeur est < 0.05, alors le test est significatif. Nous rejetons donc $\mathcal{H}_0$ et acceptons $\mathcal{H}_1$ au risque d’erreur de première espèce $\alpha = 0.05$.




```{r echo=TRUE, results="verbatim"}
d = data_nutri_20
m = lm(poids~taille, d)
# Fisher test
anova(m)
# Sum Sq
y_hat = d$taille * m$coefficient[2] + m$coefficient[1]
y_bar = mean(d$poids)
sum((y_hat - y_bar)^2)
sum(m$res^2)
#Mean Sq 
sum((y_hat - y_bar)^2)
sum(m$res^2) / m$df.residual
#F value   
sum((y_hat - y_bar)^2) / (sum(m$res^2) / m$df.residual)
#Pr(>F)
1-pf(sum((y_hat - y_bar)^2) / (sum(m$res^2) / m$df.residual), 1, m$df.residual)
```




**TP `MASS::cats`**

On restreint le jeu de données à 10 observations prises aléatoirement parmi les observations du jeu de données `MASS::cats`.

```{r echo=TRUE}
set.seed(1)
d = d_mass_cats_10 = MASS::cats[sample(1:nrow(MASS::cats),10),]
m = lm(Hwt~Bwt, d)
```

Etudier le modèle le modèle $Hwt \sim Bwt$

1. Ecriture analytique du modèle
2. Formulation des hypothèses $\mathcal{H}_0$ et $\mathcal{H}_1$
3. Statistique de test utilisée et conditions d’application
4. Valeur de la statistique de test
5. P-valeur associée
6. Conclusion du test
7. Augmenter le nombre d’observations prisent alétoirement (20,30), quel est l’impact sur l’analyse ?


```{r}
layout(1, respect=TRUE)
plot( d$Bwt, d$Hwt, xlab="Body weight", ylab="Heart weight", main="Heart weight ~ Body weight")
suppressWarnings(arrows(d$Bwt, d$Hwt, d$Bwt, d$Hwt-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)
abline(m, col=2)
```






















### Tester les conditions d’application des tests

- Les observations sont indépendantes
- La variance des erreurs est constante $\sigma^2$
- La loi des erreurs est une loi normale $\mathcal{N}(0, \sigma^2)$


Pour tester la normalité des résidus : 

- Test de Shapiro-Wilk
- Test de Anderson-Darling
- Test de Jarque-Bera (package `tseries`, grands échantillons)
- Test de Kolmogorov-Smirnov (comparaison de distributions)
- QQ plot 

Pour tester l’homoscedasticité : 

- Test de Goldfeld-Quandt 
- Test de Hartley (??)
- Test de Bartlett (ANOVA)


```{r echo=TRUE, results="verbatim"}
shapiro.test(rnorm(100))
shapiro.test(runif(100))
shapiro.test(m$residuals)

ks.test(rnorm(100), rnorm(100))
ks.test(rnorm(100), runif(100))
ks.test(rnorm(100), m$residuals)

layout(matrix(1:6,2), respect=TRUE)
set.seed(1)
plot(sort(rnorm(10)  ), scale(sort(runif(10)  )))
abline(0, 1, col=2)
plot(sort(rnorm(10)  ), sort(rnorm(10)  ))
abline(0, 1, col=2)
plot(sort(rnorm(100) ), scale(sort(runif(100) )))
abline(0, 1, col=2)
plot(sort(rnorm(100) ), sort(rnorm(100) ))
abline(0, 1, col=2)
plot(sort(rnorm(1000)), scale(sort(runif(1000))))
abline(0, 1, col=2)
plot(sort(rnorm(1000)), sort(rnorm(1000)))
abline(0, 1, col=2)


layout(matrix(1:3,1), respect=TRUE)
qqnorm(scale(rnorm(100)))
abline(0, 1, col=2)
qqnorm(scale(runif(100)))
abline(0, 1, col=2)
qqnorm(scale(m$residuals))
abline(0, 1, col=2)
```







```{r eval=FALSE}
d = data_nutri_20

layout(matrix(1:2, 1), respect=TRUE)
m = lm(poids~taille, d)
plot(d$taille, d$poids, main=paste0("poids~taille R^2=", signif(summary(m)$r.squared, 2)*100, "%"))
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
suppressWarnings(arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)

summary(m)
shapiro.test(m$residuals)

n = length(m$residuals)
xi = m$model$taille
s_beta1 = sqrt( sum(m$residuals^2)/(n-2) / sum((xi-mean(xi))^2))
s_beta1
stat_t =  m$coefficient[2]/s_beta1
stat_t
pval = pt(-stat_t, n-2) + 1-pt(stat_t, n-2)
pval




x=seq(-7,7, length.out=200)
plot(x, dt(x, m$df.residual), type="l", xlab="t", ylab="", main=paste0("Distibution de Student, ", m$df.residual," ddl."))
abline(v=c(-1,1) * summary(m)$coefficients[2,3], lty=2, col=4)
abline(v=c(-1,1) * qt(0.025, m$df.residual), lty=2, col=2)
x = seq(-7,qt(0.025, m$df.residual), length.out=100)
polygon(c(-7, x, qt(0.025, m$df.residual)), c(0, dt(x, m$df.residual), 0), col=adjustcolor(2, alpha.f=.5), border=2, lwd=3)
x = seq(7,qt(0.975, m$df.residual), length.out=100)
polygon(c(7, x, qt(0.975, m$df.residual)), c(0, dt(x, m$df.residual), 0), col=adjustcolor(2, alpha.f=.5), border=2, lwd=3)
x = seq(-7,-summary(m)$coefficients[2,3], length.out=100)
polygon(c(-7, x, -summary(m)$coefficients[2,3]), c(0, dt(x, m$df.residual), 0), col=adjustcolor(4, alpha.f=.5), border=4, lwd=3)
x = seq(7,summary(m)$coefficients[2,3], length.out=100)
polygon(c(7, x, summary(m)$coefficients[2,3]), c(0, dt(x, m$df.residual), 0), col=adjustcolor(4, alpha.f=.5), border=4, lwd=3)
legend("topleft",c("t_beta1", "c_alpha", "p-value", "5%"), col=c(4,2, 4, 2), lty=c(2,2,1,1), lwd=c(1,1,3,3), cex=.8)



power.t.test(delta=m$coefficient[2], sd=s_beta1, sig.level = 0.05, power = 0.8)
power.t.test(delta=mean(v)-mean(p), sd=sd(c(v,c,p)), sig.level = 0.05, power = 0.8)


m = lm(d$poids~d$age)
plot(d$age, d$poids, main=paste0("poids~age R^2=", signif(summary(m)$r.squared, 2)*100, "%"))
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
suppressWarnings(arrows(d$age, d$poids, d$age, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1))
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)
```



```{r eval=FALSE}
# Note (vérifier les notations):
#
#
# - $\widehat\sigma^2_n$ : la variance résiduelle
# - $s^2_Y$ : la variance de l’échantillon des $Y_i$
# - $s^2_x$ : la variance de l’échantillon des $x_i$
# - $s^2_{\widehat\beta_1}$ : La variance de l’estimateur $\widehat\beta_1$
# - $s^2_{\widehat\beta_0}$ : La variance de l’estimateur $\widehat\beta_0$
#
#
# $$\widehat\sigma^2_n = \frac{n}{n-2}s^2_Y(1-R^2)$$
#
# Avec $s^2_Y = \frac{1}{n}(\sum^n_{i=1}Y_i^2)-(\overline{Y}_n)^2$ la variance de l’échantillon des $Y_i$ et $\overline{Y}_n$ la moyenne des $Y_i$.
#
# $$ s^2_{\widehat\beta_0} = \widehat\sigma^2_n\Big(\frac{1}{n}+\frac{\overline{x}_n}{ns^2_x}\Big)$$

s_beta0 = sqrt( sum(m$residuals^2)/(n-2) * ( 1/n +  mean(xi) / sum((xi-mean(xi))^2) ))
s_beta0

sum(m$residuals^2)/(n-2) * ( 1/n +  mean(xi) / sum((xi-mean(xi))^2) )

stat_t =  m$coefficient[1]/s_beta0
stat_t

s_beta1 = sqrt( 1/(n-2) * sum(m$residuals^2) / (sum((xi-mean(xi))^2)))
stat_t =  m$coefficient[2]/s_beta1
stat_t


# Fisher test
anova(m)
n = length(m$residuals)
p = m$rank
res_sum_sq = sum(m$residuals^2)
tot_sum_sq = sum((d$Hwt - mean(d$Hwt))^2)
fac_sum_sq = tot_sum_sq - res_sum_sq
res_mean_sq = res_sum_sq / (n-p)
fac_mean_sq = fac_sum_sq
stat_f = fac_mean_sq / res_mean_sq
stat_f
pval = 1-pf(stat_f, p-1, n-p)
pval
```














































**Travaux pratiques**

- Construire le modèle de régréssion linéaire sur la totalité des données `cats`.
- Tracer la droite de régression correspondante.
- Tester l’hypothèse $\mathcal{H}_0 : {\beta_1 = 0}$ contre $\mathcal{H}_1 : {\beta_1 \neq 0}$ sur ce modèle.
- Comparer la p-valeur obtenue avec ce modèle avec celle obtenue sur le modèle du cours (30 observations).
  
```{r echo=TRUE, results="verbatim"}
layout(1, respect=TRUE)
plot(
  MASS::cats$Bwt, MASS::cats$Hwt, 
  xlab="Body weight", ylab="Heart weight", 
  main="Heart weight ~ Body weight"
)
```

```{r eval=FALSE}
m2 = lm(Hwt~Bwt, MASS::cats)
abline(m, col="grey")
abline(m2, col=2)
shapiro.test(m2$residuals)
summary(m2)
``` 

---

### Intervalle de confiance

On peut construire les intervalles de confiance suivants

$$IC_{1-\alpha}(\widehat\beta_1) = \Big]\widehat\beta_1 - t_{n-2;1-\alpha/2}*s_{\beta_1}; \widehat\beta_1 + t_{n-2;1-\alpha/2}*s_{\beta_1}\Big[$$

$$IC_{1-\alpha}(\widehat\beta_0) = \Big]\widehat\beta_0 - t_{n-2;1-\alpha/2}*s_{\beta_0}; \widehat\beta_0 + t_{n-2;1-\alpha/2}*s_{\beta_0}\Big[$$


```{r echo=TRUE, results="verbatim"}
confint(m)
```

Ainsi que l’intervalle de prédiction d’une valeur moyenne de $Y$, sachant que $x = x_0$. 

L’estimation ponctuelle pour cette valeur de $x_0$ est alors égale a $\widehat Y_0 = \widehat\beta_0 + \widehat\beta_1x_0$

$$IP_{1-\alpha}(Y) = \Big]\widehat Y_0 - t_{n-2;1-\alpha/2}\sqrt{\widehat\sigma^2_n\Big(\frac{1}{n}+\frac{(x_0-\overline{x}_n)^2}{(n-1)s^2_x}\Big)}; \widehat Y_0 + t_{n-2;1-\alpha/2}*\sqrt{\widehat\sigma^2_n\Big(\frac{1}{n}+\frac{(x_0-\overline{x}_n)^2}{(n-1)s^2_x}\Big)}\Big[$$


```{r echo=TRUE, results="verbatim"}
layout(1, respect=TRUE)
plot(
  d$Bwt, d$Hwt, 
  xlab="Body weight", ylab="Heart weight", 
  main="Heart weight ~ Body weight"
)
abline(m, col=2)

x = seq(min(m$model$Bwt), max(m$model$Bwt), length.out=100)
x = seq(2.5, 3, length.out=100)
pred =  predict(m, interval="predict", level=0.95)
head(pred)
points(m$model$Bwt, pred[,2], col=2, pch=3)
points(m$model$Bwt, pred[,3], col=2, pch=3)
pred =  predict(m, interval="predict", level=0.99)
points(m$model$Bwt, pred[,2], col=3, pch=3)
points(m$model$Bwt, pred[,3], col=3, pch=3)
pred =  predict(m, interval="predict", level=0.90)
points(m$model$Bwt, pred[,2], col=4, pch=3)
points(m$model$Bwt, pred[,3], col=4, pch=3)
legend("topleft",c("95% ci", "99% ci", "90% ci"), col=2:4, pch=3)

```

**Travaux pratiques**


- Tracer l’intervalle de prédiction du modèle obtenu sur la totalité des données `cats`.
- Comparer la l’intervalle de prédiction obtenu avec ce modèle avec celui obtenu sur le modèle du cours (30 observations).

```{r eval=FALSE}
layout(1, respect=TRUE)
m2 = lm(MASS::cats$Hwt~MASS::cats$Bwt)
pred =  predict(m2, interval="predict")
plot(
  MASS::cats$Bwt, MASS::cats$Hwt, 
  xlab="Body weight", ylab="Heart weight", 
  main="Heart weight ~ Body weight"
)
abline(m2, col=2)
head(pred)
points(m2$model$Bwt, pred[,2], col=4, pch=3)
points(m2$model$Bwt, pred[,3], col=4, pch=3)
pred =  predict(m2, interval="predict")

```


