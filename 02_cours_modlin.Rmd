# Régression linéaire multiple

L’objectif de cette section est : 

  - de généraliser les concepts vus avec la regression linéaire simple dans un cadre **multivarié**
  - mettre en perspective les modèles linéaires et l’analyse de données (**ACP**)
  - différencier le test de Student du test de Fisher
  
---

La regression linéaire multiple characterise la relation entre une variable **quantitatives** observée $Y$ et plusieurs variables **quantitatives** explicatives $X_1, ..., X_p$. 

$$ Y \sim X_1 +  ... + X_p $$


## Présentation du modèle

Y est expliquée (modélisée) par  les variables explicatives $X_1, ..., X_p$, la formulation aanlytique de la régression linéaire simple se généraliser ainsi :

$$Y  = \beta_0 + \beta_1X_1 + ... + \beta_pX_p + \epsilon  $$

En pratique, dans un échantillon de $n$ individus, nous mesurons $y_i, x_{i,1},...,x_{i,p}$ pour $i=1,...,n$.

---

**Exemple**

Le jeu de données `Life.Exp` comprend les mesures **quantitaves** suivantes prisent sur 50 états des Etats-Unis.

- **Life.Exp** : espérance de vie moyenne (1969-1971)
- Murder :  taux d’homicide pour 100 000 individus (1976)
- HS Grad :  pourcentage de diplômés niveau baccalauréat --high-school graduates-- (1970)
- Frost : nombre de jours moyens avec des températures inférieures à 0$^o$C dans les grandes villes (1931-1960)
- Population : population estimée au 1er juillet 1975
- Income : revenu par individu (1974)
- Illiteracy : illettrisme (1970, pourcentage de la population)



```{r echo=TRUE, results="verbatim"}
data(state)
d = state_us =  data.frame(state.x77, row.names=state.abb)[,c(4:7, 1:3)]
head(d)
dim(d)
```

On étudie la variables *Life.Exp* en fonction des variables *Murder*, *HS.Grad* et *Frost*

$$Life.Exp \sim Murder + HS.Grad + Frost$$


La formulation analytique du modèle est :

$$Life.Exp = \beta_0 + \beta_{Murder} Murder + \beta_{HS.Grad} HS.Grad + \beta_{Frost} Frost + \epsilon$$





```{r fig.width=6, eval=FALSE}
# Voyons comment ces variables sont corrélées entre elles :
layout(1, respect=TRUE)
pairs(d[,1:4])
```

---

















## Estimation des paramètres

### Méthode des moindres carrés


On cherche à estimer les paramètres $\beta_0, ..., \beta_p$ du modèle de regression, à partir des observations.

On généralise en $p$ dimensions la méthode des moindres carrés vu dans le cadre de la régression linéaire simple.

Ceci revient à minimiser la quantité suivante :

\begin{eqnarray}
min_{\beta_0, ...,\beta_p}\sum^n_{i=1} \epsilon_i ^2
  &=&min_{\beta_0, ...,\beta_p}\sum^n_{i=1}\Big(y_i -\widehat y_i \Big)^2 \\
  &=&min_{\beta_0, ...,\beta_p}\sum^n_{i=1}\Big(y_i -(\beta_0+ \beta_1X_1 + ... + \beta_pX_p)\Big)^2
\end{eqnarray}


**Mise en oeuvre sous R**

Dans notre exemple `Life.Exp` : 

```{r echo=TRUE, results="verbatim"}
d = state_us
m = lm(Life.Exp ~ Murder + HS.Grad + Frost, d)
m$coef
```



**La formulation matricielle du problème**

Le système peut se réecrire :

$$\left(\begin{array}
{rrr}
y_1\\
\vdots\\
y_n
\end{array}\right) = \left(\begin{array}
{rrrr}
1 & x_{1,1} & ... & x_{1,p}\\
\vdots & \vdots & \vdots & \vdots \\
1 & x_{n,1} & ... & x_{n,p}
\end{array}\right)  \left(\begin{array}
{rrr}
\beta_0\\
\vdots\\
\beta_p
\end{array}\right) + \left(\begin{array}
{rrr}
\epsilon_1\\
\vdots\\
\epsilon_n
\end{array}\right)
$$

Avec $y = X\beta + \epsilon$ 

Et le vecteur des résidus $\widehat{e} = y - \widehat{y} = y - X\widehat\beta$


Les variables $y$ et $X$ sont mesurées tandis que l’estimateur $\beta$  est à déterminer.
La **méthode des moindres carrés ordinaires** consiste à trouver le vecteur $\beta$  qui minimise $||\epsilon||^2 =  {}^t{\epsilon}\epsilon$.  


La formulation matricielle du problème montre qu’idéalement $n > p$ et la matice des résidus est inversible.
 
Qu’est-ce qui pourrait rendre la matrice non inversible ?

Si $p >> n$, il faudra mettre en oeuvre des statégies de **sélection de variables** et/ou de **réduction de dimensions** et par conséquent contrôler la surinterprétation (surapprentisage, *overfitting*).


---











**Développements** 

\begin{eqnarray}
         ||\epsilon||^2    & = & {}^t(y-X\widehat\beta) (y-X\widehat\beta)            \\
                           & = & {}^tyy - {}^t\widehat\beta{}^tXy - {}^tyX\widehat\beta + {}^t\widehat\beta{}^tXX\widehat\beta           \\
                           & = & {}^tyy - 2{}^t\widehat\beta^t(X)y - {}^t\widehat\beta{}^tXX\widehat\beta     
\end{eqnarray}

comme ${}^t\widehat\beta{}^tXy$ est un scalaire, il est égal à sa transposée.

La dérivée par rapport à $\beta$ est alors égale à : $-2{}^tXy+2{}^tXX\widehat\beta$.



**Objectif** :  Nous cherchons $\beta$  qui annule cette dérivée. Donc nous devons résoudre l’équation suivante :

$$^tXX\widehat\beta = {}^tXy$$
Nous trouvons après avoir inversé la matrice ${}^tXX$ (il faut naturellement vérifier que ${}^tXX$ est carrée et inversible c’est-à-dire qu’aucune des colonnes qui compose cette matrice ne soit proportionnelle aux autres colonnes)
$$\widehat\beta  = ({}^t XX)^{-1}{}^{t} Xy.$$


**Application** : cas simple avec $p=2$ 

$$\left(\begin{array}
{rrr}
{}^tXX
\end{array}\right) = \left(\begin{array}
{rrr}
n & \sum{x_i}\\
\sum{x_i} & \sum{x_i}^2 
\end{array}\right) ;  \left(\begin{array}
{rrr}
{}^tXy
\end{array}\right) = \left(\begin{array}
{rrr}
\sum{y_i}\\
\sum{x_iy_i}
\end{array}\right)
$$

Donc

$$\left(\begin{array}
{rrr}
({}^tXX)^{-1}
\end{array}\right) = \frac{1}{n\sum{x_i^2 - (\sum{x_i^2})^2}}
\left(\begin{array}
{rrr}
\sum{x_i}^2 & -\sum{x_i}\\
-\sum{x_i} & n 
\end{array}\right) 
=  \frac{1}{\sum{x_i - \overline{x}_n}}
\left(\begin{array}
{rrr}
\sum{x_i}^2/n & -\overline{x}_n\\
-\overline{x}_n & 1 
\end{array}\right) 
$$

---






















### Maximum de vraisemblance

On peut également trouver les paramètres $\beta$ qui maximisent la vraisemblance.

La vraisemblance est la **Probabilité** d’observer l’évènement $Y$, sachant les observations $X$, en considérant le modèle $\mathcal{M}$

**Important** : la vraisemblance dépend

  - des observations ($Y_i, X_{ij}, i\in1..n, j\in1..p$)
  - du modèle $\mathcal{M}$ et de ses hypothèses (conditions d’application).

  
On écrit : 

\begin{eqnarray}
\mathcal{L}_{\mathcal{M}, Y,X} &=& \mathbb{P}_\mathcal{M}(Y|X) \\
                               &=& \prod^n_{i=1} \mathbb{P}_\mathcal{M}(Y_i|X_i)\\
\end{eqnarray}

On calcule : 
                               
\begin{eqnarray}
log\mathcal{L}_{\mathcal{M}, Y,X} &=& log(\mathbb{P}_\mathcal{M}(Y|X)) \\
                                  &=& \sum^n_{i=1} log(\mathbb{P}_\mathcal{M}(Y_i|X_i))\\
\end{eqnarray}

---

Dans le cadre de la régression  linéaire multiple (**normalité des résidus**), la vraisemblance vaut : 

\begin{eqnarray}
\mathcal{L}(Y,\beta,\sigma^2) &=& \Big(\frac{1}{\sqrt{2\pi\sigma^2}}\Big)^nexp\Big[\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-x_i\beta_i)^2\Big]\\
                              &=& \Big(\frac{1}{\sqrt{2\pi\sigma^2}}\Big)^nexp\Big[\frac{1}{2\sigma^2}||Y-X\beta||^2\Big]
\end{eqnarray}


D’où l’on déduit la log-vraisemblance

$$log\mathcal{L}(Y,\beta,\sigma^2) = \frac{n}{2}log(2\pi) -\frac{n}{2}log(\sigma^2) - \frac{1}{2\sigma^2}||Y-X\beta||^2$$

Maximiser la log-vraisemblance revient à minimiser la quantité $||Y-X\beta||^2$, ce qui est l’objectif des *moindres carrés ordinaires*.






---

**Mise en oauvre sous R** 


```{r ex1, echo=TRUE, results="verbatim"}
d = state_us
m = lm(Life.Exp ~ Murder + HS.Grad + Frost, d)
logLik(m)

# LLR rapport des vraisemblances ~ Chi2
anova(m, lm(Life.Exp ~ 1, d), test="Chisq")
```

---



















## Qualité du modèle

### Décomposition de la variance 
La variation de $Y$ vient du fait de sa dépendance aux variables explicatives $X$. C’est la **variation expliquée par le modèle**.


Variation totale de Y = Variation expliquée par le modèle + Variation inexpliquée par le modèle


La méthode des moindres carrés, en considérant la somme des carrés de ces différences, nous assure la décomposition de la variance totale en variance expliquée et variance résiduelle :

$$ \sum(y_i - \overline{y})^2 = \sum(\widehat{y}_i - \overline{y})^2 + \sum(y_i - \widehat{y}_i)^2  $$

Soit : 

$$ SC_{tot} = SC_{reg} + SC_{res}$$ 

Avec $SC_{tot}$ la somme des carrés totale,  $SC_{reg}$ la somme des carrés due à la régression et  $SC_{res}$ la somme des carrés des résidus.


### $R^2$


La mesure du pourcentage de la variation totale expliquée par le modèle se fait par le **coefficient de détermination**.

$$ R^2 = \frac{SC_{reg}}{SC_{tot}}$$


**Attention**, l’ajout de variables explicatives augmente systématiquement le pourcentage de variance expliquée. Le $R^2$ s’en trouve systématiquement augmenté san pour autant que le modèle qui en résulte soit pertinent. 


**Exemple**

On calcule chacun des modèles de regression linéaire univariés permettant d’expliquer la variable *Life.Exp*.

Pour chacun des modèles, on calculer le $R^2$. Que peut-on dire ?


```{r fig.height=3}
m_Murder     = lm(Life.Exp ~ Murder , d)
m_HS.Grad    = lm(Life.Exp ~ HS.Grad, d)
m_Frost      = lm(Life.Exp ~ Frost  , d)
layout(matrix(1:3, 1, byrow=TRUE), respect=TRUE)
plot(d$Murder , d$Life.Exp, main=paste0("Life.Exp~Murder" , ", R^2=", signif(summary(m_Murder )$r.squared, 3))); abline(m_Murder , col=2)
plot(d$HS.Grad, d$Life.Exp, main=paste0("Life.Exp~HS.Grad", ", R^2=", signif(summary(m_HS.Grad)$r.squared, 3))); abline(m_HS.Grad, col=2)
plot(d$Frost  , d$Life.Exp, main=paste0("Life.Exp~Frost"  , ", R^2=", signif(summary(m_Frost  )$r.squared, 3))); abline(m_Frost  , col=2)
```

On calcule ensuite les modèles de regression linéaire ci-après. Que peut-on dire ?

```{r echo=TRUE, results="verbatim"}
summary(lm(Life.Exp ~ Murder                  , d))$r.squared
summary(lm(Life.Exp ~ Murder + HS.Grad        , d))$r.squared
summary(lm(Life.Exp ~ Murder + HS.Grad + Frost, d))$r.squared
```



```{r et_pairs, echo=FALSE, fig.height=9}
et_pairs = function (...) 
{
    panel.cor = function(x, y, digits = 2, prefix = "", cex.cor, ...) {
        usr = par("usr")
        on.exit(par(usr))
        par(usr = c(0, 1, 0, 1))
        r = abs(cor(x, y))
        txt = format(c(r, 0.123456789), digits = digits)[1]
        txt = paste(prefix, txt, sep = "")
        if (missing(cex.cor)) 
            cex = 0.8/strwidth(txt)
        test = cor.test(x, y)
        Signif = symnum(test$p.value, corr = FALSE, na = FALSE, 
            cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c("***", 
                "**", "*", ".", " "))
        text(0.5, 0.5, txt, cex = cex * r)
        text(0.8, 0.8, Signif, cex = cex, col = 2)
    }
    pairs(..., lower.panel = panel.cor)
}

layout(1, respect=TRUE)
et_pairs(d[,1:4])
```

**Décomposition de variance et analyse en composante principale.**

Les graphiques suivants présentent les résultats de l’analyse en composante principale pour notre jeu de données.

Que voit-on ?

```{r echo=FALSE, eval=TRUE}
d = state_us
pca = prcomp(d[,1:4], scale=TRUE)
v = pca$sdev * pca$sdev
p = v / sum(v) * 100

layout(matrix(1:2,1), respect=TRUE)
barplot(p, ylab="% of explained variance", xlab="PCs")
i = 1
j = i+1
scale_factor = min(abs(c(min(c(pca$x[,i], pca$x[,j])), max(c(pca$x[,i], pca$x[,j])))))  
plot(pca$x[,i], pca$x[,j], xlab=paste0("PC", i, "(", signif(p[i], 3), "%)"), ylab=paste0("PC", j, "(", signif(p[j], 3), "%)"), pch=16, col="grey", 
  xlim=c(min(c(-scale_factor, pca$x[,i])), max(c(scale_factor, pca$x[,i]))),
  ylim=c(min(c(-scale_factor, pca$x[,j])), max(c(scale_factor, pca$x[,j])))
)  
# plotrix::draw.ellipse(0,0,scale_factor,scale_factor, lty=2, border="grey")
arrows(0,0,pca$rotation[,i]*scale_factor, pca$rotation[,j]*scale_factor, length=.1)
text(pca$rotation[,i]*scale_factor, pca$rotation[,j]*scale_factor, rownames(pca$rotation), col=2)

```



---









































## Tests statistiques

### Test de Student


Le test de Student dans le cadre de la regression linéaire multiple teste un des coéfficients du modèle de régression (*e.g.* le *k*-ième).

Il pose la question de l’effet de la variables $X_k$ sur $Y$.


**Ecriture analytique du modèle**

$$Y = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k + ... + \beta_p X_p + \epsilon$$


**Hypothèse nulle et hypothèse alternative**

On se pose la question de l’effet de la variable $X_k$ :

$$\left  \lbrace
\begin{array}{l}
\mathcal{H}_0 : {\beta_k = 0} \\
\mathcal{H}_1 : {\beta_k \neq 0} 
\end{array}
\right.$$



**Conditions d’application du test**

- Les observations sont indépendantes
- La loi des erreurs est une loi normale
- La variance des erreurs est constante

**Statistique du test** 

Sous $\mathcal{H}_0$, 

$$t_{\widehat\beta_k,n-2} = \frac{\widehat\beta_k}{s_{\widehat\beta_k}} \sim \mathcal{T}_{n-2}$$


**Risque de première espèce $\alpha$ et p-valeur**

Le *risque de première espèce* ou risque $\alpha$ est le risque de rejeter l’hypothèse nulle $\mathcal{H}_0$ alors que celle-ci est vraie.

On prend généralement $\alpha=0.05$.


**Décision et conclusion du test**

Critère, la *p-valeur* est la probabilité d’observer la valeur de la statistique de test sous $\mathcal{H}_0$ en ayant préalablement vérifié les conditions d’application du test.

Ainsi :

  - si la p-valeur calculée est plus petite que notre risque $\alpha$ fixé, alors le test est significatif, $\mathcal{H}_0$ est improbable, on rejete  $\mathcal{H}_0$ et on décide que $\mathcal{H}_1$ est vraie avec un risque d’erreur de première espèce $\alpha$.

  - si la p-valeur calculée est plus grande que notre risque $\alpha$ fixé, alors le test n’est pas significatif, on conserve $\mathcal{H}_0$ avec un risque d’erreur de deuxième espèce $\beta$ à évaluer. 




**Exemple**


```{r echo=TRUE, results="verbatim"}
d = state_us
m = lm(Life.Exp ~ Murder + HS.Grad + Frost, d)
summary(m)
```

---






















### Test de Fisher

Le test de Fisher repose sur la décomposition de la variance.

Il pose la question de l’effet **des** variables $X$.


**Ecriture analytique du modèle**

$$Y = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k + ... + \beta_p X_p + \epsilon$$


**Hypothèses nulle et hypothèse alternative**



$$\left  \lbrace
\begin{array}{l}
\mathcal{H}_0 : {\beta_1 = ... = \beta_p = 0}\\
\mathcal{H}_1 : \exists j \in 1..p \text{ tel que } \beta_j \neq 0
\end{array}
\right.$$


*Remarque* : Si l’hypothèse nulle est vérifiée, alors $y_i = \beta_0 + \epsilon_i$



**Conditions d’application du test**

- Les observations sont indépendantes
- La loi des erreurs est une loi normale
- La variance des erreurs est constante

**Statistique du test** 

La statistique de test $\mathcal{F}$ est obtenue à partir du tableau de l’analyse de la variance : 

| Source de variation   | sc        | ddl  |  cm  | $F_{obs}$
| :-------------------- | :-------: | :--: | :--: | ---:
| Régression            | $sc_{reg} = \sum^n_{i=1}(\widehat{y}_i - \overline{y})^2$    | p-1  | $\frac{sc_{reg}}{p-1}$ | $\frac{cm_{reg}}{cm_{res}}$
| Résiduelle            | $sc_{res} = \sum^n_{i=1}(y_i - \widehat{y})^2$    | n-p  | $\frac{sc_{res}}{n-p}$ 
| Totale            | $sc_{tot} = \sum^n_{i=1}(y_i - \overline{y})^2$    | n-1  



Sous $\mathcal{H}_0$, 


$$F_{obs} = \frac{CM_{reg}} {CM_{res}} \sim \mathcal{F}_{p-1, n-p}$$


**Risque de première espèce $\alpha$ et p-valeur**

Le *risque de première espèce* ou risque $\alpha$ est le risque de rejeter l’hypothèse nulle $\mathcal{H}_0$ alors que celle-ci est vraie.

La *p-valeur* est la probabilité d’observer la valeur de la statistique de test sous $\mathcal{H}_0$ en ayant préalablement vérifié les conditions d’application du test.

**Décision et conclusion du test**

La valeur critique du test, notée $c_\alpha$ est lue dans une table de la loi de Fisher (ou avec le logiciel R). 
C’est la valeur de la statistique de test pour laquelle $\mathcal{H}_0$ devient improbable compte tenu du risuqe $alpha$ que l’on s’est fixé. 

Ainsi : 

- si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $F_{obs}$ est supérieure ou égale à $c_\alpha$, alors le test est significatif. Vous rejetez  $\mathcal{H}_0$ et vous décidez que $\mathcal{H}_1$ est vraie avec un risque d’erreur de première espèce $\alpha$.

- si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $F_{obs}$ est strictement inférieure à $c_\alpha$, alors le test n’est pas significatif. Vous conservez $\mathcal{H}_0$ avec un risque d’erreur de deuxième espèce $\beta$ à évaluer. 





**Exemple**


```{r echo=TRUE, results="verbatim"}
d = state_us
m = lm(Life.Exp ~ Murder + HS.Grad + Frost, d)
summary(m)
anova(m)

m = lm(Life.Exp ~ HS.Grad + Murder + Frost, d)
summary(m)
anova(m)

anova(lm(Life.Exp ~ Illiteracy + Murder, d))
anova(lm(Life.Exp ~ Murder + Illiteracy, d))
```

---
















**TP `ISwR::cystfibr`**

Les données `ISwR::cystfibr` concernent la fonction respiratoire de patients atteints de fibrose kystique (mucoviscidose).

On cherche à expliquer la variable `pemax` qui caracterise cette maladie.

```{r echo=TRUE, results="verbatim"}
# ?ISwR::cystfibr
d = ISwR::cystfibr
head(d)
dim(d)
# removing sex
d = d[,-2]
```


1) On étudie la corrélation des variables à l’aide du code suivant. Que peut-on dire ?

```{r echo=TRUE, fig.width=9, fig.height=9, eval=TRUE}
pairs(d, gap=0)
```

2) Le ciode suivant réaliser l’analyse en composante principale du tableau de données. Commenter le graphique obtenu.

```{r echo=TRUE, eval=TRUE, fig.height=5}
# mat = x[,-9]
mat = d
pca = prcomp(mat, scale=TRUE)
v = pca$sdev * pca$sdev
p = v / sum(v) * 100

# layout(matrix(1:6,2), respect=TRUE)
layout(matrix(1:2,1), respect=TRUE)
barplot(p)

for (i in 1:5) {
  j = i+1
  plot(pca$x[,i], pca$x[,j], xlab=paste0("PC", i, "(", signif(p[i], 3), "%)"), ylab=paste0("PC", j, "(", signif(p[j], 3), "%)"), pch=16)  
  scale_factor = min(abs(c(min(c(pca$x[,i], pca$x[,j])), max(c(pca$x[,i], pca$x[,j])))))  
  # scale_factor = min(abs(c(max(min(pca$x[,i]), min(pca$x[,j])), min(max(pca$x[,i]), max(pca$x[,j])))))
  arrows(0,0,pca$rotation[,i]*scale_factor, pca$rotation[,j]*scale_factor, col="grey")
  text(pca$rotation[,i]*scale_factor, pca$rotation[,j]*scale_factor, rownames(pca$rotation))
}
```

3) Compléter le code suivant afin de proposer un modèle expliquant la variable `pemax`. Détaillez votre méthodologie. Discuter et interpréter le modèle obtenue d’un point de vue statistique et du point de vue biologique.

```{r echo=TRUE, results="verbatim"}
d = ISwR::cystfibr
model_formula = formula(paste0("pemax~", "weight"))
print(model_formula)
m = lm(model_formula, d)
summary(m)
anova(m)
```

Vous pouvez par exemple construire différents modèles en ajoutant les variables une à une et en suivant l’évolution du $R^2$ au fur et à mesure de la construction du modèle.

4) Le code suivant execute une procédure qui réalise automatiquement la selection des variables d’intéret. Commenter la procédure.

```{r echo=TRUE, results="verbatim"}
m_lo = lm(pemax ~ 1, data=d)
m_up = lm(pemax ~ ., data=d)
m_step = step(m_lo, method="forward", scope=list(upper=m_up,lower=m_lo))

m_step$anova
summary(m_step)

m_w = lm(pemax ~ weight, data=d)
summary(m_w)
m_wb = lm(pemax ~  weight + bmp, data=d)
summary(m_wb)
m_wbf = lm(pemax ~  weight + bmp + fev1 , data=d)
summary(m_wbf)
summary(m_w)
```


























