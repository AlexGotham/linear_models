# Régression linéaire multiple


La regression linéaire multiple characterise la relation entre une variable **quantitatives** observée $Y$ et plusieurs variables **quantitatives** explicatives $X_1, ..., X_p$. 

$$ Y \sim X_1 +  ... + X_p $$


## Présentation du modèle

Y est expliquée (modélisée) par  les variables explicatives $X_1, ..., X_p$. 

On applique alors le modèle linéaire suivant :

$$Y  = \beta_0 + \beta_1X_1 + ... + \beta_pX_p + \epsilon  $$

En pratique, dans un échantillon de $n$ individus, nous mesurons $y_i, x_{i,1},...,x_{i,p}$ pour $i=1,...,n$.
Les variables $x_{i,j}$ sont fixes, tandis que les variables $y_i$ sont aléatoires.

---

**Exemple**

La base de données `data(state)` comprend les mesures suivantes prisent sur 50 états des Etats-Unis.

```{r echo=TRUE, results="verbatim"}
data(state)
d = state_us =  data.frame(state.x77, row.names=state.abb)[,-8]
head(d)
dim(d)
```

- Life.Exp : espérance de vie moyenne (1969-1971)
- Population : population estimée au 1er juillet 1975
- Income : revenu par individu (1974)
- Illiteracy : illettrisme (1970, pourcentage de la population)
- Murder :  taux d’homicide pour 100 000 individus (1976)
- HS Grad :  pourcentage de diplômés niveau baccalauréat --high-school graduates-- (1970)
- Frost : nombre de jours moyens avec des températures inférieures à 0$^o$C dans les grandes villes (1931-1960)


On cherchera par exemples à expliquer la variables *Life.Exp* par les variables *Murder*, *HS.Grad* et *Frost*


$$Life.Exp \sim Murder + HS.Grad + Frost$$

Soit une formualtion analytique du modèle : 

$$Life.Exp = \beta_0 + \beta_{Murder} Murder + + \beta_{HS.Grad} HS.Grad + \beta_{Frost} Frost + \epsilon$$


```{r fig.height=9}
pairs(d)
```

---

















## Estimation des paramètres

### Méthode des moindres carrés


Pour l’instant, la droite de régression est inconnue.
Tout le problème est d’estimer les paramètres $\beta_0, ..., \beta_p$ du modèle de regression, à partir d’un échantillon de données.

Utiliser les **moindres carrés** revient à minimiser la quantité suivante :

$$min_{\beta_0, ...,\beta_p}\sum^n_{i=1}\Big(y_i -(\beta_0+ \beta_1X_1 + ... + \beta_pX_p)\Big)^2$$


**Version matricielle du problème**

Le système peut se réecrire :

$$\left(\begin{array}
{rrr}
y_1\\
\vdots\\
y_n
\end{array}\right) = \left(\begin{array}
{rrr}
1 & x_{1,1} & ... & x_{1,p}\\
\vdots & \vdots & \vdots & \vdots \\
1 & x_{n,1} & ... & x_{n,p}
\end{array}\right)  \left(\begin{array}
{rrr}
\beta_0\\
\vdots\\
\beta_n
\end{array}\right) + \left(\begin{array}
{rrr}
\epsilon_1\\
\vdots\\
\epsilon_n
\end{array}\right)
$$

Avec $y = X\beta + \epsilon$ 

Et le vecteur des résidus $\widehat{e} = y - \widehat{y} = y - X\widehat\beta$


Les variables $y$ et $X$ sont mesurées tandis que l’estimateur $\beta$  est à déterminer.
La **méthode des moindres carrés ordinaires** consiste à trouver le vecteur $\beta$  qui minimise $||\epsilon||^2 =  {}^t{\epsilon}\epsilon$.  


La formulation matricielle du problème montre qu’idéalement $n >> p$. 
Dans le cas inverse il faudra contrôler le **surapprentisage** et/ou la surinterprétation (*overfitting*) et mettre en oeuvre des statégies de **selection de variables** et/ou de **réduction de dimensions**.

---











**Exercice** : démontrer les développements suivants



\begin{eqnarray}
         ||\epsilon||^2    & = & {}^t(y-X\widehat\beta) (y-X\widehat\beta)            \\
                           & = & {}^tyy - {}^t\widehat\beta{}^tXy - {}^tyX\widehat\beta + {}^t\widehat\beta{}^tXX\widehat\beta           \\
                           & = & {}^tyy - 2{}^t\widehat\beta^t(X)y - {}^t\widehat\beta{}^tXX\widehat\beta     
\end{eqnarray}

comme ${}^t\widehat\beta{}^tXy$ est un scalaire, il est égal à sa transposée.

La dérivée par rapport à $\beta$ est alors égale à : $−2{}^tXy+2{}^tXX\widehat\beta$.



**Objectif** :  Nous cherchons $\beta$  qui annule cette dérivée. Donc nous devons résoudre l’équation suivante :

$$^tXX\widehat\beta = {}^tXy$$
Nous trouvons après avoir inversé la matrice ${}^tXX$ (il faut naturellement vérifier que ${}^tXX$ est carrée et inversible c’est-à-dire qu’aucune des colonnes qui compose cette matrice ne soit proportionnelle aux autres colonnes)
$$\widehat\beta  = ({}^t XX)^{-1}{}^{t} Xy.$$


**Application** : cas simple avec $p=2$ 

$$\left(\begin{array}
{rrr}
{}^tXX
\end{array}\right) = \left(\begin{array}
{rrr}
n & \sum{x_i}\\
\sum{x_i} & \sum{x_i}^2 
\end{array}\right) ;  \left(\begin{array}
{rrr}
{}^tXy
\end{array}\right) = \left(\begin{array}
{rrr}
\sum{y_i}\\
\sum{x_iy_i}
\end{array}\right)
$$

Donc

$$\left(\begin{array}
{rrr}
({}^tXX)^{-1}
\end{array}\right) = \frac{1}{n\sum{x_i^2 - (\sum{x_i^2})^2}}
\left(\begin{array}
{rrr}
\sum{x_i}^2 & -\sum{x_i}\\
-\sum{x_i} & n 
\end{array}\right) 
=  \frac{1}{\sum{x_i - \overline{x}_n}}
\left(\begin{array}
{rrr}
\sum{x_i}^2/n & -\overline{x}_n\\
-\overline{x}_n & 1 
\end{array}\right) 
$$

---






















### Maximum de vraisemblance


**Définition** : Probabilité d’observer les évènements $Y$, sachant les $X$, en considérant le modèle $\mathcal{M}$

**Important** : la vraisemblance dépend

  - des observations ($Y$ et $X$)
  - du modèle $\mathcal{M}$

  
En effet, 

\begin{eqnarray}

\mathcal{L}_{\mathcal{M}, Y,X} &=& \mathbb{P}_\mathcal{M}(Y|X) \\
                               &=& \prod^n_{i=1} \mathbb{P}_\mathcal{M}(Y_i|X_i)\\

log\mathcal{L}_{\mathcal{M}, Y,X} &=& log(\mathbb{P}_\mathcal{M}(Y|X)) \\
                                   &=& \sum^n_{i=1} log(\mathbb{P}_\mathcal{M}(Y_i|X_i))\\

\end{eqnarray}

---









**Exercice** : démontrer les développements suivants

Dans le cadre de la régression  linéaire multiple, la vraisemblance vaut : 

\begin{eqnarray}

 \mathcal{L}(Y,\beta,\sigma^2) &=& \Big(\frac{1}{\sqrt{2\pi\sigma^2}}\Big)^nexp\Big[\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-x’_i\beta)^2\Big] \\
                               &=& \Big(\frac{1}{\sqrt{2\pi\sigma^2}}\Big)^nexp\Big[\frac{1}{2\sigma^2}||Y-X\beta||^2\Big] 

\end{eqnarray}


D’où l’on déduit la log-vraisemblance

$$ log\mathcal{L}(Y,\beta,\sigma^2) = \frac{n}{2}log(2\pi) -\frac{n}{2}log(\sigma^2) - \frac{1}{2\sigma^2}||Y-X\beta||^2 $$
Pour maximiser la log-vraisemblance, il faut d’abord minimiser la quantité $||Y-X\beta||^2$, ce qui est justement le principe des moindres carrés ordinaires.


Dans notre **exemple** : 

```{r echo=TRUE, results="verbatim"}
m = lm(Life.Exp ~ Murder + HS.Grad + Frost, d)
m$coef
```

---



















## Qualité du modèle

### Décomposition de la variance 
La variation de $Y$ vient du fait de sa dépendance aux variables explicatives $X$. C’est la **variation expliquée par le modèle**.


Variation totale de Y = Variation expliquée par le modèle + Variation inexpliquée par le modèle


La méthode des moindres carrés, en considérant la somme des carrés de ces différences, nous assure la décomposition de la variance totale en variance expliquée et variance résiduelle :

$$ \sum(y_i - \overline{y})^2 = \sum(y_i - \widehat{y}_i)^2  \sum(\widehat{y}_i - \overline{y})^2 $$

Soit : 

$$ SC_{tot} = SC_{reg} + SC_{res}$$ 

Avec $SC_{tot}$ la somme des carrés totale,  $SC_{reg}$ la somme des carrés due à la régression et  $SC_{res}$ la somme des carrés des résidus.


### $R^2$


La mesure du pourcentage de la variation totale expliquée par le modèle se fait par le **coefficient de détermination**.

$$ R^2 = \frac{SC_{reg}}{SC_{tot}}$$

**Attention**, l’ajout de variables explicatives augmente systématiquement le pourcentage de variance expliquée.


**Exemple**

On calcule chacun des modèles de regression linéaire univariés permettant d’expliquer la variable *Life.Exp*.

Pour chacun des modèles, on calculer le $R^2$. Que peut-on dire ?


```{r}
m_Population = lm(Life.Exp ~ Population, d)
m_Income     = lm(Life.Exp ~ Income    , d)
m_Illiteracy = lm(Life.Exp ~ Illiteracy, d)
m_Murder     = lm(Life.Exp ~ Murder    , d)
m_HS.Grad    = lm(Life.Exp ~ HS.Grad   , d)
m_Frost      = lm(Life.Exp ~ Frost     , d)
layout(matrix(1:6, 2, byrow=TRUE), respect=TRUE)
plot(d$Population , d$Life.Exp, main=paste0("Life.Exp~Population", ", R^2=", signif(summary(m_Population)$r.squared, 3))); abline(m_Population, col=2)
plot(d$Income     , d$Life.Exp, main=paste0("Life.Exp~Income"    , ", R^2=", signif(summary(m_Income    )$r.squared, 3))); abline(m_Income    , col=2)
plot(d$Illiteracy , d$Life.Exp, main=paste0("Life.Exp~Illiteracy", ", R^2=", signif(summary(m_Illiteracy)$r.squared, 3))); abline(m_Illiteracy, col=2)
plot(d$Murder     , d$Life.Exp, main=paste0("Life.Exp~Murder"    , ", R^2=", signif(summary(m_Murder    )$r.squared, 3))); abline(m_Murder    , col=2)
plot(d$HS.Grad    , d$Life.Exp, main=paste0("Life.Exp~HS.Grad"   , ", R^2=", signif(summary(m_HS.Grad   )$r.squared, 3))); abline(m_HS.Grad   , col=2)
plot(d$Frost      , d$Life.Exp, main=paste0("Life.Exp~Frost"     , ", R^2=", signif(summary(m_Frost     )$r.squared, 3))); abline(m_Frost     , col=2)
```

On calcule ensuite les modèles de regression linéaire ci-après. Que peut-on dire ?

```{r echo=TRUE, results="verbatim"}
summary(lm(Life.Exp ~ Murder                  , d))$r.squared
summary(lm(Life.Exp ~ Murder + HS.Grad        , d))$r.squared
summary(lm(Life.Exp ~ Murder + HS.Grad + Frost, d))$r.squared
```

---









































## Tests statistiques

### Test de Student


Le test de Student dans le cadre de la regression linéaire multiple teste un des coéfficients du modèle de régression (*e.g* le *k*-ième).

Il pose la question de l’effet de la variables $X_k$ sur $Y$.


**Ecriture analytique du modèle**

$$Y = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k + ... + \beta_p X_p + \epsilon$$


**Hypothèse nulle et hypothèse alternative**

On se pose la question de l’effet de la variable $X_k$ :

$\mathcal{H}_0 : {\beta_k = 0}$ contre $\mathcal{H}_1 : {\beta_k \neq 0}$



**Conditions d’application du test**

- Les observations sont indépendantes
- La loi des erreurs est une loi normale
- La variance des erreurs est constante

**Statistique du test** 

Sous $\mathcal{H}_0$, 

$$t_{\widehat\beta_k,n-2} = \frac{\widehat\beta_k}{s_{\widehat\beta_k}} \sim \mathcal{T}_{n-2}$$


**Risque de première espèce $\alpha$ et p-valeur**

Le *risque de première espèce* ou risque $\alpha$ est le risque de rejeter l’hypothèse nulle $\mathcal{H}_0$ alors que celle-ci est vraie.

On prend généralement $\alpha=0.05$.

La *p-valeur* est la probabilité d’observer la valeur de la statistique de test sous $\mathcal{H}_0$ en ayant préalablement vérifié les conditions d’application du test.

**Décision et conclusion du test**

Ainsi :

  - si la p-valeur calculée est plus petite que notre risque $\alpha$ fixé, alors le test est significatif, H_0 est improbable), on rejete  $\mathcal{H}_0$ et on décide que $\mathcal{H}_1$ est vraie avec un risque d’erreur de première espèce $\alpha$.

  - si la p-valeur calculée est plus grande que notre risque $\alpha$ fixé, alors le test n’est pas significatif, on conserve $\mathcal{H}_0$ avec un risque d’erreur de deuxième espèce $\beta$ à évaluer. 




**Exemple**


```{r echo=TRUE, results="verbatim"}
d = state_us
m = lm(Life.Exp ~ Murder + HS.Grad + Frost, d)
summary(m)
```

---



### Test de Fisher

Le test de Fisher repose sur cette décomposition de la variance.

Il pose la question de l’effet de la variables $x$.


**Ecriture analytique du modèle**

$$Y = \beta_0 + \beta_1 x + \epsilon$$

**Hypothèses nulle et hypothèse alternative**

$\mathcal{H}_0 : {\beta_1 = ... = \beta_p = 0}$

contre 

$\mathcal{H}_1 : \exists$ au moins un $j$ tel que ${\beta_j \neq 0}$ où $j$ varie de $1$ à $p$.

*Remarque*: Si l’hypothèse nulle est vérifiée, alors $y_i = \beta_0 + \epsilon_i$



**Conditions d’application du test**

- Les observations sont indépendantes
- La loi des erreurs est une loi normale
- La variance des erreurs est constante

**Statistique du test** 

La statistique de test $\mathcal{F}$ est obtenue à partir du tableau de l’analyse de la variance : 

| Source de variation   | sc        | ddl  |  cm  | $F_{obs}$
| :-------------------- | :-------: | :--: | :--: | ---:
| Régression            | $sc_{reg} = \sum^n_{i=1}(\widehat{y}_i - \overline{y})^2$    | p-1  | $\frac{sc_{reg}}{p-1}$ | $\frac{cm_{reg}}{cm_{res}}$
| Résiduelle            | $sc_{res} = \sum^n_{i=1}(y_i - \widehat{y})^2$    | n-p  | $\frac{sc_{res}}{n-p}$ 
| Totale            | $sc_{tot} = \sum^n_{i=1}(y_i - \overline{y})^2$    | n-1  



Sous $\mathcal{H}_0$, 


$$F_{obs} = \frac{CM_{reg}} {CM_{res}} \sim \mathcal{F}_{p-1, n-p}$$


**Risque de première espèce $\alpha$ et p-valeur**

Le *risque de première espèce* ou risque $\alpha$ est le risque de rejeter l’hypothèse nulle $\mathcal{H}_0$ alors que celle-ci est vraie.

La *p-valeur* est la probabilité d’observer la valeur de la statistique de test sous $\mathcal{H}_0$ en ayant préalablement vérifié les conditions d’application du test.

**Décision et conclusion du test**

La valeur critique du test, notée $c_\alpha$ est lue dans une table de la loi de Fisher (ou avec le logiciel R). 
C’est la valeur de la statistique de test pour laquelle $\mathcal{H}_0$ devient improbable compte tenu du risuqe $alpha$ que l’on s’est fixé. 

Ainsi : 

- si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $F_{obs}$ est supérieure ou égale à $c_\alpha$, alors le test est significatif. Vous rejetez  $\mathcal{H}_0$ et vous décidez que $\mathcal{H}_1$ est vraie avec un risque d’erreur de première espèce $\alpha$.

- si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $F_{obs}$ est strictement inférieure à $c_\alpha$, alors le test n’est pas significatif. Vous conservez $\mathcal{H}_0$ avec un risque d’erreur de deuxième espèce $\beta$ à évaluer. 





**Exemple**


```{r echo=TRUE, results="verbatim"}
d = state_us
m = lm(Life.Exp ~ Murder + HS.Grad + Frost, d)
summary(m)
anova(m)
```
---





















