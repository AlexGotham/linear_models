## Exercices

### Sélection de variables

Le fléau de la dimension (*curse of dimensionality*), concept introduit par Bellman [1], précise que le nombre d’observations nécessaires pour estimer précisément une variable aléatoire augmente de maniére exponentielle par rapport au nombre de variable explicatives. lire [2]

[1] Bellman R.E. Adaptive Control Processes. Princeton University Press, Princeton, NJ, 1961.

[2] *Le fléau de la dimension : techniques de sélection de variables*, sept. 2015, https://www.quantmetry.com


**Problèmes**, quand $p > n$ :

  - la solution du système linéaire des moindres carrés n’est pas unique.
  - il existe toujours un ensemble de plans qui sépare parfaitement n’importe quelle partition de l’espace des observations.
  
Il est donc nécesaire de réduire le nombre de variables explicatives d’un modèle.

**Remarque**, si on veut à *expliquer* un phénomène ($Y$), on cherche souvent un modèle *parcimonieux* (peu de $X$). Si on veut *prédire*, ce n’est pas forcément le cas.

Pour sélectionner les variables d’intérêt, il existe classiquement plusieurs : 
 
- critères de choix : 
  -- Critère du $R^2$ 
  -- Critère d’information d’Akaike (AIC)
  -- Critère d’information bayesien (BIC)

- procédure :
-- Forward (ou pas à pas ascendante)
-- Backward (ou pas à pas descendante)
-- Stepwise

---

#### Les critères

**$R^2$**

La variance expliquée par le modèle. Augmente de façon monotone avec l’introduction de nouvelles variables.


**Critère d’information d’Akaike (AIC)**

Le critère AIC est défini par : 

$$AIC = - 2 ln(\mathcal{L}) + 2p $$

**Critère d’information bayesien (BIC)**

Le critère BIC est défini par : 

$$BIC =  - 2 ln(\mathcal{L}) + log(n)p $$

avec :

 - $\mathcal{L}$ le maximum de la fonction de vraisemblance
 - $n$ le nombre d’observations
 - $p$ le nombre de paramètres
 
 
Pour l’AIC et le BIC : 

- le critère s’applique aux modèles estimés par une méthode du **maximum de vraisemblance**
- le meilleur modèle est celui qui minimise le critère
- le nombre de paramètres du modèle ($p$) vient donc pénaliser le modèle.

Par définition le BIC est plus parcimonieux que l’AIC.

---

#### Les méthodes

Méthodes les plus classiques :

- Forward (ou pas à pas ascendante)
- Backward (ou pas à pas descendante)
- Stepwise

Ces méthodes s’appuient sur les *données recueillies*. Elles sont *itératives*. Elle dépendent de *paramètres*.

Bien que l’efficacité de ces méthodes ne puisse être démentie par la pratique, il ne serait pas raisonnable de se fier uniquement aux résultats statistiques fournis par un algorithme. 

En effet, pour décider d’ajouter ou de supprimer une variable dans un modèle, l’analyste ne se limite pas à la *technique*, il fait également appel à son *intuition*, sa *déduction* et son esprit de *synthèse*. Pour cela il confronte les approches (statistiques, analyse de données, classification) et il travaille en étroite collaboration avec les experts métiers (le métier dont sont issues les données).



#### Exemple détaillé sous R


On souhaite expliquer l’espérance de vie Life.Exp en fonction des autres variables. On va utiliser pour cela une méthode descendante.

Si nous souhaitons minimiser le critère AIC, nous pouvons l’obtenir de la façon suivante :

```{r echo=TRUE, results="verbatim"}
life.lm = lm(Life.Exp ~ ., data=state_us) 
summary(life.lm)
extractAIC(life.lm)
```

On préférera extractAIC à la fonction AIC qui donne un critère légèrement différent. 

Ici, on commence par enlever les variables dont la p-valeur est supérieure à 0.3.

```{r echo=TRUE, results="verbatim"}
life.lm =  update(life.lm,.~.-Area-Illiteracy-Income)
summary(life.lm)
extractAIC(life.lm)
```
On constate que l’AIC a bien diminué.
On enlève ensuite la variable dont le coefficient est le moins significatif, ici Population.

```{r echo=TRUE, results="verbatim"}
life.lm =  update(life.lm,.~.-Population)
summary(life.lm)
extractAIC(life.lm)
```

On constate que l’AIC a augmenté (et que l’écart-type estimé des résidus a augmenté : il vaut 0.7427 contre 0.7197 auparavant). On préfère donc le modèle conservant la variable Population.

```{r echo=TRUE, results="verbatim"}
life.lm = lm(Life.Exp ~ Murder + HS.Grad + Frost + Population, data=state_us)
```
R peut faire ce raisonnement de manière automatisée. Il suffit d’appliquer 

```{r echo=TRUE, results="verbatim"}
life.lm = step(lm(Life.Exp ~ ., data=state_us), method="bakward")
```

Le critère choisi par défaut est alors l’AIC. Il est également possible de choisir les méthodes "forward" et "both". Ensuite on peut résumer les différentes étapes de la façon suivante :

```{r echo=TRUE, results="verbatim"}
life.lm$anova
```
On peut également utiliser les fonctions add1 et drop1, non détaillées ici.

---


**Travaux pratiques**

- Réaliser la selection de variables avec la method **Forward**
- Comparer les 2 modèles obtenus.
- Réaliser la selection de variables sur le jeu de donnée réduit `d_train`
- Comparer
- Réaliser l’étude complète du modèle obtenu dur le jeu de données réduit


```{r ex2, echo=TRUE, eval=FALSE}
m_lo = lm(Life.Exp ~ 1, data=state_us)
m_up = lm(Life.Exp ~ ., data=state_us)
life.lm = step(m_lo, method="forward", scope=list(upper=m_up,lower=m_lo))
life.lm = step(m_lo, method="both", scope=list(upper=m_up,lower=m_lo))

set.seed(1)
d_train = d[sample(1:nrow(d), 25),]
m_lo = lm(Life.Exp ~ 1, data=d_train)
m_up = lm(Life.Exp ~ ., data=d_train)
life.lm = step(m_lo, method="both", scope=list(upper=m_up,lower=m_lo))

d_train = d[sample(1:nrow(d), 10),]
m_lo = lm(Life.Exp ~ 1, data=d_train)
m_up = lm(Life.Exp ~ ., data=d_train)
life.lm = step(m_lo, method="both", scope=list(upper=m_up,lower=m_lo))

d_train = d[sample(1:nrow(d), 10),]
m_lo = lm(Life.Exp ~ 1, data=d_train)
m_up = lm(Life.Exp ~ ., data=d_train)
life.lm = step(m_lo, method="both", scope=list(upper=m_up,lower=m_lo))

```

---







































### Exemple complet `ISwR::cystfibr`

Les données `ISwR::cystfibr` concernent la fonction respiratoire de patients atteints de fibrose kystique (mucoviscidose).

On cherche à expliquer la variable `pemax` qui caracterise cette maladie.

```{r echo=TRUE, results="verbatim"}
# ?ISwR::cystfibr
d = ISwR::cystfibr
head(d)
dim(d)
# removing sex
d = d[,-2]
```


2.1) Etudier la corrélation des variables à l’aide du code suivant. Commenter les graphiques obtenus.

```{r echo=TRUE, fig.width=9, fig.height=9, eval=TRUE}
pairs(d, gap=0)
lattice::levelplot((cor(d)))
```

2.2) Réaliser l’analyse en composante principale à l’aide du code suivant. Commenter les graphiques obtenus.

```{r echo=TRUE, eval=TRUE, fig.height=12}
# mat = x[,-9]
mat = d
pca = prcomp(mat, scale=TRUE)
v = pca$sdev * pca$sdev
p = v / sum(v) * 100

# layout(matrix(1:6,2), respect=TRUE)
layout(matrix(1:2,1), respect=TRUE)
barplot(p)

for (i in 1:5) {
  j = i+1
  plot(pca$x[,i], pca$x[,j], xlab=paste0("PC", i, "(", signif(p[i], 3), "%)"), ylab=paste0("PC", j, "(", signif(p[j], 3), "%)"), pch=16)  
  scale_factor = min(abs(c(min(c(pca$x[,i], pca$x[,j])), max(c(pca$x[,i], pca$x[,j])))))  
  # scale_factor = min(abs(c(max(min(pca$x[,i]), min(pca$x[,j])), min(max(pca$x[,i]), max(pca$x[,j])))))
  plotrix::draw.ellipse(0,0,scale_factor,scale_factor, lty=2, border="grey")
  arrows(0,0,pca$rotation[,i]*scale_factor, pca$rotation[,j]*scale_factor, col="grey")
  text(pca$rotation[,i]*scale_factor, pca$rotation[,j]*scale_factor, rownames(pca$rotation))
}

m_lo = lm(pemax ~ 1, data=d)
m_up = lm(pemax ~ ., data=d)
m_step = step(m_lo, method="forward", scope=list(upper=m_up,lower=m_lo))

m_step$anova
summary(m_step)

m_w = lm(pemax ~ weight, data=d)
summary(m_w)
m_wb = lm(pemax ~  weight + bmp, data=d)
summary(m_wb)
m_wbf = lm(pemax ~  weight + bmp + fev1 , data=d)
summary(m_wbf)
summary(m_w)

```

2.3) Proposer un modèle permettant d’expliquer la variable `pemax`. Détaillez votre méthodologie.

4) Discuter et interpréter le modèle obtenue (point de vue statistique)

5) Discuter et interpréter le modèle obtenue (point de vue métier, ici biologie, santé)




















































### Effet levier, distance de Cook

La distance de Cook est utilisée pour mesurer l’influence de l’observation  $i$ sur l’estimation.

On introduit $h_i$ tel que:

$$h_i = \frac{1}{n} + \frac{(x_i -\overline{x})^2}{\sum^n_{k=1}(x_k -\overline{x})^2}$$


Le terme $h_i$ représente le poids de l’observation $i$ sur sa propre estimation.

```{r}
# On peut montrer que :
# $$ \widehat{Y_i} = h_iY_i + \sum_{j\neq i}h_jY_j$$
```


De même $h_{ij}$ représente le poids de l’observation $i$ sur l’estimation de $\widehat Y_j$.

$$ h_{ij} = \frac{1}{n} + \frac{(x_i - \overline{x})(x_j - \overline{x}) }{\sum^n_{k=1}(x_k - \overline{x})^2}$$

Si $h_{ij}$ est grand ($\geq \frac{1}{2}$), alors le point $i$ est un point **levier** (point atypique).


La **distance de Cook** est définie comme : 

$$ D_i= \frac{\sum^n_{j=1}(\widehat Y_{(i)j}-\widehat Y_j)^2}{2\widehat\sigma^2}$$ `r #= \frac{h_{ij}}{2(1-h_{ij})}r^2_i$$`

avec $\widehat Y_{(i)j}$  la prédiction pour l’observation $j$ à partir d’un modèle de régression ajustée dans lequel l’observation $i$ a été omise.




**Représentation graphique**

La fonction `plot.lm` représente les principaux graphiques qui concernent les résidus.

- L’hypothèse d’homoscédasticité des résidus se vérifie visuellement sur le graphique "Residuals vs Fitted".
- L’hypothèse de normalité des résidus se vérifie visuellement sur le "QQ-plot" (comparaison quantiles observés avec quantiles théoriques)
- Les graphiques "Cook’s distance", "Residuals vs Leverage" et "Cook’s distance vs Leverage" présente l’influence de chacunes des observations.

**Exemple**

```{r echo=TRUE, results="verbatim"}
d = state_us
m = lm(Life.Exp ~ Murder + HS.Grad + Frost, d)
layout(matrix(1:6, 2, byrow=TRUE), respect=TRUE)
plot(m, which=1:6)
```

---





### Auto correlation des résidus

On vérifie plus précisément l’hypothèse d’indépendance des observations en étudiant l’autocorrélation des résidus. 

La fonction ACF indique si la valeur actuelle dépend toujours des valeurs précédentes (les décalages = lag).

**ATTENTION**, il faut : 

  1) réordonner aléatoirement les observations.
  2) ordonner les résidus en fonction des valeurs prédites ($Y_i$).


Les deux exemples suivants (séries temporelles), l’hypothèse d’indépendance ne semble pas vérifée.


```{r echo=TRUE, results="verbatim"}
layout(matrix(1:2, 1), respect=TRUE)
sub_CO2 = CO2[CO2$Type=="Quebec" & CO2$Treatment=="chilled", ]
sub_CO2 = sub_CO2[sample(1:nrow(sub_CO2)),]
m = lm(uptake~conc, sub_CO2)
plot(m$model$conc, m$model$uptake)
abline(m, col=2)
acf(m$residuals[order(m$fitted.values)])

layout(matrix(1:2, 1), respect=TRUE)
m = lm(y2~x2, anscombe)
plot(anscombe$x2, anscombe$y2)
abline(m, col=2)
acf(m$residuals[order(m$fitted.values)])
```




Dans l’exemple suivant, on voit l’effet de l’ordre des observations sur l’autocorrélation.

```{r echo=TRUE, results="verbatim"}
layout(matrix(1:2, 1), respect=TRUE)
m = lm(Hwt~Bwt, MASS::cats)
plot(m$model$Bwt, m$model$Hwt)
abline(m, col=2)
acf(m$residuals[order(m$fitted.values)])

layout(matrix(1:2, 1), respect=TRUE)
m = lm(Hwt~Bwt, MASS::cats[sample(1:nrow(MASS::cats)),])
plot(m$model$Bwt, m$model$Hwt)
abline(m, col=2)
acf(m$residuals[order(m$fitted.values)])
```

---






### Validation croisée

**Exercice** 

```{r ex3, eval = FALSE, echo=TRUE, results="verbatim"}
formulas = c(                                                                 
formula_1 = formula(Life.Exp ~ Murder)                                        ,
formula_2 = formula(Life.Exp ~ Murder + HS.Grad )                             ,
formula_3 = formula(Life.Exp ~ Murder + HS.Grad + Frost)                      ,
formula_4 = formula(Life.Exp ~ Murder + HS.Grad + Frost + Population)         ,
formula_5 = formula(Life.Exp ~ Murder + HS.Grad + Frost + Population + Income),
formula_6 = formula(Life.Exp ~ Murder + HS.Grad + Frost + Population + Income  + Illiteracy)
)
# Evaluer la capacité prédictive du modéle un jeu de données réduit                                                       
d = state_us
results = lapply(c(10, 20, 30, 40), function(n) {
  results = lapply(1:30, function(seed) {
    set.seed(seed)
    print(n)
    d_train = d[sample(1:nrow(d), n),]
    # d_test = d[sample(setdiff(rownames(d), rownames(d_train)), nrow(d_train)),]
    d_test = d[setdiff(rownames(d), rownames(d_train)),]
    # d_test = d[!rownames(d) %in% rownames(d_train),]
    results = lapply(formulas, function(f) {
      m_train = lm(Life.Exp ~ 
        Murder + 
        HS.Grad +
        Frost +
        Population +
        1 
        , d_train
      )
      m_train = lm(f, d_train)
      summary(m_train)

      # Prédire les valeurs Life.Exp du jeu de données d_train a partir de m_train et calculer les résidus standardisés et l'erreur standard
      pred_train = predict(m_train)
      res = m_train$res
      stdres_train = res/sd(res) 
      error_train = sqrt(mean((stdres_train)^2))

      # Prédire les valeurs Life.Exp du jeu de données restant (d_test) et calculer  l'erreur standard 
      pred_test = predict(m_train, newdata=d_test)
      res = pred_test - d_test$Life.Exp
      stdres_test = res/sd(res)
      error_test = sqrt(mean((stdres_test)^2))

      # Illustrer graphiquement
      # if (seed==1) {
      #   layout(matrix(1:2,1), respect=TRUE)
      #   plot(pred_train, d_train$Life.Exp, main=paste0("seed=", seed), )
      #   abline(a=0, b=1, col=2)
      #   points(pred_test, d_test$Life.Exp, pch=2, col=4)
      #   legend("topleft", c("stdres_train", "stdres_test"), col=c(1,4), pch=1:2)
      #   plot(density(stdres_train), ylim=0:1, lty=2)
      #   abline(v=0, col=2)
      # } else {
      #   lines(density(stdres_train), lty=2)
      # }
      # lines(density(stdres_test), col=4, lty=2)

      results = data.frame(
        error = c(error_train, error_test),
        dataset = c("d_train", "d_test")
      )
      results$formula = as.character(f)[3]
      results$n = n
      results
    })
    legend("topright", c("stdres_train", "stdres_test"), lty=1, col=c(1,4), lwd=3)

    results = do.call(rbind, results)
    results = data.frame(lapply(data.frame(results, stringsAsFactors=FALSE), unlist), stringsAsFactors=FALSE)
    results
  })
  results = do.call(rbind, results)
  results = data.frame(lapply(data.frame(results, stringsAsFactors=FALSE), unlist), stringsAsFactors=FALSE)
  results
})
results = do.call(rbind, results)
results = data.frame(lapply(data.frame(results, stringsAsFactors=FALSE), unlist), stringsAsFactors=FALSE)
results


head(results)
boxplot(error~dataset+formula+n, results, ylim=c(min(results$error),1.5), las=2)


```

---























### Exemple complet `Life.Exp`

On souhaitre réaliser le test de Fisher sur le modèle *Life.Exp ~ Murder + HS.Grad + Frost*.

- Poser les hypothèses nulle et hypothèse.
- Vérifier les conditions d’applications du test de Fisher au modéle 
- Définir le risque de première espèce $\alpha$ 
- Calculer la statistique de test
- Conclure 
- A l’aide de la function `anova` calculer le tableau de l’analyse de la variance pour les modèles : 
  - *Life.Exp ~ Murder + HS.Grad + Frost*
  - *Life.Exp ~ Frost + Murder + HS.Grad*
  - *Life.Exp ~ HS.Grad + Murder + Frost*


```{r eval=FALSE}
life.lm = lm(Life.Exp ~ Murder + HS.Grad + Frost, data = state_us)
summary(life.lm)
```

avec le modèle `Life.Exp ~ Murder + HS.Grad + Frost`

**Contribution des individus**



```{r echo=TRUE, results="verbatim"}
d = state_us
m = lm(Life.Exp ~ Murder + HS.Grad + Frost, data = d)
layout(matrix(1:3, 1, byrow=TRUE), respect=TRUE)
plot(m$model$Murder, m$model$Life.Exp, col=0, main="Life.Exp~Murder")
text(m$model$Murder, m$model$Life.Exp , labels=rownames(m$model))
plot(m$model$HS.Grad, m$model$Life.Exp, col=0, main="Life.Exp~HS.Grad")
text(m$model$HS.Grad, m$model$Life.Exp , labels=rownames(m$model))
plot(m$model$Frost, m$model$Life.Exp, col=0, main="Life.Exp~Frost")
text(m$model$Frost, m$model$Life.Exp , labels=rownames(m$model))

layout(matrix(1:6, 2, byrow=TRUE), respect=TRUE)
plot(m, which=1:6)

layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
acf(residuals(m[order(m$fitted.values)]),ci=0.99)
acf(residuals(m[order(m$fitted.values)]),ci=0.95)
```

Toutes les hypothèses semblent vérifiées : 

1) pas de structure dans le premier graphique ni dans le troisième (résidus standardisé). Les résidus étant centrés, la droite horizontale d’ordonnée 0 est ajoutée, pour pouvoir juger plus facilement de la répartition aléatoire des points. Si le graphique présente une quelconque structure, il convient de réfléchir à une nouvelle modélisation.
2) pas d’homoscédasticité au vu du premier graphique, 
3) hypothèse gaussienne non remise en cause par le QQ-plot (comparaison quantiles observés avec quantiles théoriques)
4) aucun point aberrant d’après la distance de Cook.

**Autocorrélation**

```{r echo=TRUE, results="verbatim"}
layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
acf(m$residuals[order(m$fitted.values)], ci=0.99, main="ci=0.99")
acf(m$residuals[order(m$fitted.values)], ci=0.95, main="ci=0.95")
```

L’hypothèse d’indépendance est cohérente graphiquement, au seuil 0.99, mais elle ne l’est pas valide avec une confiance de 0.95.

**Tests statistiques**

*Shapiro test* : test de la normalité des résidus.

```{r echo=TRUE, results="verbatim"}
shapiro.test(resid(m))
```
On obtient une p-valeur de 10.4%. On ne rejette pas l’hypothèse de normalité avec un seuil de 5%.

*Rainbow test* : test de la linéarité du modèle.

```{r echo=TRUE, results="verbatim"}
lmtest::raintest(m)
```
On obtient une p-valeur de 32.98%. On ne rejette pas le modèle linéaire. On teste ensuite l’hypothèse d’homogénéité des variances :

*Goldfeld-Quandt test* : test de l’homoscedasticité.

```{r echo=TRUE, results="verbatim"}
lmtest::gqtest(m)
```
On obtient une p-valeur de 75.62%. L’homogénéité de la variance des résidus n’est pas rejetée.

*Durbin-Watson test* : test de l’indépendance des résidus.


```{r echo=TRUE, results="verbatim"}
lmtest::dwtest(m)
```

On obtient une p-valeur de 23.23%. L’indépendance des résidus n’est pas rejetée (Attention, le test de Durbin Wastson teste l’autocorrélation d’ordre 1 et d’autres tests peuvent être plus adaptés,le test de Box-Pierce par exemple).
Nous vérifions enfin qu’il n’y a pas de colinéarité forte dans le modèle, c’est-à-dire que les variables explicatives ne sont pas linéairement dépendantes.

```{r echo=TRUE, results="verbatim"}
car::vif(m)
```

Les valeurs des VIF (Variance Inflation Factors) étant inférieures à 10, les variables ne présentent pas de forte colinéarité.

---








### Test de Fisher

Le test de Fisher repose sur cette décomposition de la variance.

Il pose la question de l’effet des variables $X$.



**Hypothèses nulle et hypothèse alternative**

$\mathcal{H}_0 : {\beta_1 = ... = \beta_p = 0}$

contre 

$\mathcal{H}_1 : \exists$ au moins un $j$ tel que ${\beta_j \neq 0}$ où $j$ varie de $1$ à $p$.

*Remarque*: Si l’hypothèse nulle est vérifiée, alors $y_i = \beta_0 + \epsilon_i$




**Conditions d’applications du test**

- Les observations sont indépendantes
- La variance des erreurs est homogène $\sigma^2$
- La loi des erreurs est une loi normale $\mathcal{N}(0, \sigma^2)$



**Risque de première espèce $\alpha$**

Le risque de première espèce ou risque $\alpha$ est le risque de rejeter l’hypothèse nulle $\mathcal{H}_0$ alors que celle-ci est vraie.

Généralement on fixe $\alpha=.05$.




**Statistique du test** 


La statistique de test *F* est obtenue à partri de tableau de l’analyse de la variance : 

| Source de variation   | sc        | ddl  |  cm  | $F_{obs}$
| :-------------------- | :-------: | :--: | :--: | ---:
| Régression            | $sc_{reg} = \sum^n_{i=1}(\widehat{y}_i - \overline{y})^2$    | p-1  | $\frac{sc_{reg}}{p-1}$ | $\frac{cm_{reg}}{cm_{res}}$
| Résiduelle            | $sc_{res} = \sum^n_{i=1}(y_i - \widehat{y})^2$    | n-p  | $\frac{sc_{res}}{n-p}$ 
| Totale            | $sc_{tot} = \sum^n_{i=1}(y_i - \overline{y})^2$    | n-1  


1) Calculer la statistique
$$F_{obs} = \frac{CM_{reg}} {CM_{res}}$$

2) Lire la valeur critique $F_{1−\alpha,p−1,n−p}$ où $F_{1−\alpha,p−1,n−p}$ est le $(1 − \alpha)$-quantile d’une loi de Fisher avec $(p − 1)$ et $(n − p)$ degrés de liberté, car si l’hypothèse nulle $\mathcal{H}_0$ est vraie, alors $F_{obs}$ suit une loi de Fisher avec $(p − 1)$ et $(n − p)$ degrés de liberté.

3) Comparer la statistique c’est-à-dire la valeur observée à la valeur critique.





**Décision et conclusion du test**: 

La valeur critique du test, notée $c_\alpha$ est lue dans une table de la loi de Fisher.

Nous décidons de rejeter l’hypothèse nulle  $\mathcal{H}_0$ et d’accepter l’hypothèse alternative  $\mathcal{H}_1$, au seuil $\alpha = 5\%$, si
$$|F_{obs}| \geq F_{1−\alpha,p−1,n−p}$$

Nous décidons de ne pas rejeter l’hypothèse nulle $\mathcal{H}_0$ et
donc de l’accepter si
$$|F_{obs}| < F_{1−\alpha,p−1,n−p}$$




---



















```{r eval=FALSE}
# ### Analyse de la variance: Test de Student
#
# On se pose la question de l’effet de la variable $x$ :
#
# $\mathcal{H}_0 : {\beta_j = 0}$ contre $\mathcal{H}_1 : {\beta_j \neq 0}$ pour $j = 0, ..., p$
#
# **Conditions d’applications du test** : Les variables aléatoire $\epsilon_i$ sont indépendantes et suivent la loi normale centrée et de variance $\sigma^2$.
#
# *Test d’ajustement des résidus à une gaussienne* : Test de shapiro, Kolmogoroff-Smirnoff, QQ plot
#
# **Statistique du test** : Si l’hypothèse nulle $\mathcal{H}_0$ est vérifiée, alors la variable aléatoire $\mathcal{T}_{\widehat\beta_j,n-2} = \frac{\widehat\beta_j-0}{s_{\widehat\beta_j}}$ suit la loi de Student $\mathcal{T}(n-2)$
#
# **Décision et conclusion du test**: La valeur critique du test, notée $c_\alpha$ est lue dans une table de la loi de Student.
#
# - Si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $\mathcal{T}_{\widehat\beta_j,n-2}(obs)$ est supérieure ou égale à $c_\alpha$, alors le test est significatif. Vous rejetez  $\mathcal{H}_0$ et vous décidez que $\mathcal{H}_1$ est vraie avec un risque d’erreur de première espèce $\alpha$.
#
# - Si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $\mathcal{T}_{\widehat\beta_j,n-2}(obs)$ est strictement inférieure à $c_\alpha$, alors le test n’est pas significatif. Vous conservez $\mathcal{H}_0$ avec un risque d’erreur de deuxième espèce $\beta$ à évaluer.
#
# ---
#
# ### Test de Student: exemple avec R
#
#
# ```{r}
# summary(life.lm)
# ```
#
# La p-valeur (p-value = 8.04e-10) du test de Student, associée à
# « Murder » étant inférieure ou égale à  $\alpha = 5\%$, le test est significatif. Nous rejetons H0 et nous décidons que H1 est vraie avec un risque de première espèce  $\alpha = 5\%$.
#
# La p-valeur (p-value = 0.00195  ) du test de Student, associée à
# « HS.Grad » étant inférieure ou égale à  $\alpha = 5\%$, le test est significatif. Nous rejetons H0 et nous décidons que H1 est vraie avec un risque de première espèce  $\alpha = 5\%$.
#
# La p-valeur (p-value = 0.00699  ) du test de Student, associée à
# « Frost » étant inférieure ou égale à  $\alpha = 5\%$, le test est significatif. Nous rejetons H0 et nous décidons que H1 est vraie avec un risque de première espèce  $\alpha = 5\%$.
#
# ---
#
# ### Intervalle de confiance
#
# On peut construire les intervalles de confiance suivants
#
# $$IC_{1-\alpha}(\widehat\beta_j) = \Big]\widehat\beta_j - t_{n-2;1-\alpha/2}*s_{\beta_j}; \widehat\beta_j + t_{n-2;1-\alpha/2}*s_{\beta_j}\Big[$$
#
# Cet intervalle de confiance est construit de telle sorte qu’il contienne le paramètre inconnu $\beta_j$ avec une probabilité de $(1−\alpha)$.
#
# ---
#
# ### IC/IP exemples sous R
#
# 1) Intervalle de confiance
# ```{r}
# confint(life.lm)
# ci=  c(life.lm$coefficients - 1.96*summary(life.lm)$coefficients[,2], life.lm$coefficients+ 1.96*summary(life.lm)$coefficients[,2])
# ci
# ```
#
# 2) Intervalle de prédiction
#
# Supposons que nous disposions des données pour un nouveau pays, par exemple européen. Nous souhaitons comparer l’espérance de vie estimée avec ce modèle avec l’espérance de vie observée. L’intervalle de con ance sur la valeur prédite est donné par l’instruction suivante :
#
# ```{r}
# life.pred = predict(life.lm,data.frame(Murder=8,  HS.Grad=75, Frost=80, Population=4250), interval="confidence",se.fit = TRUE)
# ci=  c(life.pred$fit[1] - 1.96*life.pred$se.fit, life.pred$fit[1]+ 1.96*life.pred$se.fit)
# life.pred
# ci
# ```
#
# ---
#
#
#
# ## Test statistique et IC
#
# ### Hypothèses fondamentales
#
# Il est important de noter que la construction du modèle de régression et l’estimation des paramètres par MCO ne fait pas appel aux hypothèses de distribution
#
# Les hypothèses de distribution sont essentielles lorsqu’il s’agit de construire des tests et des intervalles de confiance et de prédiction.
#
# Hypothèses fondamentales:
#
# - Les observations sont indépendantes
# - La variance des erreurs est constante $\sigma^2$
# - La loi des erreurs est une loi normale $\mathcal{N}(0, \sigma^2)$
#
# ---
#
#
#
# $y = X\beta + \epsilon$ où le vecteur aléatoire $\epsilon$ suit une loi multinormale qui vérifie les hypothèses suivantes:
# $$ \mathbb{E}[\epsilon] = 0 $$
#
# $$ Var[\epsilon] = \sigma^2I_n $$
# où $\sigma^2$ est la variance de la population et $I_n$ est la matrice identité de taille $n$.
#
# Les hypothèses précédentes impliquent $\mathbb{E}[y] = X\beta$ et $Var[y] = \sigma^2I_n$.
#
# ---
#
#
# Sous ces hypothèses, on peut alors démontrer les propriétés des estimateurs:
#
# $\mathbb{E}[\widehat\beta] = \beta$ : estimateur sans biais
#
# $Var[\widehat\beta] = \sigma^2({}^tXX)^{-1}$
#
# La variance $\sigma^2$ est inconnue. Il faut l’estimer:
#
# $$CM_{res} = \frac{\sum(y_i -\widehat{y}_i)^2}{n-p} = \frac{SC_{res}}{n-p} = \frac{SC_{tot}-SC_{reg}}{n-p}$$
#
# où $n$ est le nombre d’individus/observations, $p$ est le nombre de variables explicatives, et $(n-p)$ le nombre de degrés de liberté associé à $SC_{res}$.
#
# ---
```












